\chapter{大数定律与中心极限定理}

\section{切比雪夫不等式}

\begin{theorem}[（切比雪夫不等式）]
    设随机变量 $X$ 具有数学期望 $E(X)=\mu$ 和方差 $D(X) = \sigma^2$，则对于任意给定的正数 $\varepsilon$，有
    $$
    P\{ |X-E(X)| \geqslant \varepsilon \} \leqslant \dfrac{D(X)}{\varepsilon^2}
    $$
    它的等价形式是
    $$
    P\{ |X-E(X)| < \varepsilon \} \geqslant 1 - \dfrac{D(X)}{\varepsilon^2}
    $$
\end{theorem}

\begin{myproof}
    如果 $X$ 是离散型随机变量，设 $X$ 的概率分布为 $P\{X = x_k\} = p_k, \; k=1,2,\cdots$，根据概率的可加性可得
    $$
    P\{ |X-\mu| \geqslant \varepsilon \} = \sum_{|x_k-\mu| \geqslant \varepsilon} P\{ X=x_k \} = \sum_{|x_k-\mu| \geqslant \varepsilon} p_k
    $$
    由 $|x_k-\mu| \geqslant \varepsilon$ 得 $(x_k-\mu)^2 \geqslant \varepsilon^2$，即
    $$
    \dfrac{(x_k-\mu)^2}{\varepsilon^2} \geqslant 1
    $$
    从而有
    $$
    \begin{aligned}
        \sum_{|x_k-\mu| \geqslant \varepsilon} p_k & \leqslant \sum_{|x_k-\mu| \geqslant \varepsilon} \dfrac{(x_k-\mu)^2}{\varepsilon^2} p_k \\
        & \leqslant \dfrac{1}{\varepsilon^2} \sum_{k=1}^{\infty} (x_k-\mu)^2 p_k \\
        &= \dfrac{D(X)}{\varepsilon^2}
    \end{aligned}
    $$
    因此
    $$
    P\{ |X-E(X)| \geqslant \varepsilon \} \leqslant \dfrac{D(X)}{\varepsilon^2}
    $$

    如果 $X$ 是连续型随机变量，设 $X$ 的概率密度为 $f(x)$，则
    $$
    \begin{aligned}
        P\{ |X-E(X)| \geqslant \varepsilon \} &= \underset{|x-\mu| \geqslant \varepsilon}{\int} f(x) \, \text{d}x \\
         & \leqslant \underset{|x-\mu| \geqslant \varepsilon}{\int} \dfrac{(x-\mu)^2}{\varepsilon^2} f(x) \, \text{d}x \\
         & \leqslant \dfrac{1}{\varepsilon^2} \int_{-\infty}^{+\infty} (x-\mu)^2 f(x) \, \text{d}x \\
         &= \dfrac{D(X)}{\varepsilon^2}
    \end{aligned}
    $$
\end{myproof}

切比雪夫不等式给出了在随机变量 $X$ 的分布未知的情况下随机事件 $\{ |X-\mu| < \varepsilon \}$ 的概率的一种估计. 例如，取 $\varepsilon = 3\sigma$，则 $P\{ |X-\mu| < 3\sigma \} \geqslant 0.8889$.

\section{大数定律}

\subsection{依概率收敛}

\begin{definition}
    设 $X_1, X_2, \cdots, X_n, \cdots$ 是一个随机变量序列，$a$ 是一个常数.如果对于任意给定的正数 $\varepsilon$，有
    $$
    \lim_{n \to \infty} P \{ |X_n-a| < \varepsilon \} = 1
    $$
    则称随机变量序列 $X_1, X_2, \cdots, X_n, \cdots$ \textbf{依概率收敛}于 $a$，记作
    $$
    X_n \overset{P}{\longrightarrow} a
    $$
\end{definition}

性质：设 $X_n \overset{P}{\longrightarrow} a$，$Y_n \overset{P}{\longrightarrow} b$，函数 $g(x,y)$ 在点 $(a,b)$ 连续，则
$$
g(X_n, Y_n) \overset{P}{\longrightarrow} g(a,b)
$$

\subsection{大数定律}

\begin{theorem}[（切比雪夫定理）]
    设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，分别具有数学期望 $E(X_1), E(X_2), \cdots, E(X_n), \cdots$ 及方差 $D(X_1), D(X_2), \cdots, D(X_n), \cdots$，并且方差是一致有上界的（即存在正数 $M$，使得 $D(X_n) \leqslant M$，$n=1,2,\cdots$），则对任意给定的正数 $\varepsilon$，恒有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \dfrac{1}{n} \sum_{k=1}^n E(X_k) \right| < \varepsilon \right\} = 1
    $$
\end{theorem}

\begin{myproof}
    $$
    \begin{gathered}
        E(\dfrac{1}{n} \sum_{k=1}^n X_k) = \dfrac{1}{n} \sum_{k=1}^n E(X_k) \\
        D(\dfrac{1}{n} \sum_{k=1}^n X_k) = \dfrac{1}{n^2} \sum_{k=1}^n D(X_k)
    \end{gathered}
    $$
    根据切比雪夫不等式，得
    $$
    P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \dfrac{1}{n} \sum_{k=1}^n E(X_k) \right| < \varepsilon \right\} \geqslant 1 - \dfrac{\displaystyle\sum_{k=1}^n D(X_k)}{n^2 \varepsilon^2}
    $$
    由于方差一致有上界，因此
    $$
    \sum_{k=1}^n D(X_k) \leqslant nM
    $$
    从而得
    $$
    1 \geqslant P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \dfrac{1}{n} \sum_{k=1}^n E(X_k) \right| < \varepsilon \right\} \geqslant 1 - \dfrac{M}{n \varepsilon^2}
    $$
    令 $n \to \infty$，则有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \dfrac{1}{n} \sum_{k=1}^n E(X_k) \right| < \varepsilon \right\} = 1
    $$
\end{myproof}

利用依概率收敛的概念，切比雪夫定理可叙述成：设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，分别具有数学期望和方差，并且方差一致有上界，则 $X_1, X_2, \cdots, X_n$ 的算术平均值与它们的数学期望的算术平均值之差当 $n \to \infty$ 时依概率收敛于零.

\begin{corollary} \label{corollary:average}
    设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，并且具有相同的数学期望 $E(X_k) = \mu$ 和相同的方差 $D(X_k) = \sigma^2 \, (k=1,2,\cdots)$，则 $X_1, X_2, \cdots, X_n$ 的算术平均值 $\dfrac{1}{n} \displaystyle\sum_{k=1}^n X_k$ 当 $n \to \infty$ 时依概率收敛于数学期望 $\mu$，即对任意给定的正数 $\varepsilon$，恒有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \mu \right| < \varepsilon \right\} = 1
    $$
\end{corollary}

推论 \ref{corollary:average} 是实际问题中使用算术平均值的依据.为了测量某一个量 $a$，在相同的条件下重复测量 $n$ 次，得到 $n$ 个测量结果 $x_1,x_2,\cdots,x_n$，可以认为 $x_1,x_2,\cdots,x_n$ 分别是服从同一分布、有相同数学期望 $\mu$ 和方差 $\sigma^2$ 的随机变量 $X_1,X_2,\cdots,X_n$ 的试验数值.由推论 \ref{corollary:average} 可知，当 $n$ 充分大时，取这 $n$ 次测量结果的算术平均值作为 $a$ 的近似值，所发生的误差将很小.

\begin{theorem}[（伯努利定理）]
    设 $n_A$ 是在 $n$ 次独立重复试验中事件 $A$ 发生的次数，$p$ 是事件 $A$ 在一次试验中发生的概率，则对于任意给定的正数 $\varepsilon$，有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{n_A}{n} - p \right| < \varepsilon \right\} = 1
    $$
    其等价形式为
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{n_A}{n} - p \right| \geqslant \varepsilon \right\} = 0
    $$
\end{theorem}

\begin{myproof}
    $n_A$ 是一个随机变量，且服从二项分布 $B(n,p)$，从而有 $E(n_A) = np$，$D(n_A) = np(1-p)$.

    因为
    $$
    \begin{aligned}
        & E(\dfrac{n_A}{n}) = \dfrac{E(n_A)}{n} = p \\
        & D(\dfrac{n_A}{n}) = \dfrac{D(n_A)}{n^2} = \dfrac{p(1-p)}{n}
    \end{aligned}
    $$
    根据切比雪夫不等式，对任意给定的正数 $\varepsilon$，有
    $$
    1 \geqslant P \left\{ \left| \dfrac{n_A}{n} - p \right| < \varepsilon \right\} \geqslant 1 - \dfrac{p(1-p)}{n \varepsilon^2}
    $$
    令 $n \to \infty$，则有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{n_A}{n} - p \right| < \varepsilon \right\} = 1
    $$
\end{myproof}