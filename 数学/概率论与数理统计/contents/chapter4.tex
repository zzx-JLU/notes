% !TeX root = main.tex

\chapter{随机变量的数字特征}
\thispagestyle{plain}

\section{数学期望}

\subsection{数学期望的定义}

\begin{definition}
    \indent 设离散型随机变量 $X$ 的概率分布为 $P(X=x_k) = p_k, \; k=1,2,\cdots$,如果无穷级数 $\displaystyle\sum_{k=1}^{\infty} x_k p_k$ 绝对收敛,即
    $$
    \sum_{k=1}^{\infty} |x_k| p_k < \infty
    $$
    则称无穷级数 $\displaystyle\sum_{k=1}^{\infty} x_k p_k$ 的和为离散型随机变量 $X$ 的\textbf{数学期望}(mathematic expectation)或\textbf{均值},记作 $E(X)$ 或 $EX$,即
    $$
    E(X) = \sum_{k=1}^{\infty} x_k p_k
    $$

    设连续型随机变量 $X$ 的概率密度为 $f(x)$,如果反常积分 $\displaystyle\int_{-\infty}^{+\infty} x f(x) \, \text{d}x$ 绝对收敛,即
    $$
    \int_{-\infty}^{+\infty} |x| f(x) \, \text{d}x < \infty
    $$
    则称反常积分 $\displaystyle\int_{-\infty}^{+\infty} x f(x) \, \text{d}x$ 的值为连续型随机变量 $X$ 的\textbf{数学期望}或\textbf{均值},记作 $E(X)$ 或 $EX$,即
    $$
    E(X) = \int_{-\infty}^{+\infty} x f(x) \, \text{d}x
    $$
\end{definition}

\begin{note}
    \indent 以上定义中,要求无穷级数绝对收敛的目的在于使数学期望唯一.因为随机变量的取值可正可负,取值次序可先可后,如果此无穷级数绝对收敛,则可保证其和不受次序变动的影响.

    由于有限项的和不受次序变动的影响,因此取有限个可能值的随机变量的数学期望总是存在的.
\end{note}

数学期望的物理意义是重心.如果把概率 $P(X=x_k) = p_k$ 看做点 $x_k$ 上的质量,概率分布看做质量在 $x$ 轴上的分布,则 $X$ 的数学期望 $E(X)$ 就是该质量分布的重心所在位置.

\subsection{随机变量函数的数学期望}

\begin{theorem}
    \indent 设随机变量 $Y$ 是随机变量 $X$ 的函数,$Y=g(X)$,其中 $g$ 是一元连续函数.

    若 $X$ 是离散型随机变量,其概率分布为 $P(X=x_k) = p_k, \; k=1,2,\cdots$,如果无穷级数 $\displaystyle\sum_{k=1}^{\infty} g(x_k) p_k$ 绝对收敛,则随机变量 $Y$ 的数学期望为
    $$
    E(Y) = E[g(X)] = \sum_{k=1}^{\infty} g(x_k) \, p_k
    $$

    若 $X$ 是连续型随机变量,其概率密度为 $f(x)$,如果反常积分 $\displaystyle\int_{-\infty}^{+\infty} g(x) f(x) \, \text{d}x$ 绝对收敛,则随机变量 $Y$ 的数学期望为
    $$
    E(Y) = E[g(X)] = \int_{-\infty}^{+\infty} g(x) f(x) \, \text{d}x
    $$
\end{theorem}

\begin{theorem}
    \indent 设随机变量 $Z$ 是随机变量 $X$ 和 $Y$ 的函数,$Z=g(X,Y)$,其中 $g$ 是二元连续函数.

    若 $(X,Y)$ 是二维离散型随机变量,其概率分布为 $P(X=x_i,Y=y_j) = p_{ij}, \; i,j=1,2,\cdots$,如果无穷级数 $\displaystyle\sum_{j=1}^{\infty} \displaystyle\sum_{i=1}^{\infty} g(x_i,y_j) \, p_{ij}$ 绝对收敛,则随机变量 $Z$ 的数学期望为
    $$
    E(Z) = E[g(X,Y)] = \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} g(x_i,y_j) \, p_{ij}
    $$

    若 $(X,Y)$ 是二维连续型随机变量,其概率密度为 $f(x,y)$,如果反常积分
    $$
    \displaystyle\int_{-\infty}^{+\infty} \displaystyle\int_{-\infty}^{+\infty} g(x,y) f(x,y) \, \text{d}x \text{d}y
    $$
    绝对收敛,则随机变量 $Z$ 的数学期望为
    $$
    E(Z) = E[g(X,Y)] = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x,y) f(x,y) \, \text{d}x \text{d}y
    $$
\end{theorem}

\subsection{数学期望的性质}

\begin{property}[][][property:E(C)=C]
    \indent 若 $C$ 是常数,则 $E(C)=C$.
\end{property}

\begin{proof}
    如果随机变量 $X$ 恒取常数 $C$,则有 $P(X=C)=1$,从而有 $E(C) = C \times 1 = C$.
\end{proof}

\begin{property}[][][property:E(CX)=CE(X)]
    \indent 对于任意常数 $C$,有 $E(CX)=CE(X)$.
\end{property}

\begin{proof}
    若 $X$ 为离散型随机变量,其概率分布为 $P(X=x_i)=p_i, \; i=1,2,\cdots$,则
    $$
    E(CX) = \sum_{i=1}^{\infty} C x_i p_i = C \sum_{i=1}^{\infty} x_i p_i = CE(X)
    $$

    若 $X$ 为连续型随机变量,其概率密度为 $f(x)$,则
    $$
    E(CX) = \int_{-\infty}^{+\infty} Cx f(x) \, \text{d}x = C \int_{-\infty}^{+\infty} x f(x) \, \text{d}x = CE(X)
    $$
\end{proof}

\begin{property}[][][property:E(X+Y)=E(X)+E(Y)]
    \indent $E(X+Y)=E(X)+E(Y)$
\end{property}

\begin{proof}
    设二维离散型随机变量 $(X,Y)$ 的概率分布为 $P(X=x_i,Y=y_j) = p_{ij}, \; i,j=1,2,\cdots$,$(X,Y)$ 关于 $X$ 的边缘概率分布为 $P(X=x_i)=p_{i\cdot}, \; i=1,2,\cdots$,$(X,Y)$ 关于 $Y$ 的边缘概率分布为 $P(Y=y_j)=p_{\cdot j}, \; j=1,2,\cdots$,则有
    $$
    \begin{aligned}
        E(X+Y) &= \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} (x_i + y_j) \, p_{ij} \\
        &= \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} x_i p_{ij} + \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} y_j p_{ij} \\
        &= \sum_{i=1}^{\infty} \left( x_i \sum_{j=1}^{\infty} p_{ij} \right) + \sum_{j=1}^{\infty} \left( y_j \sum_{i=1}^{\infty} p_{ij} \right) \\
        &= \sum_{i=1}^{\infty} x_i p_{i \cdot} + \sum_{j=1}^{\infty} y_j p_{\cdot j} \\
        &= E(X)+E(Y)
    \end{aligned}
    $$

    设二维连续型随机变量 $(X,Y)$ 的概率密度为 $f(x,y)$,$(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘概率密度分别为 $f_X(x)$ 和 $f_Y(y)$,则有
    $$
    \begin{aligned}
        E(X+Y) &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x+y) f(x,y) \, \text{d}x \text{d}y \\
        &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x f(x,y) \, \text{d}x \text{d}y + \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} y f(x,y) \, \text{d}x \text{d}y \\
        &= \int_{-\infty}^{+\infty} x \left[ \int_{-\infty}^{+\infty} f(x,y) \, \text{d}y \right] \text{d}x + \int_{-\infty}^{+\infty} y \left[ \int_{-\infty}^{+\infty} f(x,y) \, \text{d}x \right] \text{d}y \\
        &= \int_{-\infty}^{+\infty} x f_X(x) \, \text{d}x + \int_{-\infty}^{+\infty} y f_Y(y) \, \text{d}y \\
        &= E(X)+E(Y)
    \end{aligned}
    $$
\end{proof}

一般地,对任意的两个函数 $g_1(x)$ 和 $g_2(x)$ ,有
$$
E[g_1(x) \pm g_2(x)] = E[g_1(x)] \pm E[g_2(x)]
$$

综合性质 \ref{property:E(C)=C}、\ref{property:E(CX)=CE(X)}、\ref{property:E(X+Y)=E(X)+E(Y)},有
$$
E(aX+bY+c) = aE(X) + bE(Y) + c \quad (a,b,c \, \text{均为常数})
$$

\begin{property}[][][prop:E(XY)=E(X)E(Y)]
    \indent 若随机变量 $X$ 和 $Y$ 相互独立,则有 $E(XY)=E(X) \, E(Y)$.
\end{property}

\begin{proof}
    设二维离散型随机变量 $(X,Y)$ 的概率分布为 $P(X=x_i,Y=y_j) = p_{ij}, \; i,j=1,2,\cdots$,$(X,Y)$ 关于 $X$ 的边缘概率分布为 $P(X=x_i)=p_{i\cdot}, \; i=1,2,\cdots$,$(X,Y)$ 关于 $Y$ 的边缘概率分布为 $P(Y=y_j)=p_{\cdot j}, \; j=1,2,\cdots$. 因为 $X$ 和 $Y$ 相互独立,所以 $p_{ij} = p_{i\cdot} \, p_{\cdot j}$,从而有
    $$
    \begin{aligned}
        E(XY) &= \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} x_i y_j p_{ij} \\
        &= \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} x_i y_j p_{i\cdot} p_{\cdot j} \\
        &= \sum_{i=1}^{\infty} x_i p_{i\cdot} \sum_{j=1}^{\infty} y_j p_{\cdot j} \\
        &= E(X) \, E(Y)
    \end{aligned}
    $$

    设二维连续型随机变量 $(X,Y)$ 的概率密度为 $f(x,y)$,$(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘概率密度分别为 $f_X(x)$ 和 $f_Y(y)$. 因为 $X$ 和 $Y$ 相互独立,所以 $f(x,y) = f_X(x) \, f_Y(y)$,因此
    $$
    \begin{aligned}
        E(XY) &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} xy f(x,y) \, \text{d}x \text{d}y \\
        &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} xy f_X(x) \, f_Y(y) \, \text{d}x \text{d}y \\
        &= \int_{-\infty}^{+\infty} x f_X(x) \, \text{d}x \int_{-\infty}^{+\infty} y f_Y(y) \, \text{d}y \\
        &= E(X) \, E(Y)
    \end{aligned}
    $$
\end{proof}

\begin{property}[][柯西-施瓦茨不等式]
    \indent 对于两个随机变量 $X$ 和 $Y$,设 $E(X^2)$ 和 $E(Y^2)$ 都存在,则
    $$
    [E(XY)]^2 \leqslant E(X^2) E(Y^2)
    $$
\end{property}

\begin{proof}
    对于任意实数 $t$,令 $g(t) = E[(X+tY)^2]$,则由数学期望的性质有
    $$
    g(t) = E[(X+tY)^2] = E(X^2 + 2tXY + t^2 Y^2) = E(X^2) + 2tE(XY) + t^2 E(Y^2)
    $$
    由于 $g(t) \geqslant 0$,所以有
    $$
    \varDelta = 4 [E(XY)]^2 - 4 E(X^2) E(Y^2) \leqslant 0
    $$
    从而
    $$
    [E(XY)]^2 \leqslant E(X^2) E(Y^2)
    $$
\end{proof}

\subsection{常见概率分布的数学期望}

\begin{conclusion}
    \indent 若随机变量 $X$ 服从参数为 $p$ 的0-1分布,则 $E(X) = p$.
\end{conclusion}

\begin{proof}
    $X$ 的分布律为

    \begin{center}
        \begin{tabular}{c | c c}
            \hline
            $X$ & 0 & 1 \\
            \hline
            $P$ & $1-p$ & $p$ \\
            \hline
        \end{tabular}
    \end{center}

    因此
    $$
    E(X) = 0 \times (1-p) + 1 \times p = p
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim B(n,p)$,则 $E(X) = np$.
\end{conclusion}

\begin{proof}
    $X$ 的概率分布为 $P(X=k) = C_n^k p^k (1-p)^{n-k}, \; k=0,1,2,\cdots,n$,因此
    $$
    \begin{aligned}
        E(X) &= \sum_{k=0}^n k C_n^k p^k (1-p)^{n-k} \\
        &= \sum_{k=0}^n k \dfrac{n!}{k! (n-k)!} p^k (1-p)^{n-k} \\
        &= \sum_{k=1}^n np \dfrac{(n-1)!}{(k-1)! \, [(n-1)-(k-1)]!} p^{k-1} (1-p)^{(n-1)-(k-1)} \\
        &= np[p+(1-p)]^{n-1} \\
        &= np
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim P(\lambda)$,则 $E(X) = \lambda$.
\end{conclusion}

\begin{proof}
    $X$ 的概率分布为 $P(X=k) = \dfrac{\lambda^k e^{-\lambda}}{k!}, \; k=0,1,2,\cdots$,因此
    $$
    \begin{aligned}
        E(X) &= \sum_{k=0}^{\infty} k \dfrac{\lambda^k e^{-\lambda}}{k!} \\
        &= \lambda e^{-\lambda} \sum_{k=1}^{\infty} \dfrac{\lambda^{k-1}}{(k-1)!} \\
        &= \lambda e^{-\lambda} e^{\lambda} \\
        &= \lambda
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Ge(p)$,则 $E(X) = \dfrac{1}{p}$.
\end{conclusion}

\begin{proof}
    $X$ 的概率分布为 $P(X=k) = (1-p)^{k-1} p, \; k=1,2,\cdots$,令 $q=1-p$,则
    $$
    \begin{aligned}
        E(X) &= \sum_{k=1}^{\infty} k (1-p)^{k-1} p \\
        &= p \sum_{k=1}^{\infty} k q^{k-1} \\
        &= p \left( \dfrac{\text{d}}{\text{d}q} \sum_{k=1}^{\infty} q^k \right) \\
        &= p \left( \dfrac{\text{d}}{\text{d}q} \dfrac{q}{1-q} \right) \\
        &= p \dfrac{1}{(1-q)^2} \\
        &= p \dfrac{1}{p^2} \\
        &= \dfrac{1}{p}
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim H(n,N,M)$,则 $E(X) = \dfrac{nM}{N}$.
\end{conclusion}

\begin{proof}
    首先给出一些重要的引理.

    \vspace{0.5em}

    \begin{lemma} \label{lemma:超几何分布1}
        \indent 对于正整数 $n,m$,如果 $n \geqslant m$,则对任意正整数 $a \leqslant m$,有
        \begin{equation} \label{equation:超几何分布1}
            m(m-1) \cdots (m-a+1) C_n^m = n(n-1) \cdots (n-a+1) C_{n-a}^{m-a}
        \end{equation}
        \begin{equation} \label{equation:超几何分布2}
            \dfrac{C_{n-a}^{m-a}}{C_n^m} = \dfrac{m(m-1) \cdots (m-a+1)}{n(n-1) \cdots (n-a+1)}
        \end{equation}
    \end{lemma}

    \vspace{-1em}

    \begin{proof}
        $$
        \begin{aligned}
            m(m-1) \cdots (m-a+1) C_n^m &= m(m-1) \cdots (m-a+1) \dfrac{n!}{m! (n-m)!} \\
            &= n(n-1) \cdots (n-a+1) \dfrac{(n-a)!}{(m-a)! [(n-a) - (m-a)]!} \\
            &= n(n-1) \cdots (n-a+1) C_{n-a}^{m-a}
        \end{aligned}
        $$
        式 \eqref{equation:超几何分布1} 得证.由式 \eqref{equation:超几何分布1} 整理即得式 \eqref{equation:超几何分布2}.
    \end{proof}

    \begin{lemma} \label{lemma:超几何分布2}
        \indent 对于正整数 $M,N,n$,如果 $N \geqslant n$ 且 $N \geqslant M$,则对任意正整数 $a$,当 $a \leqslant M, a \leqslant n$ 时,有
        \begin{equation} \label{equation:超几何分布3}
            \sum_{k=\max\{ a, n+M-N \}}^{\min\{ M,n \}} C_{M-a}^{k-a} C_{N-M}^{n-k} = C_{N-a}^{n-a}
        \end{equation}
    \end{lemma}

    \begin{proof}
        构造恒等式
        $$
        (1+x)^{M-a} (1+x)^{N-M} = (1+x)^{N-a}
        $$
        利用二项式定理将恒等式两侧展开,得
        $$
        \left( \sum_{i=0}^{M-a} C_{M-a}^i x^i \right) \left( \sum_{j=0}^{N-M} C_{N-M}^j x^j \right) = \sum_{k=0}^{N-a} C_{N-a}^k x^k
        $$
        等号左侧展开式中 $x^{n-a}$ 项的系数具有如下形式:
        $$
        \sum_{k=m}^{r} C_{M-a}^{k-a} C_{N-M}^{n-k}
        $$
        其中
        $$
        \begin{cases}
            0 \leqslant k-a \leqslant M-a \\[-5pt]
            k-a \leqslant n-a \\[-5pt]
            0 \leqslant n-k \leqslant N-M \\[-5pt]
            n-k \leqslant n-a
        \end{cases}
        $$
        即
        $$
        \begin{cases}
            k \leqslant M \\[-5pt]
            k \leqslant n \\[-5pt]
            k \geqslant n+M-N \\[-5pt]
            k \geqslant a
        \end{cases}
        $$
        因此 $\max\{ a, n+M-N \} \leqslant k \leqslant \min\{ M,n \}$,即等号左侧展开式中 $x^{n-a}$ 项的系数为
        $$
        \sum_{k=\max\{ a, n+M-N \}}^{\min\{ M,n \}} C_{M-a}^{k-a} C_{N-M}^{n-k}
        $$
        等号右侧展开式中 $x^{n-a}$ 项的系数为 $C_{N-a}^{n-a}$,而左右两侧的展开式中 $x^{n-a}$ 项的系数相等,因此
        $$
        \sum_{k=\max\{ a, n+M-N \}}^{\min\{ M,n \}} C_{M-a}^{k-a} C_{N-M}^{n-k} = C_{N-a}^{n-a}
        $$

        \vspace{-2em}
    \end{proof}

    下面证明超几何分布的数学期望.

    (1)当 $N=M$ 时,$P(X=n)=1$,因此 $E(X) = n = \dfrac{nM}{N}$.

    (2)当 $N>M=1$ 时,$X$ 的概率分布为
    $$
    \begin{aligned}
        & P(X=0) = \dfrac{C_{N-1}^n}{C_N^n} \\
        & P(X=1) = \dfrac{C_{N-1}^{n-1} C_1^1}{C_N^n} = \dfrac{n}{N}
    \end{aligned}
    $$
    此时
    $$
    E(X) = 0 \times \dfrac{C_{N-1}^n}{C_N^n} + 1 \times \dfrac{n}{N} = \dfrac{n}{N} = \dfrac{nM}{N}
    $$

    (3)当 $N>M \geqslant 2$ 时,$X$ 的概率分布为 $P(X=k) = \dfrac{C_M^k C_{N-M}^{n-k}}{C_N^n},\; k = m, m+1, m+2, \cdots, r$,则
    $$
    E(X) = \sum_{k=m}^r k \dfrac{C_M^k C_{N-M}^{n-k}}{C_N^n}
    $$
    由于 $k=0$ 的项恒为 $0$,可以忽略,因此 $k$ 的取值范围可以为 $\max\{ 1, n+M-N \} \leqslant k \leqslant r$,即
    $$
    E(X) = \sum_{k=\max\{ 1, n+M-N \}}^r k \dfrac{C_M^k C_{N-M}^{n-k}}{C_N^n}
    $$
    由式 \eqref{equation:超几何分布1} 可得 $k C_M^k = M C_{M-1}^{k-1}$,因此
    $$
    E(X) = \sum_{k=\max\{ 1, n+M-N \}}^{r} \dfrac{M C_{M-1}^{k-1} C_{N-M}^{n-k}}{C_N^n} = \dfrac{M}{C_N^n} \sum_{k=\max\{ 1, n+M-N \}}^{r} C_{M-1}^{k-1} C_{N-M}^{n-k}
    $$
    由式 \eqref{equation:超几何分布3} 可得 $\displaystyle\sum_{k=\max\{ 1, n+M-N \}}^{r} C_{M-1}^{k-1} C_{N-M}^{n-k} = C_{N-1}^{n-1}$,因此
    $$
    E(X) = \dfrac{M}{C_N^n} C_{N-1}^{n-1}
    $$
    由式 \eqref{equation:超几何分布2} 可得 $\dfrac{C_{N-1}^{n-1}}{C_N^n} = \dfrac{n}{N}$,因此
    $$
    E(X) = \dfrac{nM}{N}
    $$

    综上,当 $X \sim H(n,N,M)$ 时,$E(X) = \dfrac{nM}{N}$.
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Nb(r,p)$,则 $E(X) = \dfrac{r}{p}$.
\end{conclusion}

\begin{proof}
    $X$ 的概率分布为 $P(X=k) = C_{k-1}^{r-1} p^r (1-p)^{k-r},\; k = r, r+1, r+2, \cdots$,因此
    $$
    \begin{aligned}
        E(X) &= \sum_{k=r}^{\infty} k C_{k-1}^{r-1} p^r (1-p)^{k-r} \\
        &= \sum_{k=r}^{\infty} k \dfrac{r}{k} C_{k}^{r} p^r (1-p)^{k-r} \\
        &= \dfrac{r}{p} \sum_{k=r}^{\infty} C_{k}^{r} p^{r+1} (1-p)^{k-r} \\
    \end{aligned}
    $$

    设随机变量 $Y \sim Nb(r+1, p)$,则
    $$
    P(Y=k+1) = C_{k+1-1}^{r+1-1} p^{r+1} (1-p)^{(k+1)-(r+1)} = C_{k}^{r} p^{r+1} (1-p)^{k-r}
    $$
    其中 $k+1 = r+1, r+2, r+3, \cdots$,即 $k = r, r+1, r+2, \cdots$,因此
    $$
    \sum_{k=r}^{\infty} C_{k}^{r} p^{r+1} (1-p)^{k-r} = 1
    $$
    所以 $E(X) = \dfrac{r}{p}$.
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim U(a,b)$,则 $E(X) = \dfrac{a+b}{2}$.
\end{conclusion}

\begin{proof}
    $X$ 的概率密度为
    $$
    f(x) = \begin{cases}
        \dfrac{1}{b-a}, & a \leqslant x \leqslant b \\[0.5em]
        0, & \text{其他}
    \end{cases}
    $$
    因此
    $$
    E(X) = \int_{-\infty}^{+\infty} x f(x) \, \text{d}x = \int_a^b \dfrac{x}{b-a} \text{d}x = \dfrac{a+b}{2}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Exp(\lambda)$,则 $E(X) = \dfrac{1}{\lambda}$.
\end{conclusion}

\begin{proof}
    $X$ 的概率密度为
    $$
    f(x) = \begin{cases}
        \lambda e^{-\lambda x}, & x>0 \\
        0, & x \leqslant 0
    \end{cases}
    $$
    因此
    $$
    \begin{aligned}
        E(X) &= \int_0^{+\infty} x \lambda e^{-\lambda x} \, \text{d}x \\
        &= \int_0^{+\infty} x \, \text{d} (-e^{-\lambda x}) \\
        &= -xe^{-\lambda x} \Big|_0^{+\infty} + \int_0^{+\infty} e^{-\lambda x} \, \text{d}x \\
        &= \left. -\dfrac{1}{\lambda} e^{-\lambda x} \right|_0^{+\infty} \\
        &= \dfrac{1}{\lambda}
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim N(\mu,\sigma^2)$,则 $E(X) = \mu$.
\end{conclusion}

\begin{proof}
    $X$ 的概率密度为
    $$
    f(x) = \dfrac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}, \; -\infty < x < +\infty
    $$
    因此
    $$
    \begin{aligned}
        E(X) &= \int_{-\infty}^{+\infty} x \dfrac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \text{d}x \\
        & \xlongequal{t = \frac{x-\mu}{\sigma}} \dfrac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} (\sigma t + \mu) e^{-\frac{t^2}{2}} \text{d}t \\
        &= \dfrac{\sigma}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} te^{-\frac{t^2}{2}} \text{d}t + \mu \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} \text{d}t \\
        &= \mu
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Ga(\alpha,\lambda)$,则 $E(X) = \dfrac{\alpha}{\lambda}$.
\end{conclusion}

\begin{proof}
    $$
    \begin{aligned}
        E(X) &= \int_{-\infty}^{+\infty} x f(x) \, \text{d}x \\
        &= \int_{0}^{+\infty} \dfrac{\lambda^\alpha}{\Gamma(\alpha)} x^\alpha e^{-\lambda x} \text{d}x \\
        &= \dfrac{1}{\Gamma(\alpha)} \dfrac{1}{\lambda} \int_{0}^{+\infty} (\lambda x)^\alpha e^{-\lambda x} \text{d}(\lambda x) \\
        &= \dfrac{\Gamma(\alpha + 1)}{\Gamma(\alpha)} \dfrac{1}{\lambda} \\
        &= \dfrac{\alpha \Gamma(\alpha)}{\Gamma(\alpha)} \dfrac{1}{\lambda} \\
        &= \dfrac{\alpha}{\lambda}
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Be(a,b)$,则 $E(X) = \dfrac{a}{a+b}$.
\end{conclusion}

\begin{proof}
    $$
    \begin{aligned}
        E(X) &= \int_{-\infty}^{+\infty} x f(x) \, \text{d}x \\
        &= \int_{0}^{1} \dfrac{\Gamma(a+b)}{\Gamma(a) \, \Gamma(b)} x^a (1-x)^{b-1} \, \text{d}x \\
        &= \dfrac{\Gamma(a+b)}{\Gamma(a) \, \Gamma(b)} B(a+1,b) \\
        &= \dfrac{\Gamma(a+b)}{\Gamma(a) \, \Gamma(b)} \dfrac{\Gamma(a+1) \, \Gamma(b)}{\Gamma(a+b+1)} \\
        &= \dfrac{\Gamma(a+b)}{(a+b) \, \Gamma(a+b)} \dfrac{a \Gamma(a)}{\Gamma(a)} \\
        &= \dfrac{a}{a+b}
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 柯西分布的数学期望不存在.
\end{conclusion}

\begin{proof}
    柯西分布的概率密度为
    $$
    f(x) = \dfrac{1}{\pi} \dfrac{\gamma}{(x + x_0)^2 + \gamma^2},\; -\infty < x < +\infty
    $$
    而
    $$
    \begin{aligned}
        \int_{-\infty}^{+\infty} |x| f(x)\,\text{d}x &= \int_{-\infty}^{+\infty} |x| \dfrac{1}{\pi} \dfrac{\gamma}{(x - x_0)^2 + \gamma^2} \text{d}x \\
        &= \int_{-\infty}^{0} -x \dfrac{1}{\pi} \dfrac{\gamma}{(x - x_0)^2 + \gamma^2} \text{d}x + \int_{0}^{+\infty} x \dfrac{1}{\pi} \dfrac{\gamma}{(x - x_0)^2 + \gamma^2} \text{d}x \\
        &= 2 \int_{0}^{+\infty} x \dfrac{1}{\pi} \dfrac{\gamma}{(x - x_0)^2 + \gamma^2} \text{d}x \\
        &= \dfrac{1}{\pi} \int_{0}^{+\infty} \dfrac{2x}{\gamma \big[(\dfrac{x - x_0}{\gamma})^2 + 1 \big]} \text{d}x \\
        &= \dfrac{1}{\pi} \int_{0}^{+\infty} \dfrac{2(x - x_0) + 2x_0}{\gamma \big[(\dfrac{x - x_0}{\gamma})^2 + 1 \big]} \text{d}x \\
        &= \dfrac{1}{\pi} \left[ \int_{0}^{+\infty} \dfrac{2(x - x_0)}{\gamma \big[ (\dfrac{x - x_0}{\gamma})^2 + 1 \big]} \text{d}x + \int_{0}^{+\infty} \dfrac{2x_0}{\gamma \big[(\dfrac{x - x_0}{\gamma})^2 + 1 \big]} \text{d}x \right] \\
        &= \dfrac{1}{\pi} \left[ \int_{-\frac{x_0}{\gamma}}^{+\infty} \dfrac{2t}{t^2 + 1} \text{d}(\gamma t + x_0) + \int_{0}^{+\infty} \dfrac{2x_0}{(\dfrac{x - x_0}{\gamma})^2 + 1} \text{d} \dfrac{x - x_0}{\gamma} \right] \\
        &= \dfrac{1}{\pi} \left[ \int_{-\frac{x_0}{\gamma}}^{+\infty} \dfrac{2 \gamma t}{t^2 + 1} \text{d}t + 2x_0 \arctan(\dfrac{x - x_0}{\gamma}) \Big|_0^{+\infty} \right] \\
        &= \dfrac{1}{\pi} \left[ \int_{-\frac{x_0}{\gamma}}^{+\infty} \dfrac{\gamma}{t^2 + 1} \text{d} (t^2 + 1) + 2x_0 \arctan(\dfrac{x - x_0}{\gamma}) \Big|_0^{+\infty} \right] \\
        &= \dfrac{1}{\pi} \left[ \gamma \ln(t^2 + 1) \Big|_{-\frac{x_0}{\gamma}}^{+\infty} + 2x_0 \arctan(\dfrac{x - x_0}{\gamma}) \Big|_0^{+\infty} \right] \\
        &= +\infty
    \end{aligned}
    $$
    积分 $\displaystyle\int_{-\infty}^{+\infty} x f(x)\,\text{d}x$ 不绝对收敛,所以柯西分布的数学期望不存在.
\end{proof}

\section{方差}

\subsection{方差的定义}

\begin{definition}[][][def:variance]
    \indent 设 $X$ 是一个随机变量,如果 $E([X-E(X)]^2)$ 存在,则称之为随机变量 $X$ 的\textbf{方差}(variance),记作 $D(X)$ 或 $DX$,即
    $$
    D(X) = E([X-E(X)]^2)
    $$
    称 $\sqrt{D(X)}$ 为随机变量 $X$ 的\textbf{标准差}(standard deviation)或\textbf{均方差},记作 $\sigma(X)$,即
    $$
    \sigma(X) = \sqrt{D(X)}
    $$
\end{definition}

随机变量 $X$ 的方差反映了 $X$ 与其数学期望 $E(X)$ 的偏离程度. 如果 $X$ 取值集中在 $E(X)$ 附近,则 $D(X)$ 较小;如果 $X$ 取值比较分散,则 $D(X)$ 较大.

如果 $X$ 是离散型随机变量,概率分布为 $P(X=x_k) = p_k, \; k=1,2,\cdots$,则由定义 \ref{def:variance},有
$$
D(X) = E([X-E(X)]^2) = \sum_{k=1}^{\infty} [x_k - E(X)]^2 p_k
$$

如果 $X$ 是连续型随机变量,其概率密度为 $f(x)$,则由定义 \ref{def:variance},有
$$
D(X) = E([X-E(X)]^2) = \int_{-\infty}^{+\infty} [x - E(X)]^2 f(x) \, \text{d}x
$$

\subsection{方差的性质}

\begin{property}
    \indent $D(X) = E(X^2) - [E(X)]^2$
\end{property}

\begin{proof}
    $$
    \begin{aligned}
        D(X) &= E([X-E(X)]^2) \\
        &= E(X^2 - 2XE(X) + [E(X)]^2) \\
        &= E(X^2) - 2[E(X)]^2 + [E(X)]^2 \\
        &= E(X^2) - [E(X)]^2
    \end{aligned}
    $$
\end{proof}

\begin{corollary}
    \indent 由于 $D(X) = E(X^2) - [E(X)]^2 \geqslant 0$,因此
    \begin{enumerate}
        \item $E(X^2) \geqslant [E(X)]^2$.
        \item 若 $E(X^2) = 0$,则 $E(X)=0$,同时 $D(X)=0$.
    \end{enumerate}
\end{corollary}

\begin{property}
    \indent 若 $C$ 是常数,则 $D(C) = 0$.
\end{property}

\begin{proof}
    $$
    D(C) = E([C-E(C)]^2) = E([C-C]^2) = E(0) = 0
    $$
\end{proof}

\begin{property}
    \indent 若 $C$ 是常数,则 $D(CX) = C^2 D(X)$.
\end{property}

\begin{proof}
    $$
    \begin{aligned}
        D(CX) &= E([CX - E(CX)]^2) \\
        &= E([CX - CE(X)]^2) \\
        &= E(C^2 [X-E(X)]^2) \\
        &= C^2 E([X-E(X)]^2) \\
        &= C^2 D(X)
    \end{aligned}
    $$
\end{proof}

\begin{property}
    \indent 若 $C$ 是常数,则 $D(X+C) = D(X)$.
\end{property}

\begin{proof}
    $$
    \begin{aligned}
        D(X+C) &= E([(X+C) - E(X+C)]^2) \\
        &= E([X + C - E(X) - C]^2) \\
        &= E([X-E(X)]^2) \\
        &= D(X)
    \end{aligned}
    $$
\end{proof}

\begin{property}[][][prop:D(X+Y)=D(X)+D(Y)]
    \indent 如果随机变量 $X$ 和 $Y$ 相互独立,则有 $D(X \pm Y) = D(X)+D(Y)$.
\end{property}

\begin{proof}
    $$
    \begin{aligned}
        D(X \pm Y) &= E([X \pm Y - E(X \pm Y)]^2) \\
        &= E([(X-E(X)) \pm (Y-E(Y))]^2) \\
        &= E([X-E(X)]^2) \pm 2E([X-E(X)][Y-E(Y)]) + E([Y-E(Y)]^2)
    \end{aligned}
    $$

    因为 $X$ 与 $Y$ 相互独立,故 $X-E(X)$ 与 $Y-E(Y)$ 也相互独立,又 $E(X-E(X)) = 0$,再由性质 \ref{prop:E(XY)=E(X)E(Y)} 及方差的定义,得
    $$
    \begin{aligned}
        D(X \pm Y) &= D(X) \pm 2E(X-E(X)) \, E(Y-E(Y)) + D(Y) \\
        &= D(X) + D(Y)
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 对于任意常数 $C$,有 $E[(X-C)^2] \geqslant E([X-E(X)]^2) = D(X)$,当且仅当 $C=E(X)$ 时等号成立,此时 $E[(X-C)^2]$ 取得最小值.
\end{conclusion}

\begin{proof}
    对于任意常数 $C$,有
    $$
    E[(X-C)^2] = E(X^2 - 2CX + C^2) = E(X^2) - 2CE(X) + C^2
    $$
    将 $E(X^2) - 2CE(X) + C^2$ 看做以 $C$ 为自变量的二次函数,其中 $a=1$,$b=-2E(X)$,$c=E(X^2)$.根据二次函数的性质可知,当且仅当 $C=-\dfrac{b}{2a}=E(X)$ 时函数取得最小值,最小值为 $E(X^2) - [E(X)]^2 = D(X)$.
    
    因此 $E[(X-C)^2] \geqslant D(X)$,当且仅当 $C=E(X)$ 时等号成立,此时 $E[(X-C)^2]$ 取得最小值.
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X$ 仅在区间 $[a,b]$ 上取值,则
    \begin{enumerate}
        \item $a \leqslant E(X) \leqslant b$
        \item $D(X) \leqslant \left( \dfrac{b-a}{2} \right)^2$
    \end{enumerate}
\end{conclusion}

\begin{proof}
    (1)如果 $X$ 是离散型随机变量,设 $X$ 的概率分布为 $P(X = x_k) = p_k, \; k=1,2,\cdots$,则
    $$
    E(X) = \sum_{k=1}^{\infty} x_k p_k
    $$
    由于 $x_k \in [a,b]$,所以 $a \leqslant x_k \leqslant b$,因此
    $$
    a = \sum_{k=1}^{\infty} a p_k \leqslant E(X) \leqslant \sum_{k=1}^{\infty} b p_k = b
    $$

    如果 $X$ 是连续型随机变量,设 $X$ 的概率密度为 $f(x)$,则
    $$
    E(X) = \int_{a}^{b} x f(x)\,\text{d}x
    $$
    由于 $x \in [a,b]$,所以 $a \leqslant x \leqslant b$,因此
    $$
    a = \int_{a}^{b} a f(x)\,\text{d}x \leqslant E(X) \leqslant \int_{a}^{b} b f(x)\,\text{d}x = b
    $$

    (2)前面已经证明,对于任意常数 $C$,有 $E[(X-C)^2] \geqslant D(X)$.令 $C = \dfrac{a+b}{2}$,该不等式仍然成立,即
    $$
    D(X) \leqslant E[(X - \dfrac{a+b}{2})^2 ]
    $$
    因为 $X$ 的取值范围为 $[a,b]$,所以 $X - \dfrac{a+b}{2}$ 的取值范围为 $\left[ \dfrac{a-b}{2}, \dfrac{b-a}{2} \right]$,$\left( X - \dfrac{a+b}{2} \right)^2$ 的取值范围为 $\left[ 0, \left( \dfrac{b-a}{2} \right)^2 \right]$.根据(1)所得结论,有
    $$
    D(X) \leqslant E[(X - \dfrac{a+b}{2})^2] \leqslant \left( \dfrac{b-a}{2} \right)^2
    $$
\end{proof}

\subsection{切比雪夫不等式}

\begin{theorem}[][切比雪夫不等式]
    \indent 设随机变量 $X$ 具有数学期望 $E(X)=\mu$ 和方差 $D(X) = \sigma^2$,则对于任意给定的正数 $\varepsilon > 0$,有
    $$
    P(|X-E(X)| \geqslant \varepsilon) \leqslant \dfrac{D(X)}{\varepsilon^2}
    $$
    它的等价形式是
    $$
    P(|X-E(X)| < \varepsilon) \geqslant 1 - \dfrac{D(X)}{\varepsilon^2}
    $$
\end{theorem}

\begin{proof}
    如果 $X$ 是离散型随机变量,设 $X$ 的概率分布为 $P(X = x_k) = p_k, \; k=1,2,\cdots$,根据概率的可加性可得
    $$
    P(|X-E(X)| \geqslant \varepsilon) = \sum_{|x_k-E(X)| \geqslant \varepsilon} P(X=x_k) = \sum_{|x_k-E(X)| \geqslant \varepsilon} p_k
    $$
    由 $|x_k-E(X)| \geqslant \varepsilon$ 得 $(x_k-E(X))^2 \geqslant \varepsilon^2$,即
    $$
    \dfrac{(x_k-E(X))^2}{\varepsilon^2} \geqslant 1
    $$
    从而有
    $$
    \begin{aligned}
        \sum_{|x_k-E(X)| \geqslant \varepsilon} p_k & \leqslant \sum_{|x_k-E(X)| \geqslant \varepsilon} \dfrac{(x_k-E(X))^2}{\varepsilon^2} p_k \\
        & \leqslant \dfrac{1}{\varepsilon^2} \sum_{k=1}^{\infty} (x_k-E(X))^2 p_k \\
        &= \dfrac{D(X)}{\varepsilon^2}
    \end{aligned}
    $$
    因此
    $$
    P(|X-E(X)| \geqslant \varepsilon) \leqslant \dfrac{D(X)}{\varepsilon^2}
    $$

    如果 $X$ 是连续型随机变量,设 $X$ 的概率密度为 $f(x)$,则
    $$
    \begin{aligned}
        P(|X-E(X)| \geqslant \varepsilon) &= \underset{|x-E(X)| \geqslant \varepsilon}{\int} f(x) \, \text{d}x \\
         & \leqslant \underset{|x-E(X)| \geqslant \varepsilon}{\int} \dfrac{(x-E(X))^2}{\varepsilon^2} f(x) \, \text{d}x \\
         & \leqslant \dfrac{1}{\varepsilon^2} \int_{-\infty}^{+\infty} (x-E(X))^2 f(x) \, \text{d}x \\
         &= \dfrac{D(X)}{\varepsilon^2}
    \end{aligned}
    $$
\end{proof}

在概率论中,事件 $\{ |X-E(X)| \geqslant \varepsilon \}$ 称为\textbf{大偏差},其概率 $P(|X-E(X)| \geqslant \varepsilon)$ 称为\textbf{大偏差发生概率}.切比雪夫不等式给出大偏差发生概率的上界,这个上界与方差成正比,方差越大上界就越大.这说明随机变量的取值大概率集中在数学期望附近,方差越小,发生大偏差的概率就越小,随机变量的取值就越集中.

切比雪夫不等式给出了在随机变量 $X$ 的分布未知的情况下随机事件 $\{ |X-\mu| < \varepsilon \}$ 的概率的一种估计. 例如,取 $\varepsilon = 3\sigma$,则 $P(|X-\mu| < 3\sigma) \geqslant 0.8889$.

\begin{theorem}[][][prop:D(X)=0]
    \indent 若随机变量 $X$ 的方差存在,则 $D(X)=0$ 的充分必要条件是 $X$ 以概率 $1$ 取常数 $C$,即 $P(X=C)=1$,其中 $C=E(X)$.
\end{theorem}

\begin{proof}
    先证充分性.当 $P(X=C)=1$ 时,有 $D(X)=D(C)=0$,充分性成立.

    再证必要性.当 $D(X)=0$ 时,$E(X)$ 必定存在.因为
    $$
    \{ |X-E(X)|>0 \} = \bigcup_{n=1}^{\infty} \{ |X-E(X)| \geqslant \dfrac{1}{n} \}
    $$
    所以有
    $$
    \begin{aligned}
        P(|X-E(X)|>0) &= P \left( \bigcup_{n=1}^{\infty} \{ |X-E(X)| \geqslant \dfrac{1}{n} \} \right) \\
        & \leqslant \sum_{n=1}^{\infty} P(|X-E(X)| \geqslant \dfrac{1}{n})
    \end{aligned}
    $$
    根据切比雪夫不等式,有
    $$
    P(|X-E(X)| \geqslant \dfrac{1}{n}) \leqslant \dfrac{D(x)}{(1/n)^2}
    $$
    因此
    $$
    P(|X-E(X)|>0) \leqslant \sum_{n=1}^{\infty} \dfrac{D(x)}{(1/n)^2} = 0
    $$
    由概率的非负性可知
    $$
    P(|X-E(X)|>0) = 0
    $$
    又由于 $P(|X-E(X)|<0) = 0$,从而有
    $$
    P(|X-E(X)|=0) = 1
    $$
    即
    $$
    P(X=E(X)) = 1
    $$
    必要性成立.
\end{proof}

定理 \ref{prop:D(X)=0} 表明,方差为 $0$ 就意味着随机变量的取值集中在一点.

\begin{example}
    \indent 设 $g(x)$ 为随机变量 $X$ 取值的集合上的非负不减函数,且 $E(g(X))$ 存在,证明:对任意的 $\varepsilon > 0$,有
    $$
    P(X > \varepsilon) \leqslant \dfrac{E(g(X))}{g(\varepsilon)}
    $$
\end{example}

\begin{proof}
    因为 $g(x)$ 是非负不减函数,所以当 $x > \varepsilon$ 时有 $g(x) > g(\varepsilon)$,进而有 $\dfrac{g(x)}{g(\varepsilon)} > 1$.

    如果 $X$ 是离散型随机变量,设 $X$ 的概率分布为 $P(X = x_k) = p_k, \; k=1,2,\cdots$,则
    $$
    P(X > \varepsilon) = \sum_{x_k > \varepsilon} p_k \leqslant \sum_{x_k > \varepsilon} \dfrac{g(x_k)}{g(\varepsilon)} p_k \leqslant \dfrac{1}{g(\varepsilon)} \sum_{k=1}^{\infty} g(x_k) p_k = \dfrac{E(g(X))}{g(\varepsilon)}
    $$

    如果 $X$ 是连续型随机变量,设 $X$ 的概率密度为 $f(x)$,则
    $$
    P(X > \varepsilon) = \int_{\varepsilon}^{+\infty} f(x)\,\text{d}x \leqslant \int_{\varepsilon}^{+\infty} \dfrac{g(x)}{g(\varepsilon)} f(x)\,\text{d}x \leqslant \dfrac{1}{g(\varepsilon)} \int_{-\infty}^{+\infty} g(x) f(x)\,\text{d}x = \dfrac{E(g(X))}{g(\varepsilon)}
    $$
\end{proof}

\subsection{常见概率分布的方差}

\begin{conclusion}
    \indent 若随机变量 $X$ 服从参数为 $p$ 的0-1分布,则 $D(X) = p(1-p)$.
\end{conclusion}

\begin{proof}
    $X$ 服从参数为 $p$ 的0-1分布,则 $E(X)=p$,所以
    $$
    \begin{aligned}
        D(X) &= E([X-E(X)]^2) \\
        &= (0-p)^2 (1-p) + (1-p)^2 p \\
        &= p(1-p)
    \end{aligned}
    $$

    \vspace{-1.3em}
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim B(n,p)$,则 $D(X) = np(1-p)$.
\end{conclusion}

\begin{proof}
    根据二项分布的意义可知,$p$ 为 $n$ 重伯努利试验中每次试验成功的概率. 引入随机变量
    $$
    X_k = \begin{cases}
        1, & \text{第 $k$ 次试验成功} \\
        0, & \text{第 $k$ 次试验不成功}
    \end{cases} \quad k=1,2,\cdots,n
    $$
    则有
    $$
    X = X_1 + X_2 + \cdots + X_n
    $$
    由于 $X_k$ 只依赖于第 $k$ 次试验,而各次试验相互独立,于是 $X_1, X_2, \cdots, X_n$ 相互独立,且均服从参数为 $p$ 的0-1分布. 由于 $D(X_i) = p(1-p), \, i=1,2,\cdots,n$,所以根据方差的性质 \ref{prop:D(X+Y)=D(X)+D(Y)},有
    $$
    D(X) = \sum_{i=1}^n D(X_i) = np(1-p)
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim P(\lambda)$,则 $D(X) = \lambda$.
\end{conclusion}

\begin{proof}
    若随机变量 $X \sim P(\lambda)$,则
    $$
    \begin{aligned}
        E(X^2) &= \sum_{k=0}^{\infty} k^2 \dfrac{\lambda^k e^{-\lambda}}{k!} \\
        &= \sum_{k=1}^{\infty} k \dfrac{\lambda^{k}}{(k-1)!} e^{-\lambda} \\
        &= \sum_{k=1}^{\infty} [(k-1)+1] \dfrac{\lambda^{k}}{(k-1)!} e^{-\lambda} \\
        &= \sum_{k=1}^{\infty} (k-1) \dfrac{\lambda^{k}}{(k-1)!} e^{-\lambda} + \sum_{k=1}^{\infty} \dfrac{\lambda^{k}}{(k-1)!} e^{-\lambda} \\
        &= \lambda^2 e^{-\lambda} \sum_{k=1}^{\infty} \dfrac{\lambda^{k-2}}{(k-2)!} + \lambda e^{-\lambda} \sum_{k=1}^{\infty} \dfrac{\lambda^{k-1}}{(k-1)!} \\
        &= \lambda^2 + \lambda
    \end{aligned}
    $$
    由于 $E(X) = \lambda$,因此
    $$
    D(X) = E(X^2) - [E(X)]^2 = \lambda^2 + \lambda - \lambda^2 = \lambda
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Ge(p)$,则 $D(X) = \dfrac{1-p}{p^2}$.
\end{conclusion}

\begin{proof}
    若 $X \sim Ge(p)$,令 $q=1-p$,则
    $$
    \begin{aligned}
        E(X^2) &= \sum_{k=1}^{\infty} k^2 (1-p)^{k-1} p \\
        &= p \sum_{k=1}^{\infty} k^2 q^{k-1} \\
        &= p \left[ \sum_{k=1}^{\infty} (k+1)k q^{k-1} - \sum_{k=1}^{\infty} k q^{k-1} \right] \\
        &= p \left( \dfrac{\text{d}^2}{\text{d}q^2} \sum_{k=1}^{\infty} q^{k+1} - \dfrac{\text{d}}{\text{d}q} \sum_{k=1}^{\infty} q^k \right) \\
        &= p \left( \dfrac{\text{d}^2}{\text{d}q^2} \dfrac{q^2}{1-q} - \dfrac{\text{d}}{\text{d}q} \dfrac{q}{1-q} \right) \\
        &= p \left[ \dfrac{2}{(1-q)^3} - \dfrac{1}{(1-q)^2} \right] \\
        &= p \left( \dfrac{2}{p^3} - \dfrac{1}{p^2} \right) \\
        &= \dfrac{2}{p^2} - \dfrac{1}{p}
    \end{aligned}
    $$
    由于 $E(X) = \dfrac{1}{p}$,因此
    $$
    D(X) = E(X^2) - [E(X)]^2 = \dfrac{2}{p^2} - \dfrac{1}{p} - \dfrac{1}{p^2} = \dfrac{1-p}{p^2}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim H(n,N,M)$,则 $D(X) = \dfrac{nM(N-M)(N-n)}{N^2 (N-1)}$.
\end{conclusion}

\begin{proof}
    (1)当 $N=M$ 时,$P(X=n)=1$,$D(X)=0$,待证结论成立.

    (2)当 $N>M=1$ 时,$X$ 的概率分布为
    $$
    \begin{aligned}
        & P(X=0) = \dfrac{C_1^0 C_{N-1}^n}{C_N^n} = \dfrac{C_{N-1}^n}{C_N^n} \\
        & P(X=1) = \dfrac{C_1^1 C_{N-1}^{n-1}}{C_N^n} = \dfrac{n}{N}
    \end{aligned}
    $$
    进而有
    $$
    \begin{aligned}
        & E(X) = 0 \times \dfrac{C_{N-1}^n}{C_N^n} + 1 \times \dfrac{n}{N} = \dfrac{n}{N} \\
        & E(X^2) = 0^2 \times \dfrac{C_{N-1}^n}{C_N^n} + 1^2 \times \dfrac{n}{N} = \dfrac{n}{N}
    \end{aligned}
    $$
    因此
    $$
    D(X) = E(X^2) - [E(X)]^2 = \dfrac{n}{N} - \left( \dfrac{n}{N} \right)^2 = \dfrac{n(N-n)}{N^2}
    $$
    待证结论成立.
    
    (3)当 $N > M \geqslant 2$ 时,有
    $$
    \begin{aligned}
        E(X^2) &= \sum_{k=m}^{r} k^2 \dfrac{C_M^k C_{N-M}^{n-k}}{C_N^n} \\
        &= \sum_{k=\max\{ 1, n+M-N \}}^{r} k(k-1+1) \dfrac{C_M^k C_{N-M}^{n-k}}{C_N^n} \\
        &= \sum_{k=\max\{ 1, n+M-N \}}^{r} k(k-1) \dfrac{C_M^k C_{N-M}^{n-k}}{C_N^n} + \sum_{k=\max\{ 1, n+M-N \}}^{r} k \dfrac{C_M^k C_{N-M}^{n-k}}{C_N^n} \\
        &= \dfrac{M}{C_N^n} \sum_{k=\max\{ 2, n+M-N \}}^{r} (k-1) C_{M-1}^{k-1} C_{N-M}^{n-k} + \dfrac{nM}{N} \\
        &= \dfrac{M(M-1)}{C_N^n} \sum_{k=\max\{ 2, n+M-N \}}^{r} C_{M-2}^{k-2} C_{N-M}^{n-k} + \dfrac{nM}{N} \\
        &= \dfrac{M(M-1)}{C_N^n} C_{N-2}^{n-2} + \dfrac{nM}{N} \\
        &= M(M-1) \dfrac{n(n-1)}{N(N-1)} + \dfrac{nM}{N}
    \end{aligned}
    $$
    因此
    $$
    D(X) = E(X^2) - [E(X)]^2 = M(M-1) \dfrac{n(n-1)}{N(N-1)} + \dfrac{nM}{N} - \left( \dfrac{nM}{N} \right)^2 = \dfrac{nM(N-n)(N-M)}{N^2 (N-1)}
    $$

    综上,当 $X \sim H(n,N,M)$ 时,$D(X) = \dfrac{nM(N-M)(N-n)}{N^2 (N-1)}$.
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Nb(r,p)$,则 $D(X) = \dfrac{r(1-p)}{p^2}$.
\end{conclusion}

\begin{proof}
    $$
    \begin{aligned}
        E(X^2) &= \sum_{k=r}^{\infty} k^2 C_{k-1}^{r-1} p^r (1-p)^{k-r} \\
        &= \sum_{k=r}^{\infty} k^2 \dfrac{r}{k} C_{k}^{r} p^r (1-p)^{k-r} \\
        &= \dfrac{r}{p} \sum_{k=r}^{\infty} k C_{k}^{r} p^{r+1} (1-p)^{k-r} \\
        &= \dfrac{r}{p} \left[ \sum_{k=r}^{\infty} (k+1) C_{k}^{r} p^{r+1} (1-p)^{k-r} - \sum_{k=r}^{\infty} C_{k}^{r} p^{r+1} (1-p)^{k-r} \right] \\
        &= \dfrac{r}{p} \left[ \sum_{k=r}^{\infty} (k+1) C_{k}^{r} p^{r+1} (1-p)^{k-r} - 1 \right] \\
    \end{aligned}
    $$

    设随机变量 $Y \sim Nb(r+1,p)$,则
    $$
    P(Y=k+1) = C_{k+1-1}^{r+1-1} p^{r+1} (1-p)^{(k+1)-(r+1)} = C_{k}^{r} p^{r+1} (1-p)^{k-r}
    $$
    其中 $k+1 = r+1, r+2, r+3, \cdots$,即 $k = r, r+1, r+2, \cdots$,则
    $$
    \begin{aligned}
        & \sum_{k=r}^{\infty} (k+1) C_{k}^{r} p^{r+1} (1-p)^{k-r} = E(Y) = \dfrac{r+1}{p} \\
        & \sum_{k=r}^{\infty} C_{k}^{r} p^{r+1} (1-p)^{k-r} = 1
    \end{aligned}
    $$
    因此
    $$
    E(X^2) = \dfrac{r}{p} \left( \dfrac{r+1}{p} - 1 \right)
    $$
    由于 $E(X) = \dfrac{r}{p}$,因此
    $$
    D(X) = E(X^2) - [E(X)]^2 = \dfrac{r}{p} \left( \dfrac{r+1}{p} - 1 \right) - \left( \dfrac{r}{p} \right)^2 = \dfrac{r(1-p)}{p^2}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim U(a,b)$,则 $D(X) = \dfrac{(b-a)^2}{12}$.
\end{conclusion}

\begin{proof}
    若随机变量 $X$ 在区间 $[a,b]$ 上服从均匀分布,则
    $$
    E(X^2) = \int_{-\infty}^{+\infty} x^2 f(x) \, \text{d}x = \int_a^b x^2 \dfrac{1}{b-a} \text{d}x = \left. \dfrac{x^3}{3(b-a)} \right|_a^b = \dfrac{b^2 + ab + a^2}{3}
    $$
    由于 $E(X) = \dfrac{a+b}{2}$,因此
    $$
    D(X) = E(X^2) - [E(X)]^2 = \dfrac{b^2 + ab + a^2}{3} - \dfrac{(a+b)^2}{4} = \dfrac{(b-a)^2}{12}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Exp(\lambda)$,则 $D(X) = \dfrac{1}{\lambda^2}$.
\end{conclusion}

\begin{proof}
    由于 $E(X) = \dfrac{1}{\lambda}$,则
    $$
    \begin{aligned}
        E(X^2) &= \int_{-\infty}^{+\infty} x^2 f(x) \, \text{d}x \\
        &= \int_0^{+\infty} x^2 \lambda e^{-\lambda x} \, \text{d}x \\
        &= \int_0^{+\infty} x^2 \, \text{d}(-e^{-\lambda x}) \\
        &= -x^2 e^{-\lambda x} \Big|_0^{+\infty} + \int_0^{+\infty} 2x e^{-\lambda x} \, \text{d}x \\
        &= \dfrac{2}{\lambda} \int_0^{+\infty} x \lambda e^{-\lambda x} \, \text{d}x \\
        &= \dfrac{2}{\lambda} E(X) \\
        &= \dfrac{2}{\lambda^2}
    \end{aligned}
    $$
    因此 $D(X) = E(X^2) - [E(X)]^2 = \dfrac{2}{\lambda^2} - \dfrac{1}{\lambda^2} = \dfrac{1}{\lambda^2}$.
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim N(\mu,\sigma^2)$,则 $D(X) = \sigma^2$.
\end{conclusion}

\begin{proof}
    随机变量 $X \sim N(\mu,\sigma^2)$,则 $E(X)=\mu$,因此
    $$
    \begin{aligned}
        D(X) &= E([X-E(X)]^2) \\
        &= \int_{-\infty}^{+\infty} (x-\mu)^2 f(x) \, \text{d}x \\
        &= \int_{-\infty}^{+\infty} (x-\mu)^2 \dfrac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \text{d}x \\
        & \xlongequal{t = \frac{x-\mu}{\sigma}} \sigma^2 \int_{-\infty}^{+\infty} t^2 \dfrac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} \text{d}t \\
        &= -\sigma^2 \dfrac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} t \, \text{d} e^{-\frac{t^2}{2}} \\
        &= -\dfrac{\sigma^2}{\sqrt{2 \pi}} \left( \left. t e^{-\frac{t^2}{2}} \right|_{-\infty}^{+\infty} - \int_{-\infty}^{+\infty} e^{-\frac{t^2}{2}} \text{d}t \right) \\
        &= \dfrac{\sigma^2}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} e^{-\frac{t^2}{2}} \text{d}t \\
        &= \sigma^2
    \end{aligned}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Ga(\alpha,\lambda)$,则 $D(X) = \dfrac{\alpha}{\lambda^2}$.
\end{conclusion}

\begin{proof}
    $$
    \begin{aligned}
        E(X^2) &= \int_{-\infty}^{+\infty} x^2 f(x) \, \mathrm{d}x \\
        &= \int_{0}^{+\infty} \dfrac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha + 1} e^{-\lambda x} \mathrm{d}x \\
        &= \dfrac{1}{\Gamma(\alpha)} \dfrac{1}{\lambda^2} \int_{0}^{+\infty} (\lambda x)^{\alpha + 1} e^{-\lambda x} \mathrm{d}(\lambda x) \\
        &= \dfrac{\Gamma(\alpha + 2)}{\Gamma(\alpha)} \dfrac{1}{\lambda^2} \\
        &= \dfrac{\alpha (\alpha + 1) \Gamma(\alpha)}{\Gamma(\alpha)} \dfrac{1}{\lambda^2} \\
        &= \dfrac{\alpha (\alpha + 1)}{\lambda^2}
    \end{aligned}
    $$
    由于 $E(X) = \dfrac{\alpha}{\lambda}$,因此
    $$
    D(X) = E(X^2) - [E(X)]^2 = \dfrac{\alpha (\alpha + 1)}{\lambda^2} - \dfrac{\alpha^2}{\lambda^2} = \dfrac{\alpha}{\lambda^2}
    $$
\end{proof}

\begin{conclusion}
    \indent 若随机变量 $X \sim Be(a,b)$,则 $D(X) = \dfrac{ab}{(a+b)^2 (a+b+1)}$.
\end{conclusion}

\begin{proof}
    $$
    \begin{aligned}
        E(X^2) &= \int_{0}^1 \dfrac{\Gamma(a+b)}{\Gamma(a) \, \Gamma(b)} x^{a+1} (1-x)^{b-1} \mathrm{d}x \\
        &= \dfrac{\Gamma(a+b)}{\Gamma(a) \, \Gamma(b)} B(a+2, b) \\
        &= \dfrac{\Gamma(a+b)}{\Gamma(a) \, \Gamma(b)} \dfrac{\Gamma(a+2) \, \Gamma(b)}{\Gamma(a+b+2)} \\
        &= \dfrac{\Gamma(a+b)}{(a+b)(a+b+1) \, \Gamma(a+b)} \dfrac{a(a+1) \, \Gamma(a)}{\Gamma(a)} \\
        &= \dfrac{a(a+1)}{(a+b)(a+b+1)}
    \end{aligned}
    $$
    由于 $E(X) = \dfrac{a}{a+b}$,因此
    $$
    D(X) = E(X^2) - [E(X)]^2 = \dfrac{a(a+1)}{(a+b)(a+b+1)} - \dfrac{a^2}{(a+b)^2} = \dfrac{ab}{(a+b)^2 (a+b+1)}
    $$
\end{proof}

\subsection{随机变量的标准化}

设随机变量 $X$ 具有数学期望 $E(X)=\mu$ 及方差 $D(X) = \sigma^2 > 0$,则称 $X^* = \dfrac{X-\mu}{\sigma}$ 为 $X$ 的\textbf{标准化随机变量}.

随机变量 $X$ 的标准化随机变量 $X^*$ 满足 $E(X^*)=0$,$D(X^*)=1$.

\vspace{0.5em}

若 $X \sim N(\mu,\sigma^2) \, (\sigma > 0)$,则 $X^* = \dfrac{X-\mu}{\sigma} \sim N(0,1)$.

\section{协方差与相关系数}

\subsection{协方差}

在方差性质 \ref*{prop:D(X+Y)=D(X)+D(Y)} 的证明中可知,如果两个随机变量 $X$ 和 $Y$ 相互独立,则有
$$
E([X-E(X)][Y-E(Y)]) = 0
$$
这表明,当 $E([X-E(X)][Y-E(Y)]) \not= 0$ 时,$X$ 与 $Y$ 不相互独立,因此可以用这个量来描述 $X$ 和 $Y$ 之间的关系.

\begin{definition}
    设随机变量 $X$ 和 $Y$ 的数学期望 $E(X)$ 和 $E(Y)$ 都存在,如果 $E([X-E(X)][Y-E(Y)])$ 存在,则称之为随机变量 $X$ 和 $Y$ 的\textbf{协方差}(covariance),记作 $\operatorname{Cov}(X,Y)$,即
    $$
    \operatorname{Cov}(X,Y) = E([X-E(X)][Y-E(Y)]) = E(XY) - E(X) E(Y)
    $$
\end{definition}

\begin{property}
    $\operatorname{Cov}(X,Y) = \operatorname{Cov}(Y,X)$
\end{property}

\begin{property}
    对于常数 $a$ 和 $b$,有 $\operatorname{Cov}(aX,bY) = ab \operatorname{Cov}(X,Y)$.
\end{property}

\begin{proof}
    $$
    \begin{aligned}
        \operatorname{Cov}(aX,bY) &= E(aX \cdot bY) - E(aX) E(bY) \\
        &= ab E(XY) - ab E(X) E(Y) \\
        &= ab[E(XY) - E(X) E(Y)] \\
        &= ab \operatorname{Cov}(X,Y)
    \end{aligned}
    $$
\end{proof}

\begin{property}
    对于随机变量 $X,Y$ 和 $Z$,有
    $$
    \operatorname{Cov}(X+Y,Z) = \operatorname{Cov}(X,Z) + \operatorname{Cov}(Y,Z)
    $$
\end{property}

\begin{proof}
    $$
    \begin{aligned}
        \operatorname{Cov}(X+Y,Z) &= E((X+Y)Z) - E(X+Y) E(Z) \\
        &= E(XZ) + E(YZ) - E(X) E(Z) - E(Y) E(Z) \\
        &= [E(XZ) - E(X) E(Z)] + [E(YZ) - E(Y) E(Z)] \\
        &= \operatorname{Cov}(X,Z) + \operatorname{Cov}(Y,Z)
    \end{aligned}
    $$
\end{proof}

\begin{property}
    对任意随机变量 $X,Y$,有
    $$
    D(X \pm Y) = D(X) + D(Y) \pm 2 \operatorname{Cov}(X,Y)
    $$
\end{property}

\subsection{相关系数}

\begin{definition}
    设随机变量 $X$ 和 $Y$ 的方差都存在且不等于零,协方差 $\operatorname{Cov}(X,Y)$ 存在,称 $\dfrac{\operatorname{Cov}(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}}$ 为随机变量 $X$ 和 $Y$ 的\textbf{相关系数}(correlation coefficient),记作 $\rho_{XY}$,即
    $$
    \rho_{XY} = \dfrac{\operatorname{Cov}(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}}
    $$
    当 $\rho_{XY} = 0$ 时,称 $X$ 与 $Y$ \textbf{不相关}.
\end{definition}

\begin{property}
    $|\rho_{XY}| \leqslant 1$
\end{property}

\vspace{-1.5em}

\begin{proof}
    由柯西-施瓦茨不等式可得
    $$
    \begin{aligned}
        \left[ \operatorname{Cov}(X,Y) \right]^2 &= [E([X-E(X)][Y-E(Y)])]^2 \\
        & \leqslant E([X-E(X)]^2) E([Y-E(Y)]^2) \\
        &= D(X) D(Y)
    \end{aligned}
    $$
    因此 $|\operatorname{Cov}(X,Y)| \leqslant \sqrt{D(X)} \sqrt{D(Y)}$,从而有
    $$
    |\rho_{XY}| = \left| \dfrac{\operatorname{Cov}(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} \right| \leqslant 1
    $$

    \vspace{-2.3em}
\end{proof}

\begin{property}
    如果随机变量 $X$ 和 $Y$ 相互独立,则 $\rho_{XY} = 0$.
\end{property}

\vspace{-1em}

\begin{proof}
    若 $X$ 和 $Y$ 相互独立,则 $\operatorname{Cov}(X,Y) = 0$,进而 $\rho_{XY} = 0$.
\end{proof}

\begin{property}
    $|\rho_{XY}|=1$ 的充分必要条件是:存在常数 $a,b$,使得 $P\{Y=a+bX\}=1$.
\end{property}

\begin{proof}
    设 $D(X) = \sigma_X^2 > 0$,$D(Y) = \sigma_Y^2 > 0$. 对于任意实数 $b$,有
    $$
    \begin{aligned}
        D(Y-bX) &= E([Y-bX - E(Y-bX)]^2) \\
        &= E( \{[Y-E(Y)] - b[X-E(X)]\}^2 ) \\
        &= E([Y-E(Y)]^2) - 2bE([Y-E(Y)][X-E(X)]) + b^2 E([X-E(X)]^2) \\
        &= \sigma_Y^2 - 2b \operatorname{Cov}(X,Y) + b^2 \sigma_X^2
    \end{aligned}
    $$
    取 $b = \dfrac{\operatorname{Cov}(X,Y)}{\sigma_X^2}$,则有
    $$
    \begin{aligned}
        D(Y-bX) &= \sigma_Y^2 - \dfrac{2 [\operatorname{Cov}(X,Y)]^2}{\sigma_X^2} + \dfrac{[\operatorname{Cov}(X,Y)]^2}{\sigma_X^2} \\
        &= \sigma_Y^2 - \dfrac{[\operatorname{Cov}(X,Y)]^2}{\sigma_X^2} \\
        &= \sigma_Y^2 \left\{ 1 - \dfrac{[\operatorname{Cov}(X,Y)]^2}{\sigma_X^2 \sigma_Y^2} \right\} \\
        &= \sigma_Y^2 (1 - \rho_{XY}^2)
    \end{aligned}
    $$
    由此得 $|\rho_{XY}|=1$ 的充分必要条件是 $D(Y-bX)=0$. 根据方差的性质\ref*{prop:D(X)=0},$D(Y-bX)=0$ 的充分必要条件是 $Y-bX$ 以概率 1 取常数 $a=E(Y-bX)$,即
    $$
    P \{ Y-bX=a \} = 1
    $$
    亦即
    $$
    P\{Y=a+bX\}=1
    $$
\end{proof}

相关系数表示随机变量 $X$ 和 $Y$ 线性相关的程度. 当 $|\rho_{XY}|=1$ 时,$X$ 与 $Y$ 之间以概率 1 存在线性关系;当 $|\rho_{XY}|$ 较大时,称 $X$ 与 $Y$ 线性相关的程度较好;当 $|\rho_{XY}|$ 较小时,称 $X$ 与 $Y$ 线性相关的程度较差. 当 $\rho_{XY} > 0$ 时,称 $X$ 与 $Y$ 正相关,这时随着 $X$ 的增加,$Y$ 的值也有增加的趋势;当 $\rho_{XY} < 0$ 时,称 $X$ 与 $Y$ 负相关,这时随着 $X$ 的增加,$Y$ 的值有减小的趋势.

如果 $X$ 与 $Y$ 相互独立,则 $\rho_{XY} = 0$,即 $X$ 与 $Y$ 不相关. 反之,如果 $X$ 与 $Y$ 不相关,则 $X$ 和 $Y$ 之间不存在线性关系,但 $X$ 与 $Y$ 未必独立,二者可能存在其他关系.

对于随机变量 $X$ 和 $Y$,下列命题是等价的:
\begin{enumerate}
    \item $\operatorname{Cov}(X,Y) = 0$.
    \item $X$ 与 $Y$ 不相关.
    \item $E(XY) = E(X) E(Y)$.
    \item $D(X+Y) = D(X) + D(Y)$.
\end{enumerate}

\begin{conclusion}
    若二维随机变量 $(X,Y)$ 服从二维正态分布 $N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$,则 $X$ 与 $Y$ 的相关系数 $\rho_{XY} = \rho$.
\end{conclusion}

\begin{proof}
    $(X,Y)$ 的概率密度为
    $$
    f(x,y) = \dfrac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} e^{\frac{-1}{2(1-\rho^2)} \left[ \frac{(x-\mu_1)^2}{\sigma_1^2} - 2 \rho \frac{(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2} \right]},\ (x,y)\in \mathbf{R}^2
    $$
    $(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘概率密度分别为
    \begin{gather*}
        f_{X}(x) = \dfrac{1}{\sqrt{2\pi} \sigma_1} e^{-\frac{(x-\mu_1)^2}{2 \sigma_1^2}}, -\infty < x < +\infty \\
        f_{Y}(y) = \dfrac{1}{\sqrt{2\pi} \sigma_2} e^{-\frac{(y-\mu_2)^2}{2 \sigma_2^2}}, -\infty < y < +\infty
    \end{gather*}
    因此 $E(X) = \mu_1, E(Y) = \mu_2, D(X) = \sigma_1^2, D(Y) = \sigma_2^2$.
    \begin{small}
    $$
    \begin{aligned}
        \operatorname{Cov}(X,Y) &= E([X-E(X)][Y-E(Y)]) \\
        &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x-\mu_1)(y-\mu_2) f(x,y) \, \text{d}x \text{d}y \\
        &= \dfrac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x-\mu_1)(y-\mu_2) e^{\frac{-1}{2(1-\rho^2)} \left[ \frac{(x-\mu_1)^2}{\sigma_1^2} - 2 \rho \frac{(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2} \right]} \text{d}x \text{d}y \\
        &= \dfrac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x-\mu_1)(y-\mu_2) e^{-\frac{(x-\mu_1)^2}{2 \sigma_1^2}} e^{-\frac{1}{2(1-\rho^2)} \left( \frac{y-\mu_2}{\sigma_2} - \rho \frac{x-\mu_1}{\sigma_1} \right)^2} \text{d}x \text{d}y
    \end{aligned}
    $$
    \end{small}
    令 $t = \dfrac{1}{\sqrt{1-\rho^2}} \left( \dfrac{y-\mu_2}{\sigma_2} - \rho \dfrac{x-\mu_1}{\sigma_1} \right)$,$u = \dfrac{x-\mu_1}{\sigma_1}$,则有
    $$
    \begin{aligned}
        \operatorname{Cov}(X,Y) = & \; \dfrac{1}{2 \pi} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \sigma_1 \sigma_2 (\sqrt{1-\rho^2} tu + \rho u^2) e^{-\frac{t^2}{2}} e^{-\frac{u^2}{2}} \text{d}t \text{d}u \\
        = & \; \dfrac{\sigma_1 \sigma_2 \sqrt{1-\rho^2}}{2 \pi} \int_{-\infty}^{+\infty} t e^{-\frac{t^2}{2}} \text{d}t \int_{-\infty}^{+\infty} u e^{-\frac{u^2}{2}} \text{d}u + \\
        & \; \rho \sigma_1 \sigma_2 \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} \text{d}t \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} u^2 e^{-\frac{u^2}{2}} \text{d}u
    \end{aligned}
    $$
    而
    $$
    \begin{aligned}
        & \int_{-\infty}^{+\infty} t e^{-\frac{t^2}{2}} \text{d}t = \left. -e^{-\frac{t^2}{2}} \right|_{-\infty}^{+\infty} = 0 \\
        & \int_{-\infty}^{+\infty} u e^{-\frac{u^2}{2}} \text{d}u = 0 \\
        & \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} \text{d}t = 1 \\
        & \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} u^2 e^{-\frac{u^2}{2}} \text{d}u = 1
    \end{aligned}
    $$
    因此
    $$
    \operatorname{Cov}(X,Y) = \rho \sigma_1 \sigma_2
    $$
    所以
    $$
    \rho_{XY} = \dfrac{\operatorname{Cov}(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \dfrac{\rho \sigma_1 \sigma_2}{\sigma_1 \sigma_2} = \rho
    $$
\end{proof}

若 $(X,Y) \sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$,则 $X$ 与 $Y$ 相互独立的充分必要条件是 $\rho=0$. 由于 $\rho_{XY} = \rho$,所以当 $(X,Y) \sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$ 时,$X$ 与 $Y$ 相互独立的充分必要条件是 $X$ 与 $Y$ 不相关.

由于二维正态随机变量 $(X,Y)$ 的概率密度中的参数 $\rho$ 就是 $X$ 与 $Y$ 的相关系数,因此二维正态随机变量 $(X,Y)$ 的分布完全由 $X$ 和 $Y$ 的数学期望、方差以及 $X$ 与 $Y$ 的相关系数确定.

\section{矩}

\subsection{矩的概念}

\begin{definition}
    设 $X$ 和 $Y$ 是随机变量. 如果 $E(X^k), \, k=1,2,\cdots$ 存在,则称之为随机变量 $X$ 的 $k$ \textbf{阶原点矩},记作 $E(X^k) = \mu_k \, (k=1,2,\cdots)$.

    如果 $E([X-E(X)]^k), \, k=1,2,\cdots$ 存在,则称之为随机变量 $X$ 的 $k$ \textbf{阶中心矩}.

    如果 $E(X^k Y^l), \; k,l=1,2,\cdots$ 存在,则称之为随机变量 $X$ 和 $Y$ 的 $k+l$ \textbf{阶混合原点矩}.

    如果 $E([X-E(X)]^k [Y-E(Y)]^l), \; k,l=1,2,\cdots$ 存在,则称之为随机变量 $X$ 和 $Y$ 的 $k+l$ \textbf{阶混合中心矩}.
\end{definition}

随机变量 $X$ 的数学期望 $E(X)$ 是 $X$ 的一阶原点矩,方差 $D(X)$ 是 $X$ 的二阶中心矩,随机变量 $X$ 和 $Y$ 的协方差 $\operatorname{Cov}(X,Y)$ 是 $X$ 和 $Y$ 的二阶混合中心矩.

\subsection{协方差矩阵}

\begin{definition}
    设二维随机变量 $(X_1,X_2)$ 关于 $X_1$ 和 $X_2$ 的二阶中心距和二阶混合中心距
    $$
    c_{ij} = E([X_i-E(X_i)][X_j-E(X_j)]), \; i,j=1,2
    $$
    都存在,则称矩阵
    $$
    \boldsymbol{C} = \begin{bmatrix}
        c_{11} & c_{12} \\
        c_{21} & c_{22}
    \end{bmatrix}
    $$
    为二维随机变量 $(X_1,X_2)$ 的\textbf{协方差矩阵}.

    设 $n$ 维随机变量 $(X_1,X_2,\cdots,X_n)$ 关于 $X_1,X_2,\cdots,X_n$ 的二阶中心距和二阶混合中心距
    $$
    c_{ij} = E([X_i-E(X_i)][X_j-E(X_j)]), \; i,j=1,2,\cdots,n
    $$
    都存在,则称矩阵
    $$
    \boldsymbol{C} = \begin{bmatrix}
        c_{11} & c_{12} & \cdots & c_{1n} \\
        c_{21} & c_{22} & \cdots & c_{2n} \\
        \vdots & \vdots & & \vdots \\
        c_{n1} & c_{n2} & \cdots & c_{nn} \\
    \end{bmatrix}
    $$
    为 $n$ 维随机变量 $(X_1,X_2,\cdots,X_n)$ 的\textbf{协方差矩阵}.
\end{definition}

由于 $c_{ij}=c_{ji} (i \not= j, \; i,j=1,2,\cdots,n)$,所以 $\boldsymbol{C}$ 是对称矩阵.

\subsection{\texorpdfstring{$n$}{} 维正态分布}

设二维随机变量 $(X_1,X_2) \sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$,则
$$
E(X_1) = \mu_1, \; E(X_2) = \mu_2, \; D(X_1) = \sigma_1^2, \; D(X_2) = \sigma_2^2
$$
又由于 $\operatorname{Cov}(X_1,X_2) = \operatorname{Cov}(X_2,X_1) = \rho \sigma_1 \sigma_2$,从而有
$$
c_{11} = \sigma_1^2, \; c_{12} = c_{21} = \rho \sigma_1 \sigma_2, \; c_{22} = \sigma_2^2
$$
所以 $(X_1,X_2)$ 的协方差矩阵为
$$
\boldsymbol{C} = \begin{bmatrix}
    \sigma_1^2 & \rho \sigma_1 \sigma_2 \\
    \rho \sigma_1 \sigma_2 & \sigma_2^2
\end{bmatrix}
$$
其行列式 $|\boldsymbol{C}| = \sigma_1^2 \sigma_2^2 (1-\rho^2)$,$\boldsymbol{C}$ 的逆矩阵为
$$
\boldsymbol{C}^{-1} = \dfrac{1}{|\boldsymbol{C}|} \begin{bmatrix}
    \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\
    -\rho \sigma_1 \sigma_2 & \sigma_1^2
\end{bmatrix}
$$
令
$$
\boldsymbol{X} = \begin{bmatrix}
    x_1 \\
    x_2
\end{bmatrix}, \;
\boldsymbol{\mu} = \begin{bmatrix}
    \mu_1 \\
    \mu_2
\end{bmatrix}
$$
则
$$
\begin{aligned}
    (\boldsymbol{X} - \boldsymbol{\mu})^{\text{T}} \boldsymbol{C}^{-1} (\boldsymbol{X} - \boldsymbol{\mu}) &= \dfrac{1}{|\boldsymbol{C}|}
    \begin{bmatrix}
        x_1-\mu_1 & x_2-\mu_2
    \end{bmatrix}
    \begin{bmatrix}
        \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\
        -\rho \sigma_1 \sigma_2 & \sigma_1^2
    \end{bmatrix}
    \begin{bmatrix}
        x_1-\mu_1 \\
        x_2-\mu_2
    \end{bmatrix} \\
    &= \dfrac{1}{1-\rho^2} \left[ \dfrac{(x_1-\mu_1)^2}{\sigma_1^2} - 2\rho \dfrac{(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1 \sigma_2} + \dfrac{(x_2-\mu_2)^2}{\sigma_2^2} \right]
\end{aligned}
$$
因此二维正态随机变量 $(X_1,X_2)$ 的概率密度可以写成
$$
f(x_1,x_2) = \dfrac{1}{(2\pi)^{\frac{2}{2}} |\boldsymbol{C}|^{\frac{1}{2}}} e^{-\frac{1}{2} (\boldsymbol{X} - \boldsymbol{\mu})^{\text{T}} \boldsymbol{C}^{-1} (\boldsymbol{X} - \boldsymbol{\mu})}
$$

设 $(X_1,X_2,\cdots,X_n)$ 为 $n$ 维随机变量,记
$$
\boldsymbol{X} = \begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
\end{bmatrix}, \;
\boldsymbol{\mu} = \begin{bmatrix}
    \mu_1 \\
    \mu_2 \\
    \vdots \\
    \mu_n
\end{bmatrix} = \begin{bmatrix}
    E(X_1) \\
    E(X_2) \\
    \vdots \\
    E(X_n)
\end{bmatrix}
$$
如果 $(X_1,X_2,\cdots,X_n)$ 具有概率密度
$$
f(x_1,x_2,\cdots,x_n) = \dfrac{1}{(2\pi)^{\frac{n}{2}} |\boldsymbol{C}|^{\frac{1}{2}}} e^{-\frac{1}{2} (\boldsymbol{X} - \boldsymbol{\mu})^{\text{T}} \boldsymbol{C}^{-1} (\boldsymbol{X} - \boldsymbol{\mu})}
$$
其中 $\boldsymbol{C}$ 为 $(X_1,X_2,\cdots,X_n)$ 的协方差矩阵,则称 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布.

\begin{property}
    $n$ 维随机变量 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布的充分必要条件是:\\
    $X_1,X_2,\cdots,X_n$ 的任意线性组合 $k_1 X_1 + k_2 X_2 + \cdots + k_n X_n$ 都服从一维正态分布,其中 $k_1,k_2,\cdots,k_n$ 是不全为零的常数.
\end{property}

\begin{property}
    设 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布. 如果 $Y_1,Y_2,\cdots,Y_m$ 是 $X_i \, (i=1,2,\cdots,n)$ 的线性函数,则 $(Y_1,Y_2,\cdots,Y_m)$ 也服从多维正态分布.
\end{property}

\begin{property}
    如果 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布,则``随机变量 $X_1,X_2,\cdots,X_n$ 相互独立"与``$X_1,X_2,\cdots,X_n$ 两两不相关"等价.
\end{property}