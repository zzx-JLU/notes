% !TeX root = main.tex

\chapter{大数定律与中心极限定理}

\section{大数定律}

\subsection{依概率收敛}

\begin{definition}
    设 $X_1, X_2, \cdots, X_n, \cdots$ 是一个随机变量序列，$a$ 是一个常数.如果对于任意给定的正数 $\varepsilon$，有
    $$
    \lim_{n \to \infty} P \{ |X_n-a| < \varepsilon \} = 1
    $$
    则称随机变量序列 $X_1, X_2, \cdots, X_n, \cdots$ \textbf{依概率收敛}于 $a$，记作
    $$
    X_n \overset{P}{\longrightarrow} a
    $$
\end{definition}

性质：设 $X_n \overset{P}{\longrightarrow} a$，$Y_n \overset{P}{\longrightarrow} b$，函数 $g(x,y)$ 在点 $(a,b)$ 连续，则
$$
g(X_n, Y_n) \overset{P}{\longrightarrow} g(a,b)
$$

\subsection{大数定律}

\begin{theorem}[][切比雪夫定理]
    设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，分别具有数学期望 $E(X_1), E(X_2), \cdots, E(X_n), \cdots$ 及方差 $D(X_1), D(X_2), \cdots, D(X_n), \cdots$，并且方差是一致有上界的（即存在正数 $M$，使得 $D(X_n) \leqslant M$，$n=1,2,\cdots$），则对任意给定的正数 $\varepsilon$，恒有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \dfrac{1}{n} \sum_{k=1}^n E(X_k) \right| < \varepsilon \right\} = 1
    $$
\end{theorem}

\begin{proof}
    $$
    \begin{gathered}
        E(\dfrac{1}{n} \sum_{k=1}^n X_k) = \dfrac{1}{n} \sum_{k=1}^n E(X_k) \\
        D(\dfrac{1}{n} \sum_{k=1}^n X_k) = \dfrac{1}{n^2} \sum_{k=1}^n D(X_k)
    \end{gathered}
    $$
    根据切比雪夫不等式，得
    $$
    P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \dfrac{1}{n} \sum_{k=1}^n E(X_k) \right| < \varepsilon \right\} \geqslant 1 - \dfrac{\displaystyle\sum_{k=1}^n D(X_k)}{n^2 \varepsilon^2}
    $$
    由于方差一致有上界，因此
    $$
    \sum_{k=1}^n D(X_k) \leqslant nM
    $$
    从而得
    $$
    1 \geqslant P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \dfrac{1}{n} \sum_{k=1}^n E(X_k) \right| < \varepsilon \right\} \geqslant 1 - \dfrac{M}{n \varepsilon^2}
    $$
    令 $n \to \infty$，则有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \dfrac{1}{n} \sum_{k=1}^n E(X_k) \right| < \varepsilon \right\} = 1
    $$
\end{proof}

利用依概率收敛的概念，切比雪夫定理可叙述成：设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，分别具有数学期望和方差，并且方差一致有上界，则 $X_1, X_2, \cdots, X_n$ 的算术平均值与它们的数学期望的算术平均值之差当 $n \to \infty$ 时依概率收敛于零.

\begin{corollary} \label{corollary:average}
    设随机变量 $X_1, X_2, \cdots, X_n, \cdots$ 相互独立，并且具有相同的数学期望 $E(X_k) = \mu$ 和相同的方差 $D(X_k) = \sigma^2 \, (k=1,2,\cdots)$，则 $X_1, X_2, \cdots, X_n$ 的算术平均值 $\dfrac{1}{n} \displaystyle\sum_{k=1}^n X_k$ 当 $n \to \infty$ 时依概率收敛于数学期望 $\mu$，即对任意给定的正数 $\varepsilon$，恒有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \mu \right| < \varepsilon \right\} = 1
    $$
\end{corollary}

推论 \ref{corollary:average} 是实际问题中使用算术平均值的依据.为了测量某一个量 $a$，在相同的条件下重复测量 $n$ 次，得到 $n$ 个测量结果 $x_1,x_2,\cdots,x_n$，可以认为 $x_1,x_2,\cdots,x_n$ 分别是服从同一分布、有相同数学期望 $\mu$ 和方差 $\sigma^2$ 的随机变量 $X_1,X_2,\cdots,X_n$ 的试验数值.由推论 \ref{corollary:average} 可知，当 $n$ 充分大时，取这 $n$ 次测量结果的算术平均值作为 $a$ 的近似值，所发生的误差将很小.

\begin{theorem}[][伯努利定理]
    设 $n_A$ 是在 $n$ 次独立重复试验中事件 $A$ 发生的次数，$p$ 是事件 $A$ 在一次试验中发生的概率，则对于任意给定的正数 $\varepsilon$，有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{n_A}{n} - p \right| < \varepsilon \right\} = 1
    $$
    其等价形式为
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{n_A}{n} - p \right| \geqslant \varepsilon \right\} = 0
    $$
\end{theorem}

\begin{proof}
    $n_A$ 是一个随机变量，且服从二项分布 $B(n,p)$，从而有 $E(n_A) = np$，$D(n_A) = np(1-p)$.

    因为
    $$
    \begin{aligned}
        & E(\dfrac{n_A}{n}) = \dfrac{E(n_A)}{n} = p \\
        & D(\dfrac{n_A}{n}) = \dfrac{D(n_A)}{n^2} = \dfrac{p(1-p)}{n}
    \end{aligned}
    $$
    根据切比雪夫不等式，对任意给定的正数 $\varepsilon$，有
    $$
    1 \geqslant P \left\{ \left| \dfrac{n_A}{n} - p \right| < \varepsilon \right\} \geqslant 1 - \dfrac{p(1-p)}{n \varepsilon^2}
    $$
    令 $n \to \infty$，则有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{n_A}{n} - p \right| < \varepsilon \right\} = 1
    $$
\end{proof}

伯努利定理表明，一个事件 $A$ 在 $n$ 次独立重复试验中发生的频率 $\dfrac{n_A}{n}$ 当 $n \to \infty$ 时依概率收敛于事件 $A$ 发生的概率 $p$.伯努利定理以严格的数学形式表达了频率的稳定性.

伯努利定理的等价形式表明，当 $n$ 很大时，事件 $A$ 在 $n$ 次独立重复试验中发生的频率与 $A$ 在一次试验中发生的概率有较大偏差的可能性很小.根据实际推断原理，在实际应用中，当试验次数 $n$ 很大时，可以利用事件 $A$ 发生的频率代替事件 $A$ 发生的概率.

\begin{theorem}[][辛钦定理]
    设随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 相互独立，服从同一分布，且具有数学期望 $\mu$，则对于任意给定的正数 $\varepsilon$，有
    $$
    \lim_{n \to \infty} P \left\{ \left| \dfrac{1}{n} \sum_{k=1}^n X_k - \mu \right| < \varepsilon \right\} = 1
    $$
\end{theorem}

伯努利定理是辛钦定理的特殊情况.

由辛钦定理可知，如果随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 相互独立，服从同一分布且具有数学期望 $\mu$，则前 $n$ 个随机变量的算术平均值 $\dfrac{1}{n} \displaystyle\sum_{k=1}^n X_k$ 依概率收敛于它们的数学期望 $\mu$.如果 $E(X_k^l) = \mu_l \, (k=1,2,\cdots)$ 存在，则 $\dfrac{1}{n} \displaystyle\sum_{k=1}^n X_k^l$ 依概率收敛于 $\mu_l \, (l=1,2,\cdots)$.这是在数理统计中求参数点估计的矩估计法的理论基础.

\section{中心极限定理}

\begin{definition} \label{def: 依分布收敛}
    设随机变量 $X,X_1,X_2,\cdots,X_n,\cdots$ 的分布函数依次是
    $$
    F(x),F_1(x),F_2(x),\cdots,F_n(x),\cdots
    $$
    如果对于 $F(x)$ 的每一个连续点 $x$，都有
    $$
    \lim_{n \to \infty} F_n(x) = F(x)
    $$
    则称随机变量序列 $X_1,X_2,\cdots,X_n,\cdots$ \textbf{依分布收敛于} $X$，记作
    $$
    X_n \overset{L}{\longrightarrow} X
    $$
\end{definition}

\begin{theorem}[][独立同分布的中心极限定理] \label{theorem: i.i.d}
    设随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 相互独立，服从相同的分布，且具有数学期望 $E(X_k) = \mu$ 和方差 $D(X_k) = \sigma^2 \not= 0 \, (k=1,2,\cdots)$，随机变量
    $$
    Y_n = \dfrac{\displaystyle\sum_{k=1}^n X_k - n \mu}{\sqrt{n} \sigma}
    $$
    的分布函数为 $F_n(x)$，即
    $$
    F_n(x) = P \{ Y_n \leqslant x \} = P \Bigg\{ \dfrac{\displaystyle\sum_{k=1}^n X_k - n \mu}{\sqrt{n} \sigma} \leqslant x \Bigg\}, \; -\infty < x < +\infty
    $$
    则对任意实数 $x$，恒有
    $$
    \lim_{n \to \infty} F_n(x) = \varPhi(x) = \int_{-\infty}^x \dfrac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \text{d}t
    $$
\end{theorem}

定理 \ref{theorem: i.i.d} 也叫做\textbf{莱维-林德伯格定理}.

利用定义 \ref{def: 依分布收敛}，独立同分布的中心极限定理可叙述成：如果随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 相互独立，服从相同的分布，且具有数学期望 $E(X_k) = \mu$ 和方差 $D(X_k) = \sigma^2 \not= 0 \, (k=1,2,\cdots)$，则随机变量序列
$$
Y_n = \dfrac{\displaystyle\sum_{k=1}^n X_k - n \mu}{\sqrt{n} \sigma}, \; n=1,2,\cdots
$$
依分布收敛于服从标准正态分布的随机变量 $u$，即 $Y_n \overset{L}{\longrightarrow} u$.

\begin{theorem}[][棣莫弗-拉普拉斯极限定理] \label{theorem: 棣莫弗-拉普拉斯极限定理}
    设随机变量 $Y_n \sim B(n,p) \, (n=1,2,\cdots)$，则对任意实数 $x$，恒有
    $$
    \lim_{n \to \infty} P \left\{ \dfrac{Y_n - np}{\sqrt{np(1-p)}} \leqslant x \right\} = \int_{-\infty}^x \dfrac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \text{d}t
    $$
\end{theorem}

\begin{proof}
    设随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 相互独立，且都服从(0-1)分布，其概率分布为
    $$
    P \{ X_k=0 \} = 1-p, \, P \{ X_k=1 \} = p, \; 0 < p < 1, \, k=1,2,\cdots
    $$
    由于 $E(X_k) = p, \, D(X_k) = p(1-p)$，根据独立同分布的中心极限定理可知，对任意实数 $x$，恒有
    $$
    \lim_{n \to \infty} P \Bigg\{ \dfrac{\displaystyle\sum_{k=1}^n X_k - np}{\sqrt{np(1-p)}} \leqslant x \Bigg\} = \int_{-\infty}^x \dfrac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \text{d}t
    $$
    随机变量 $Y_n = \displaystyle\sum_{k=1}^n X_k$ 服从参数为 $n,p$ 的二项分布，即 $Y_n \sim B(n,p)$，因此有
    $$
    \lim_{n \to \infty} P \left\{ \dfrac{Y_n - np}{\sqrt{np(1-p)}} \leqslant x \right\} = \int_{-\infty}^x \dfrac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \text{d}t
    $$
\end{proof}

定理 \ref{theorem: 棣莫弗-拉普拉斯极限定理} 表明，正态分布是二项分布的极限分布，当 $n$ 充分大时，可以利用该定理近似计算二项分布的概率.

\begin{theorem}[][李雅普诺夫定理]
    设随机变量 $X_1,X_2,\cdots,X_n,\cdots$ 相互独立，且具有数学期望和方差
    $$
    E(X_k) = \mu_k, \, D(X_k) = \sigma_k^2 \not= 0, \, k=1,2,\cdots
    $$
    记
    $$
    B_n^2 = \sum_{k=1}^n \sigma_k^2
    $$
    设随机变量
    $$
    Z_n = \dfrac{\displaystyle\sum_{k=1}^n X_k - E(\displaystyle\sum_{k=1}^n X_k)}{\sqrt{D(\displaystyle\sum_{k=1}^n X_k)}} = \dfrac{\displaystyle\sum_{k=1}^n X_k - \displaystyle\sum_{k=1}^n \mu_k}{B_n}
    $$
    $Z_n$ 的分布函数为 $F_n(x)$，即
    $$
    F_n(x) = P \Bigg\{ \dfrac{\displaystyle\sum_{k=1}^n X_k - \displaystyle\sum_{k=1}^n \mu_k}{B_n} \leqslant x \Bigg\}, \; -\infty < x < +\infty
    $$
    如果存在正数 $\delta$，使得当 $n \to \infty$ 时，有
    $$
    \dfrac{1}{B_n^{2+\delta}} \sum_{k=1}^n E(|X_k - \mu_k|^{2+\delta}) \to 0
    $$
    则对任意实数 $x$，恒有
    $$
    \lim_{n \to \infty} F_n(x) = \varPhi(x) = \int_{-\infty}^x \dfrac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}} \text{d}t
    $$
\end{theorem}

在李雅普诺夫定理的条件下，当 $n$ 很大时，随机变量
$$
Z_n = \dfrac{\displaystyle\sum_{k=1}^n X_k - \displaystyle\sum_{k=1}^n \mu_k}{B_n}
$$
近似服从标准正态分布 $N(0,1)$.因此，当 $n$ 很大时，$\displaystyle\sum_{k=1}^n X_k = B_n Z_n + \displaystyle\sum_{k=1}^n \mu_k$ 近似服从正态分布 $N(\displaystyle\sum_{k=1}^n \mu_k, B_n^2)$.这就是说，无论各随机变量 $X_k \, (k=1,2,\cdots)$ 服从什么分布，只要满足李雅普诺夫定理的条件，当 $n$ 很大时，这些随机变量的和 $\displaystyle\sum_{k=1}^n X_k$ 就近似服从正态分布.