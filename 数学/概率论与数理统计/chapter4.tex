\chapter{随机变量的数字特征}

\section{数学期望}

\subsection{数学期望的概念}

\begin{definition}
    设离散型随机变量 $X$ 的概率分布为 $P\{X=x_k\} = p_k, \; k=1,2,\cdots$，如果无穷级数 $\displaystyle\sum_{k=1}^{\infty} x_k p_k$ 绝对收敛，则称无穷级数 $\displaystyle\sum_{k=1}^{\infty} x_k p_k$ 的和为离散型随机变量 $X$ 的\textbf{数学期望}（mathematic expectation）或\textbf{均值}，记作 $E(X)$ 或 $EX$，即
    $$
    E(X) = \sum_{k=1}^{\infty} x_k p_k
    $$

    设连续型随机变量 $X$ 的概率密度为 $f(x)$，如果反常积分 $\displaystyle\int_{-\infty}^{+\infty} x f(x) \, \text{d}x$ 绝对收敛，则称反常积分 $\displaystyle\int_{-\infty}^{+\infty} x f(x) \, \text{d}x$ 的值为连续型随机变量 $X$ 的\textbf{数学期望}或\textbf{均值}，记作 $E(X)$ 或 $EX$，即
    $$
    E(X) = \int_{-\infty}^{+\infty} x f(x) \, \text{d}x
    $$
\end{definition}

若随机变量 $X$ 服从参数为 $p$ 的(0-1)分布，即 $X$ 的分布律为

\begin{table}[htbp]
    \centering

    \begin{tabular}{c | c c}
        \hline
        $X$ & 0 & 1 \\
        \hline
        $P$ & $1-p$ & $p$ \\
        \hline
    \end{tabular}
\end{table}
则有
$$
E(X) = 0 \times (1-p) + 1 \times p = p
$$
\\

若随机变量 $X \sim B(n,p)$，即 $X$ 的概率分布为 $P\{X=k\} = C_n^k p^k (1-p)^{n-k}, \; k=0,1,2,\cdots,n$，则
$$
\begin{aligned}
    E(X) &= \sum_{k=0}^n k C_n^k p^k (1-p)^{n-k} \\
    &= \sum_{k=0}^n \dfrac{kn!}{k! (n-k)!} p^k (1-p)^{n-k} \\
    &= \sum_{k=1}^n \dfrac{np(n-1)! \, p^{k-1} (1-p)^{(n-1)-(k-1)}}{(k-1)! \, [(n-1)-(k-1)]!} \\
    &= np[p+(1-p)]^{n-1} \\
    &= np
\end{aligned}
$$

若随机变量 $X \sim P(\lambda)$，即 $X$ 的概率分布为 $P\{X=k\} = \dfrac{\lambda^k e^{-\lambda}}{k!}, \; k=0,1,2,\cdots$，则
$$
\begin{aligned}
    E(X) &= \sum_{k=0}^{\infty} k \dfrac{\lambda^k e^{-\lambda}}{k!} \\
    &= \lambda e^{-\lambda} \sum_{k=1}^{\infty} \dfrac{\lambda^{k-1}}{(k-1)!} \\
    &= \lambda e^{-\lambda} e^{\lambda} \\
    &= \lambda
\end{aligned}
$$

若随机变量 $X$ 服从参数为 $p$ 的几何分布，即 $X$ 的概率分布为 $P\{X=k\} = (1-p)^{k-1} p, \; k=1,2,\cdots$，则
$$
\begin{aligned}
    E(X) &= \sum_{k=1}^{\infty} k (1-p)^{k-1} p \\
    & \xlongequal{q=1-p} p \sum_{k=1}^{\infty} k q^{k-1} \\
    &= p \left( \dfrac{\text{d}}{\text{d}q} \sum_{k=1}^{\infty} q^k \right) \\
    &= p \left( \dfrac{\text{d}}{\text{d}q} \dfrac{q}{1-q} \right) \\
    &= p \dfrac{1}{(1-q)^2} \\
    &= p \dfrac{1}{p^2} \\
    &= \dfrac{1}{p}
\end{aligned}
$$

若随机变量 $X$ 在区间 $[a,b]$ 上服从均匀分布，即 $X$ 的概率密度为
$$
f(x) = \begin{cases}
    \dfrac{1}{b-a} & a \leqslant x \leqslant b \\[0.5em]
    0 & \text{其他}
\end{cases}
$$
则
$$
E(X) = \int_{-\infty}^{+\infty} x f(x) \, \text{d}x = \int_a^b \dfrac{x}{b-a} \text{d}x = \dfrac{a+b}{2}
$$

若随机变量 $X$ 服从参数为 $\lambda \, (\lambda>0)$ 的指数分布，即 $X$ 的概率密度为
$$
f(x) = \begin{cases}
    \lambda e^{-\lambda x} & x>0 \\
    0 & x \leqslant 0
\end{cases}
$$
则
$$
\begin{aligned}
    E(X) &= \int_{-\infty}^{+\infty} x f(x) \, \text{d}x \\
    &= \int_0^{+\infty} x \lambda e^{-\lambda x} \, \text{d}x \\
    &= \left. -xe^{-\lambda x} \right|_0^{+\infty} + \int_0^{+\infty} e^{-\lambda x} \, \text{d}x \\
    &= \left. -\dfrac{1}{\lambda} e^{-\lambda x} \right|_0^{+\infty} \\
    &= \dfrac{1}{\lambda}
\end{aligned}
$$

若随机变量 $X \sim N(\mu,\sigma^2)$，即 $X$ 的概率密度为
$$
f(x) = \dfrac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}, \; -\infty < x < +\infty
$$
则
$$
\begin{aligned}
    E(X) &= \int_{-\infty}^{+\infty} x \dfrac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \text{d}x \\
    & \xlongequal{t = \frac{x-\mu}{\sigma}} \dfrac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} (\sigma t + \mu) e^{-\frac{t^2}{2}} \text{d}t \\
    &= \dfrac{\sigma}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} te^{-\frac{t^2}{2}} \text{d}t + \mu \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} \text{d}t \\
    &= \mu
\end{aligned}
$$

\subsection{随机变量函数的数学期望}

\begin{theorem}
    设随机变量 $Y$ 是随机变量 $X$ 的函数，$Y=g(X)$，其中 $g$ 是一元连续函数.

    若 $X$ 是离散型随机变量，其概率分布为 $P\{X=x_k\} = p_k, \; k=1,2,\cdots$，如果无穷级数 $\displaystyle\sum_{k=1}^{\infty} g(x_k) p_k$ 绝对收敛，则随机变量 $Y$ 的数学期望为
    $$
    E(Y) = E[g(X)] = \sum_{k=1}^{\infty} g(x_k) \, p_k
    $$

    若 $X$ 是连续型随机变量，其概率密度为 $f(x)$，如果反常积分 $\displaystyle\int_{-\infty}^{+\infty} g(x) f(x) \, \text{d}x$ 绝对收敛，则随机变量 $Y$ 的数学期望为
    $$
    E(Y) = E[g(X)] = \int_{-\infty}^{+\infty} g(x) f(x) \, \text{d}x
    $$
\end{theorem}

\begin{theorem}
    设随机变量 $Z$ 是随机变量 $X$ 和 $Y$ 的函数，$Z=g(X,Y)$，其中 $g$ 是二元连续函数.

    若 $(X,Y)$ 是二维离散型随机变量，其概率分布为 $P\{X=x_i,Y=y_j\} = p_{ij}, \; i,j=1,2,\cdots$，如果无穷级数 $\displaystyle\sum_{j=1}^{\infty} \displaystyle\sum_{i=1}^{\infty} g(x_i,y_j) \, p_{ij}$ 绝对收敛，则随机变量 $Z$ 的数学期望为
    $$
    E(Z) = E[g(X,Y)] = \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} g(x_i,y_j) \, p_{ij}
    $$

    若 $(X,Y)$ 是二维连续型随机变量，其概率密度为 $f(x,y)$，如果反常积分 $\displaystyle\int_{-\infty}^{+\infty} \displaystyle\int_{-\infty}^{+\infty} g(x,y) f(x,y) \, \text{d}x \text{d}y$ 绝对收敛，则随机变量 $Z$ 的数学期望为
    $$
    E(Z) = E[g(X,Y)] = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x,y) f(x,y) \, \text{d}x \text{d}y
    $$
\end{theorem}

\subsection{数学期望的性质}

设 $C$ 为常数，$X$ 和 $Y$ 是随机变量，且 $E(X)$ 和 $E(Y)$ 都存在.

\setcounter{propertyname}{0}

\begin{property} \label{property:E(C)=C}
    $E(C)=C$
\end{property}

\begin{myproof}
    如果随机变量 $X$ 恒取常数 $C$，则有 $P\{X=C\}=1$，从而有 $E(C) = C \times 1 = C$.
\end{myproof}

\begin{property} \label{property:E(CX)=CE(X)}
    $E(CX)=CE(X)$
\end{property}

\begin{myproof}
    若离散型随机变量 $X$ 的概率分布为 $P\{X=x_i\}=p_i, \; i=1,2,\cdots$，则
    $$
    E(CX) = \sum_{i=1}^{\infty} C x_i p_i = C \sum_{i=1}^{\infty} x_i p_i = CE(X)
    $$

    若连续型随机变量 $X$ 的概率密度为 $f(x)$，则
    $$
    E(CX) = \int_{-\infty}^{+\infty} Cx f(x) \, \text{d}x = C \int_{-\infty}^{+\infty} x f(x) \, \text{d}x = CE(X)
    $$
\end{myproof}

\begin{property} \label{property:E(X+Y)=E(X)+E(Y)}
    $E(X+Y)=E(X)+E(Y)$
\end{property}

\begin{myproof}
    设二维连续型随机变量 $(X,Y)$ 的概率密度为 $f(x,y)$，$(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘概率密度分别为 $f_X(x)$ 和 $f_Y(y)$，则有
    $$
    \begin{aligned}
        E(X+Y) &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x+y) f(x,y) \, \text{d}x \text{d}y \\
        &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x f(x,y) \, \text{d}x \text{d}y + \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} y f(x,y) \, \text{d}x \text{d}y \\
        &= \int_{-\infty}^{+\infty} x \left[ \int_{-\infty}^{+\infty} f(x,y) \, \text{d}y \right] \text{d}x + \int_{-\infty}^{+\infty} y \left[ \int_{-\infty}^{+\infty} f(x,y) \, \text{d}x \right] \text{d}y \\
        &= \int_{-\infty}^{+\infty} x f_X(x) \, \text{d}x + \int_{-\infty}^{+\infty} y f_Y(y) \, \text{d}y \\
        &= E(X)+E(Y)
    \end{aligned}
    $$

    设二维离散型随机变量 $(X,Y)$ 的概率分布为 $P\{X=x_i,Y=y_j\} = p_{ij}, \; i,j=1,2,\cdots$，$(X,Y)$ 关于 $X$ 的边缘概率分布为 $P\{X=x_i\}=p_{i\cdot}, \; i=1,2,\cdots$，$(X,Y)$ 关于 $Y$ 的边缘概率分布为 $P\{Y=y_j\}=p_{\cdot j}, \; j=1,2,\cdots$，则有
    $$
    \begin{aligned}
        E(X+Y) &= \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} (x_i + y_j) \, p_{ij} \\
        &= \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} x_i p_{ij} + \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} y_j p_{ij} \\
        &= \sum_{i=1}^{\infty} \left( x_i \sum_{j=1}^{\infty} p_{ij} \right) + \sum_{j=1}^{\infty} \left( y_j \sum_{i=1}^{\infty} p_{ij} \right) \\
        &= \sum_{i=1}^{\infty} x_i p_{i \cdot} + \sum_{j=1}^{\infty} y_j p_{\cdot j} \\
        &= E(X)+E(Y)
    \end{aligned}
    $$
\end{myproof}

综合性质\ref*{property:E(C)=C}、\ref*{property:E(CX)=CE(X)}、\ref*{property:E(X+Y)=E(X)+E(Y)}，有
$$
E(aX+bY+c) = aE(X) + bE(Y) + c \quad (a,b,c \, \text{均为常数})
$$

\begin{property} \label{prop:E(XY)=E(X)E(Y)}
    若随机变量 $X$ 和 $Y$ 相互独立，则有 $E(XY)=E(X) \, E(Y)$.
\end{property}

\begin{myproof}
    设二维连续型随机变量 $(X,Y)$ 的概率密度为 $f(x,y)$，$(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘概率密度分别为 $f_X(x)$ 和 $f_Y(y)$. 因为 $X$ 和 $Y$ 相互独立，所以 $f(x,y) = f_X(x) \, f_Y(y)$，从而有
    $$
    \begin{aligned}
        E(XY) &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} xy f(x,y) \, \text{d}x \text{d}y \\
        &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} xy f_X(x) \, f_Y(y) \, \text{d}x \text{d}y \\
        &= \int_{-\infty}^{+\infty} x f_X(x) \, \text{d}x \int_{-\infty}^{+\infty} y f_Y(y) \, \text{d}y \\
        &= E(X) \, E(Y)
    \end{aligned}
    $$

    设二维离散型随机变量 $(X,Y)$ 的概率分布为 $P\{X=x_i,Y=y_j\} = p_{ij}, \; i,j=1,2,\cdots$，$(X,Y)$ 关于 $X$ 的边缘概率分布为 $P\{X=x_i\}=p_{i\cdot}, \; i=1,2,\cdots$，$(X,Y)$ 关于 $Y$ 的边缘概率分布为 $P\{Y=y_j\}=p_{\cdot j}, \; j=1,2,\cdots$. 因为 $X$ 和 $Y$ 相互独立，所以 $p_{ij} = p_{i\cdot} \, p_{\cdot j}$，从而有
    $$
    \begin{aligned}
        E(XY) &= \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} x_i y_j p_{ij} \\
        &= \sum_{j=1}^{\infty} \sum_{i=1}^{\infty} x_i y_j p_{i\cdot} p_{\cdot j} \\
        &= \sum_{i=1}^{\infty} x_i p_{i\cdot} \sum_{j=1}^{\infty} y_j p_{\cdot j} \\
        &= E(X) \, E(Y)
    \end{aligned}
    $$
\end{myproof}

\begin{property}[（柯西-施瓦茨不等式）]
    对于两个随机变量 $X$ 和 $Y$，设 $E(X^2)$ 和 $E(Y^2)$ 都存在，则
    $$
    [E(XY)]^2 \leqslant E(X^2) E(Y^2)
    $$
\end{property}

\begin{myproof}
    对于任意实数 $t$，令 $g(t) = E[(X+tY)^2]$，则由数学期望的性质有
    $$
    g(t) = E[(X+tY)^2] = E(X^2 + 2tXY + t^2 Y^2) = E(X^2) + 2tE(XY) + t^2 E(Y^2)
    $$
    由于 $g(t) \geqslant 0$，所以有
    $$
    \varDelta = 4 [E(XY)]^2 - 4 E(X^2) E(Y^2) \leqslant 0
    $$
    从而
    $$
    [E(XY)]^2 \leqslant E(X^2) E(Y^2)
    $$
\end{myproof}

\section{方差}

\subsection{方差的概念}

\begin{definition} \label{def:variance}
    设 $X$ 是一个随机变量，如果 $E([X-E(X)]^2)$ 存在，则称之为随机变量 $X$ 的\textbf{方差}（variance），记作 $D(X)$ 或 $DX$，即
    $$
    D(X) = E([X-E(X)]^2)
    $$
    称 $\sqrt{D(X)}$ 为随机变量 $X$ 的\textbf{标准差}（standard deviation）或\textbf{均方差}，记作 $\sigma(X)$，即
    $$
    \sigma(X) = \sqrt{D(X)}
    $$
\end{definition}

随机变量 $X$ 的方差反映了 $X$ 与其数学期望 $E(X)$ 的偏离程度. 如果 $X$ 取值集中在 $E(X)$ 附近，则 $D(X)$ 较小；如果 $X$ 取值比较分散，则 $D(X)$ 较大.

如果 $X$ 是离散型随机变量，其概率分布为 $P\{X=x_k\} = p_k, \; k=1,2,\cdots$，则由定义\ref{def:variance}，有
$$
D(X) = E([X-E(X)]^2) = \sum_{k=1}^{\infty} [x_k - E(X)]^2 p_k
$$

如果 $X$ 是连续型随机变量，其概率密度为 $f(x)$，则由定义\ref{def:variance}，有
$$
D(X) = E([X-E(X)]^2) = \int_{-\infty}^{+\infty} [x - E(X)]^2 f(x) \, \text{d}x
$$

根据数学期望的性质，可得
$$
\begin{aligned}
    D(X) &= E([X-E(X)]^2) \\
    &= E(X^2 - 2XE(X) + [E(X)]^2) \\
    &= E(X^2) - 2[E(X)]^2 + [E(X)]^2 \\
    &= E(X^2) - [E(X)]^2
\end{aligned}
$$
即
\begin{equation}
    D(X) = E(X^2) - [E(X)]^2
\end{equation}

若随机变量 $X$ 服从参数为 $p$ 的(0-1)分布，由于 $E(X)=p$，所以有
$$
\begin{aligned}
    D(X) &= E([X-E(X)]^2) \\
    &= (0-p)^2 (1-p) + (1-p)^2 p \\
    &= p(1-p)
\end{aligned}
$$

若随机变量 $X \sim P(\lambda)$，则
$$
E(X^2) = \sum_{k=0}^{\infty} k^2 \dfrac{\lambda^k e^{-\lambda}}{k!} = \lambda \sum_{k=1}^{\infty} k \dfrac{\lambda^{k-1}}{(k-1)!} e^{-\lambda}
$$
记 $i=k-1$，则
$$
\begin{aligned}
    E(X^2) &= \lambda \sum_{i=0}^{\infty} (i+1) \dfrac{\lambda^i}{i!} e^{-\lambda} \\
    &= \lambda \sum_{i=0}^{\infty} i \dfrac{\lambda^i}{i!} e^{-\lambda} + \lambda \sum_{i=0}^{\infty} \dfrac{\lambda^i}{i!} e^{-\lambda} \\
    &= \lambda^2 + \lambda
\end{aligned}
$$
由于 $E(X) = \lambda$，因此
$$
D(X) = E(X^2) - [E(X)]^2 = \lambda^2 + \lambda - \lambda^2 = \lambda
$$

若随机变量 $X$ 服从参数为 $p$ 的几何分布，由于 $E(X) = \dfrac{1}{p}$，所以有
$$
\begin{aligned}
    D(X) &= E(X^2) - [E(X)]^2 \\
    &= \sum_{k=1}^{\infty} k^2 (1-p)^{k-1} p - \dfrac{1}{p^2} \\
    &= p \left[ \sum_{k=1}^{\infty} (k+1)k (1-p)^{k-1} - \sum_{k=1}^{\infty} k (1-p)^{k-1} \right] - \dfrac{1}{p^2} \\
    & \xlongequal{q=1-p} p \left[ \sum_{k=1}^{\infty} (k+1)k q^{k-1} - \sum_{k=1}^{\infty} k q^{k-1} \right] - \dfrac{1}{p^2} \\
    &= p \left( \dfrac{\text{d}^2}{\text{d}q^2} \sum_{k=1}^{\infty} q^{k+1} - \dfrac{\text{d}}{\text{d}q} \sum_{k=1}^{\infty} q^k \right) - \dfrac{1}{p^2} \\
    &= p \left( \dfrac{\text{d}^2}{\text{d}q^2} \dfrac{q^2}{1-q} - \dfrac{\text{d}}{\text{d}q} \dfrac{q}{1-q} \right) - \dfrac{1}{p^2} \\
    &= p \left[ \dfrac{2}{(1-q)^3} - \dfrac{1}{(1-q)^2} \right] - \dfrac{1}{p^2} \\
    &= p \left( \dfrac{2}{p^3} - \dfrac{1}{p^2} \right) - \dfrac{1}{p^2} \\
    &= \dfrac{1-p}{p^2}
\end{aligned}
$$

若随机变量 $X$ 在区间 $[a,b]$ 上服从均匀分布，则
$$
E(X^2) = \int_{-\infty}^{+\infty} x^2 f(x) \, \text{d}x = \int_a^b x^2 \dfrac{1}{b-a} \text{d}x = \left. \dfrac{x^3}{3(b-a)} \right|_a^b = \dfrac{b^2 + ab + a^2}{3}
$$
由于 $E(X) = \dfrac{a+b}{2}$，因此
$$
D(X) = E(X^2) - [E(X)]^2 = \dfrac{b^2 + ab + a^2}{3} - \dfrac{(a+b)^2}{4} = \dfrac{(b-a)^2}{12}
$$

若随机变量 $X$ 服从参数为 $\lambda \, (\lambda > 0)$ 的指数分布，由于 $E(X) = \dfrac{1}{\lambda}$，则
$$
\begin{aligned}
    E(X^2) &= \int_{-\infty}^{+\infty} x^2 f(x) \, \text{d}x \\
    &= \int_0^{+\infty} x^2 \lambda e^{-\lambda x} \, \text{d}x \\
    &= \left. -x^2 e^{-\lambda x} \right|_0^{+\infty} + \int_0^{+\infty} 2x e^{-\lambda x} \, \text{d}x \\
    &= \dfrac{2}{\lambda} \int_0^{+\infty} x \lambda e^{-\lambda x} \, \text{d}x \\
    &= \dfrac{2}{\lambda} E(X) \\
    &= \dfrac{2}{\lambda^2}
\end{aligned}
$$
因此
$$
D(X) = E(X^2) - [E(X)]^2 = \dfrac{2}{\lambda^2} - \dfrac{1}{\lambda^2} = \dfrac{1}{\lambda^2}
$$

若随机变量 $X \sim N(\mu,\sigma^2)$，则 $E(X)=\mu$，因此
$$
\begin{aligned}
    D(X) &= E([X-E(X)]^2) \\
    &= \int_{-\infty}^{+\infty} (x-\mu)^2 f(x) \, \text{d}x \\
    &= \int_{-\infty}^{+\infty} (x-\mu)^2 \dfrac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}} \text{d}x \\
    & \xlongequal{t = \frac{x-\mu}{\sigma}} \sigma^2 \int_{-\infty}^{+\infty} t^2 \dfrac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} \text{d}t \\
    &= -\sigma^2 \dfrac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} t \, \text{d} e^{-\frac{t^2}{2}} \\
    &= -\dfrac{\sigma^2}{\sqrt{2 \pi}} \left( \left. t e^{-\frac{t^2}{2}} \right|_{-\infty}^{+\infty} - \int_{-\infty}^{+\infty} e^{-\frac{t^2}{2}} \text{d}t \right) \\
    &= \dfrac{\sigma^2}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} e^{-\frac{t^2}{2}} \text{d}t \\
    &= \sigma^2
\end{aligned}
$$

\subsection{方差的性质}

设 $C$ 是常数，随机变量 $X$ 和 $Y$ 的方差 $D(X)$ 和 $D(Y)$ 都存在.

\setcounter{propertyname}{0}

\begin{property}
    $D(C) = 0$
\end{property}

\begin{myproof}
    $$
    D(C) = E([C-E(C)]^2) = E([C-C]^2) = E(0) = 0
    $$
\end{myproof}

\begin{property}
    $D(CX) = C^2 D(X)$
\end{property}

\begin{myproof}
    $$
    \begin{aligned}
        D(CX) &= E([CX - E(CX)]^2) \\
        &= E([CX - CE(X)]^2) \\
        &= E(C^2 [X-E(X)]^2) \\
        &= C^2 E([X-E(X)]^2) \\
        &= C^2 D(X)
    \end{aligned}
    $$
\end{myproof}

\begin{property}
    $D(X+C) = D(X)$
\end{property}

\begin{myproof}
    $$
    \begin{aligned}
        D(X+C) &= E([(X+C) - E(X+C)]^2) \\
        &= E([X + C - E(X) - C]^2) \\
        &= E([X-E(X)]^2) \\
        &= D(X)
    \end{aligned}
    $$
\end{myproof}

\begin{property} \label{prop:D(X+Y)=D(X)+D(Y)}
    如果随机变量 $X$ 和 $Y$ 相互独立，则有 $D(X \pm Y) = D(X)+D(Y)$.
\end{property}

\begin{myproof}
    $$
    \begin{aligned}
        D(X \pm Y) &= E([X \pm Y - E(X \pm Y)]^2) \\
        &= E([(X-E(X)) \pm (Y-E(Y))]^2) \\
        &= E([X-E(X)]^2) \pm 2E([X-E(X)][Y-E(Y)]) + E([Y-E(Y)]^2)
    \end{aligned}
    $$
    因为 $X$ 与 $Y$ 相互独立，故 $X-E(X)$ 与 $Y-E(Y)$ 也相互独立，又 $E(X-E(X)) = 0$，再由数学期望的性质\ref*{prop:E(XY)=E(X)E(Y)} 及方差的定义，得
    $$
    \begin{aligned}
        D(X \pm Y) &= D(X) \pm 2E(X-E(X)) \, E(Y-E(Y)) + D(Y) \\
        &= D(X) + D(Y)
    \end{aligned}
    $$
\end{myproof}

\begin{property} \label{prop:D(X)=0}
    随机变量 $X$ 的方差 $D(X)=0$ 的充分必要条件是 $X$ 以概率1取常数 $C$，即 $P\{X=C\}=1$，其中 $C=E(X)$.
\end{property}

若随机变量 $X \sim B(n,p)$，根据二项分布的意义可知，$p$ 为 $n$ 重伯努利试验中每次试验成功的概率. 引入随机变量
$$
X_k = \begin{cases}
    1 & \text{第 $k$ 次试验成功} \\
    0 & \text{第 $k$ 次试验不成功}
\end{cases} \quad k=1,2,\cdots,n
$$
则有
$$
X = X_1 + X_2 + \cdots + X_n
$$
由于 $X_k$ 只依赖于第 $k$ 次试验，而各次试验相互独立，于是 $X_1, X_2, \cdots, X_n$ 相互独立，且均服从参数为 $p$ 的(0-1)分布. 由于 $D(X_i) = p(1-p), \, i=1,2,\cdots,n$，所以根据方差的性质 \ref*{prop:D(X+Y)=D(X)+D(Y)}，有
$$
D(X) = \sum_{i=1}^n D(X_i) = np(1-p)
$$

\begin{table}
    \centering

    \begin{tabular}{c | c | c | c | c}
        \hline
        \textbf{分布} & \textbf{符号} & \textbf{概率分布/概率密度} & \textbf{数学期望} & \textbf{方差} \\
        \hline
        (0-1)分布 & $B(1,p)$ & $P\{X=k\}=p^k (1-p)^{1-k}, \; k=0,1, \; 0<p<1$ & $p$ & $p(1-p)$ \\
        \hline
        二项分布 & $B(n,p)$ & $P\{X=k\}=C_n^k p^k (1-p)^{n-k}, \; k=0,1,\cdots,n$ & $np$ & $np(1-p)$ \\
        \hline
        \rule{0pt}{24pt}泊松分布 & $P(\lambda)$ & $P\{X=k\} = \dfrac{\lambda^k e^{-\lambda}}{k!}, \; k=0,1,2\cdots$ & $\lambda$ & $\lambda$ \\[8pt]
        \hline
        \rule{0pt}{24pt}几何分布 & - & $P\{X=k\}=(1-p)^{k-1} p, \; k=1,2,\cdots$ & $\dfrac{1}{p}$ & $\dfrac{1-p}{p^2}$ \\[8pt]
        \hline
        \rule{0pt}{36pt}均匀分布 & $U(a,b)$ & $f(x)=\begin{cases}
            \dfrac{1}{b-a} & a<x<b \\[0.5em]
            0 & \text{其他}
        \end{cases}$ & $\dfrac{a+b}{2}$ & $\dfrac{(b-a)^2}{12}$ \\[8pt]
        \hline
        \rule{0pt}{35pt}指数分布 & - & $f(x)=\begin{cases}
            \lambda e^{-\lambda x} & x>0 \\
            0 & x \leqslant 0
        \end{cases}$ & $\dfrac{1}{\lambda}$ & $\dfrac{1}{\lambda^2}$ \\[8pt]
        \hline
        \rule{0pt}{24pt}正态分布 & $N(\mu, \sigma^2)$ & $f(x) = \dfrac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}, \; -\infty < x < +\infty$ & $\mu$ & $\sigma^2$ \\[8pt]
        \hline
    \end{tabular}
\end{table}

\subsection{随机变量的标准化}

设随机变量 $X$ 具有数学期望 $E(X)=\mu$ 及方差 $D(X) = \sigma^2 > 0$，则称 $X^* = \dfrac{X-\mu}{\sigma}$ 为 $X$ 的\textbf{标准化随机变量}.

随机变量 $X$ 的标准化随机变量 $X^*$ 满足 $E(X^*)=0$，$D(X^*)=1$.

若 $X \sim N(\mu,\sigma^2) \, (\sigma > 0)$，则 $X^* = \dfrac{X-\mu}{\sigma} \sim N(0,1)$.

\section{协方差与相关系数}

\subsection{协方差}

在方差性质 \ref*{prop:D(X+Y)=D(X)+D(Y)} 的证明中可知，如果两个随机变量 $X$ 和 $Y$ 相互独立，则有 \\
$E([X-E(X)][Y-E(Y)]) = 0$. 这表明，当 $E([X-E(X)][Y-E(Y)]) \not= 0$ 时，$X$ 与 $Y$ 不相互独立，因此可以用这个量来描述 $X$ 和 $Y$ 之间的关系.

\begin{definition}
    设随机变量 $X$ 和 $Y$ 的数学期望 $E(X)$ 和 $E(Y)$ 都存在，如果 $E([X-E(X)][Y-E(Y)])$ 存在，则称之为随机变量 $X$ 和 $Y$ 的\textbf{协方差}（covariance），记作 $\operatorname{Cov}(X,Y)$，即
    $$
    \operatorname{Cov}(X,Y) = E([X-E(X)][Y-E(Y)]) = E(XY) - E(X) E(Y)
    $$
\end{definition}

\setcounter{propertyname}{0}

\begin{property}
    $\operatorname{Cov}(X,Y) = \operatorname{Cov}(Y,X)$
\end{property}

\begin{property}
    对于常数 $a$ 和 $b$，有 $\operatorname{Cov}(aX,bY) = ab \operatorname{Cov}(X,Y)$.
\end{property}

\begin{myproof}
    $$
    \begin{aligned}
        \operatorname{Cov}(aX,bY) &= E(aX \cdot bY) - E(aX) E(bY) \\
        &= ab E(XY) - ab E(X) E(Y) \\
        &= ab[E(XY) - E(X) E(Y)] \\
        &= ab \operatorname{Cov}(X,Y)
    \end{aligned}
    $$
\end{myproof}

\begin{property}
    对于随机变量 $X,Y$ 和 $Z$，有
    $$
    \operatorname{Cov}(X+Y,Z) = \operatorname{Cov}(X,Z) + \operatorname{Cov}(Y,Z)
    $$
\end{property}

\begin{myproof}
    $$
    \begin{aligned}
        \operatorname{Cov}(X+Y,Z) &= E((X+Y)Z) - E(X+Y) E(Z) \\
        &= E(XZ) + E(YZ) - E(X) E(Z) - E(Y) E(Z) \\
        &= [E(XZ) - E(X) E(Z)] + [E(YZ) - E(Y) E(Z)] \\
        &= \operatorname{Cov}(X,Z) + \operatorname{Cov}(Y,Z)
    \end{aligned}
    $$
\end{myproof}

\begin{property}
    对任意随机变量 $X,Y$，有
    $$
    D(X \pm Y) = D(X) + D(Y) \pm 2 \operatorname{Cov}(X,Y)
    $$
\end{property}

\subsection{相关系数}

\begin{definition}
    设随机变量 $X$ 和 $Y$ 的方差都存在且不等于零，协方差 $\operatorname{Cov}(X,Y)$ 存在，称 $\dfrac{\operatorname{Cov}(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}}$ 为随机变量 $X$ 和 $Y$ 的\textbf{相关系数}（correlation coefficient），记作 $\rho_{XY}$，即
    $$
    \rho_{XY} = \dfrac{\operatorname{Cov}(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}}
    $$
    当 $\rho_{XY} = 0$ 时，称 $X$ 与 $Y$ \textbf{不相关}.
\end{definition}

\setcounter{propertyname}{0}

\begin{property}
    $|\rho_{XY}| \leqslant 1$
\end{property}

\begin{myproof}
    由柯西-施瓦茨不等式可得
    $$
    \begin{aligned}
        \left[ \operatorname{Cov}(X,Y) \right]^2 &= [E([X-E(X)][Y-E(Y)])]^2 \\
        & \leqslant E([X-E(X)]^2) E([Y-E(Y)]^2) \\
        &= D(X) D(Y)
    \end{aligned}
    $$
    因此 $|\operatorname{Cov}(X,Y)| \leqslant \sqrt{D(X)} \sqrt{D(Y)}$，从而有
    $$
    |\rho_{XY}| = \left| \dfrac{\operatorname{Cov}(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} \right| \leqslant 1
    $$
\end{myproof}

\begin{property}
    如果随机变量 $X$ 和 $Y$ 相互独立，则 $\rho_{XY} = 0$.
\end{property}

\begin{myproof}
    若 $X$ 和 $Y$ 相互独立，则 $\operatorname{Cov}(X,Y) = 0$，进而 $\rho_{XY} = 0$.
\end{myproof}

\begin{property}
    $|\rho_{XY}|=1$ 的充分必要条件是：存在常数 $a,b$，使得 $P\{Y=a+bX\}=1$.
\end{property}

\begin{myproof}
    设 $D(X) = \sigma_X^2 > 0$，$D(Y) = \sigma_Y^2 > 0$. 对于任意实数 $b$，有
    $$
    \begin{aligned}
        D(Y-bX) &= E([Y-bX - E(Y-bX)]^2) \\
        &= E( \{[Y-E(Y)] - b[X-E(X)]\}^2 ) \\
        &= E([Y-E(Y)]^2) - 2bE([Y-E(Y)][X-E(X)]) + b^2 E([X-E(X)]^2) \\
        &= \sigma_Y^2 - 2b \operatorname{Cov}(X,Y) + b^2 \sigma_X^2
    \end{aligned}
    $$
    取 $b = \dfrac{\operatorname{Cov}(X,Y)}{\sigma_X^2}$，则有
    $$
    \begin{aligned}
        D(Y-bX) &= \sigma_Y^2 - \dfrac{2 [\operatorname{Cov}(X,Y)]^2}{\sigma_X^2} + \dfrac{[\operatorname{Cov}(X,Y)]^2}{\sigma_X^2} \\
        &= \sigma_Y^2 - \dfrac{[\operatorname{Cov}(X,Y)]^2}{\sigma_X^2} \\
        &= \sigma_Y^2 \left\{ 1 - \dfrac{[\operatorname{Cov}(X,Y)]^2}{\sigma_X^2 \sigma_Y^2} \right\} \\
        &= \sigma_Y^2 (1 - \rho_{XY}^2)
    \end{aligned}
    $$
    由此得 $|\rho_{XY}|=1$ 的充分必要条件是 $D(Y-bX)=0$. 根据方差的性质\ref*{prop:D(X)=0}，$D(Y-bX)=0$ 的充分必要条件是 $Y-bX$ 以概率 1 取常数 $a=E(Y-bX)$，即
    $$
    P \{ Y-bX=a \} = 1
    $$
    亦即
    $$
    P\{Y=a+bX\}=1
    $$
\end{myproof}

相关系数表示随机变量 $X$ 和 $Y$ 线性相关的程度. 当 $|\rho_{XY}|=1$ 时，$X$ 与 $Y$ 之间以概率 1 存在线性关系；当 $|\rho_{XY}|$ 较大时，称 $X$ 与 $Y$ 线性相关的程度较好；当 $|\rho_{XY}|$ 较小时，称 $X$ 与 $Y$ 线性相关的程度较差. 当 $\rho_{XY} > 0$ 时，称 $X$ 与 $Y$ 正相关，这时随着 $X$ 的增加，$Y$ 的值也有增加的趋势；当 $\rho_{XY} < 0$ 时，称 $X$ 与 $Y$ 负相关，这时随着 $X$ 的增加，$Y$ 的值有减小的趋势.

如果 $X$ 与 $Y$ 相互独立，则 $\rho_{XY} = 0$，即 $X$ 与 $Y$ 不相关. 反之，如果 $X$ 与 $Y$ 不相关，则 $X$ 和 $Y$ 之间不存在线性关系，但 $X$ 与 $Y$ 未必独立，二者可能存在其他关系.

对于随机变量 $X$ 和 $Y$，下列命题是等价的：
\begin{enumerate}
    \item $\operatorname{Cov}(X,Y) = 0$.
    \item $X$ 与 $Y$ 不相关.
    \item $E(XY) = E(X) E(Y)$.
    \item $D(X+Y) = D(X) + D(Y)$.
\end{enumerate}

\begin{conclusion}
    若二维随机变量 $(X,Y)$ 服从二维正态分布 $N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$，则 $X$ 与 $Y$ 的相关系数 $\rho_{XY} = \rho$.
\end{conclusion}

\begin{myproof}
    $(X,Y)$ 的概率密度为
    $$
    f(x,y) = \dfrac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} e^{\frac{-1}{2(1-\rho^2)} \left[ \frac{(x-\mu_1)^2}{\sigma_1^2} - 2 \rho \frac{(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2} \right]},\ (x,y)\in \mathbf{R}^2
    $$
    $(X,Y)$ 关于 $X$ 和关于 $Y$ 的边缘概率密度分别为
    \begin{gather*}
        f_{X}(x) = \dfrac{1}{\sqrt{2\pi} \sigma_1} e^{-\frac{(x-\mu_1)^2}{2 \sigma_1^2}}, -\infty < x < +\infty \\
        f_{Y}(y) = \dfrac{1}{\sqrt{2\pi} \sigma_2} e^{-\frac{(y-\mu_2)^2}{2 \sigma_2^2}}, -\infty < y < +\infty
    \end{gather*}
    因此 $E(X) = \mu_1, E(Y) = \mu_2, D(X) = \sigma_1^2, D(Y) = \sigma_2^2$.
    \begin{small}
    $$
    \begin{aligned}
        \operatorname{Cov}(X,Y) &= E([X-E(X)][Y-E(Y)]) \\
        &= \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x-\mu_1)(y-\mu_2) f(x,y) \, \text{d}x \text{d}y \\
        &= \dfrac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x-\mu_1)(y-\mu_2) e^{\frac{-1}{2(1-\rho^2)} \left[ \frac{(x-\mu_1)^2}{\sigma_1^2} - 2 \rho \frac{(x-\mu_1)(y-\mu_2)}{\sigma_1 \sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2} \right]} \text{d}x \text{d}y \\
        &= \dfrac{1}{2 \pi \sigma_1 \sigma_2 \sqrt{1-\rho^2}} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} (x-\mu_1)(y-\mu_2) e^{-\frac{(x-\mu_1)^2}{2 \sigma_1^2}} e^{-\frac{1}{2(1-\rho^2)} \left( \frac{y-\mu_2}{\sigma_2} - \rho \frac{x-\mu_1}{\sigma_1} \right)^2} \text{d}x \text{d}y
    \end{aligned}
    $$
    \end{small}
    令 $t = \dfrac{1}{\sqrt{1-\rho^2}} \left( \dfrac{y-\mu_2}{\sigma_2} - \rho \dfrac{x-\mu_1}{\sigma_1} \right)$，$u = \dfrac{x-\mu_1}{\sigma_1}$，则有
    $$
    \begin{aligned}
        \operatorname{Cov}(X,Y) = & \; \dfrac{1}{2 \pi} \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \sigma_1 \sigma_2 (\sqrt{1-\rho^2} tu + \rho u^2) e^{-\frac{t^2}{2}} e^{-\frac{u^2}{2}} \text{d}t \text{d}u \\
        = & \; \dfrac{\sigma_1 \sigma_2 \sqrt{1-\rho^2}}{2 \pi} \int_{-\infty}^{+\infty} t e^{-\frac{t^2}{2}} \text{d}t \int_{-\infty}^{+\infty} u e^{-\frac{u^2}{2}} \text{d}u + \\
        & \; \rho \sigma_1 \sigma_2 \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} \text{d}t \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} u^2 e^{-\frac{u^2}{2}} \text{d}u
    \end{aligned}
    $$
    而
    $$
    \begin{aligned}
        & \int_{-\infty}^{+\infty} t e^{-\frac{t^2}{2}} \text{d}t = \left. -e^{-\frac{t^2}{2}} \right|_{-\infty}^{+\infty} = 0 \\
        & \int_{-\infty}^{+\infty} u e^{-\frac{u^2}{2}} \text{d}u = 0 \\
        & \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} e^{-\frac{t^2}{2}} \text{d}t = 1 \\
        & \int_{-\infty}^{+\infty} \dfrac{1}{\sqrt{2 \pi}} u^2 e^{-\frac{u^2}{2}} \text{d}u = 1
    \end{aligned}
    $$
    因此
    $$
    \operatorname{Cov}(X,Y) = \rho \sigma_1 \sigma_2
    $$
    所以
    $$
    \rho_{XY} = \dfrac{\operatorname{Cov}(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \dfrac{\rho \sigma_1 \sigma_2}{\sigma_1 \sigma_2} = \rho
    $$
\end{myproof}

若 $(X,Y) \sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$，则 $X$ 与 $Y$ 相互独立的充分必要条件是 $\rho=0$. 由于 $\rho_{XY} = \rho$，所以当 $(X,Y) \sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$ 时，$X$ 与 $Y$ 相互独立的充分必要条件是 $X$ 与 $Y$ 不相关.

由于二维正态随机变量 $(X,Y)$ 的概率密度中的参数 $\rho$ 就是 $X$ 与 $Y$ 的相关系数，因此二维正态随机变量 $(X,Y)$ 的分布完全由 $X$ 和 $Y$ 的数学期望、方差以及 $X$ 与 $Y$ 的相关系数确定.

\section{矩}

\subsection{矩的概念}

\begin{definition}
    设 $X$ 和 $Y$ 是随机变量. 如果 $E(X^k), \, k=1,2,\cdots$ 存在，则称之为随机变量 $X$ 的 $k$ \textbf{阶原点矩}，记作 $E(X^k) = \mu_k \, (k=1,2,\cdots)$.

    如果 $E([X-E(X)]^k), \, k=1,2,\cdots$ 存在，则称之为随机变量 $X$ 的 $k$ \textbf{阶中心矩}.

    如果 $E(X^k Y^l), \; k,l=1,2,\cdots$ 存在，则称之为随机变量 $X$ 和 $Y$ 的 $k+l$ \textbf{阶混合原点矩}.

    如果 $E([X-E(X)]^k [Y-E(Y)]^l), \; k,l=1,2,\cdots$ 存在，则称之为随机变量 $X$ 和 $Y$ 的 $k+l$ \textbf{阶混合中心矩}.
\end{definition}

随机变量 $X$ 的数学期望 $E(X)$ 是 $X$ 的一阶原点矩，方差 $D(X)$ 是 $X$ 的二阶中心矩，随机变量 $X$ 和 $Y$ 的协方差 $\operatorname{Cov}(X,Y)$ 是 $X$ 和 $Y$ 的二阶混合中心矩.

\subsection{协方差矩阵}

\begin{definition}
    设二维随机变量 $(X_1,X_2)$ 关于 $X_1$ 和 $X_2$ 的二阶中心距和二阶混合中心距
    $$
    c_{ij} = E([X_i-E(X_i)][X_j-E(X_j)]), \; i,j=1,2
    $$
    都存在，则称矩阵
    $$
    \boldsymbol{C} = \begin{bmatrix}
        c_{11} & c_{12} \\
        c_{21} & c_{22}
    \end{bmatrix}
    $$
    为二维随机变量 $(X_1,X_2)$ 的\textbf{协方差矩阵}.

    设 $n$ 维随机变量 $(X_1,X_2,\cdots,X_n)$ 关于 $X_1,X_2,\cdots,X_n$ 的二阶中心距和二阶混合中心距
    $$
    c_{ij} = E([X_i-E(X_i)][X_j-E(X_j)]), \; i,j=1,2,\cdots,n
    $$
    都存在，则称矩阵
    $$
    \boldsymbol{C} = \begin{bmatrix}
        c_{11} & c_{12} & \cdots & c_{1n} \\
        c_{21} & c_{22} & \cdots & c_{2n} \\
        \vdots & \vdots & & \vdots \\
        c_{n1} & c_{n2} & \cdots & c_{nn} \\
    \end{bmatrix}
    $$
    为 $n$ 维随机变量 $(X_1,X_2,\cdots,X_n)$ 的\textbf{协方差矩阵}.
\end{definition}

由于 $c_{ij}=c_{ji} (i \not= j, \; i,j=1,2,\cdots,n)$，所以 $\boldsymbol{C}$ 是对称矩阵.

\subsection{$n$ 维正态分布}

设二维随机变量 $(X_1,X_2) \sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$，则
$$
E(X_1) = \mu_1, \; E(X_2) = \mu_2, \; D(X_1) = \sigma_1^2, \; D(X_2) = \sigma_2^2
$$
又由于 $\operatorname{Cov}(X_1,X_2) = \operatorname{Cov}(X_2,X_1) = \rho \sigma_1 \sigma_2$，从而有
$$
c_{11} = \sigma_1^2, \; c_{12} = c_{21} = \rho \sigma_1 \sigma_2, \; c_{22} = \sigma_2^2
$$
所以 $(X_1,X_2)$ 的协方差矩阵为
$$
\boldsymbol{C} = \begin{bmatrix}
    \sigma_1^2 & \rho \sigma_1 \sigma_2 \\
    \rho \sigma_1 \sigma_2 & \sigma_2^2
\end{bmatrix}
$$
其行列式 $|\boldsymbol{C}| = \sigma_1^2 \sigma_2^2 (1-\rho^2)$，$\boldsymbol{C}$ 的逆矩阵为
$$
\boldsymbol{C}^{-1} = \dfrac{1}{|\boldsymbol{C}|} \begin{bmatrix}
    \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\
    -\rho \sigma_1 \sigma_2 & \sigma_1^2
\end{bmatrix}
$$
令
$$
\boldsymbol{X} = \begin{bmatrix}
    x_1 \\
    x_2
\end{bmatrix}, \;
\boldsymbol{\mu} = \begin{bmatrix}
    \mu_1 \\
    \mu_2
\end{bmatrix}
$$
则
$$
\begin{aligned}
    (\boldsymbol{X} - \boldsymbol{\mu})^{\text{T}} \boldsymbol{C}^{-1} (\boldsymbol{X} - \boldsymbol{\mu}) &= \dfrac{1}{|\boldsymbol{C}|}
    \begin{bmatrix}
        x_1-\mu_1 & x_2-\mu_2
    \end{bmatrix}
    \begin{bmatrix}
        \sigma_2^2 & -\rho \sigma_1 \sigma_2 \\
        -\rho \sigma_1 \sigma_2 & \sigma_1^2
    \end{bmatrix}
    \begin{bmatrix}
        x_1-\mu_1 \\
        x_2-\mu_2
    \end{bmatrix} \\
    &= \dfrac{1}{1-\rho^2} \left[ \dfrac{(x_1-\mu_1)^2}{\sigma_1^2} - 2\rho \dfrac{(x_1-\mu_1)(x_2-\mu_2)}{\sigma_1 \sigma_2} + \dfrac{(x_2-\mu_2)^2}{\sigma_2^2} \right]
\end{aligned}
$$
因此二维正态随机变量 $(X_1,X_2)$ 的概率密度可以写成
$$
f(x_1,x_2) = \dfrac{1}{(2\pi)^{\frac{2}{2}} |\boldsymbol{C}|^{\frac{1}{2}}} e^{-\frac{1}{2} (\boldsymbol{X} - \boldsymbol{\mu})^{\text{T}} \boldsymbol{C}^{-1} (\boldsymbol{X} - \boldsymbol{\mu})}
$$

设 $(X_1,X_2,\cdots,X_n)$ 为 $n$ 维随机变量，记
$$
\boldsymbol{X} = \begin{bmatrix}
    x_1 \\
    x_2 \\
    \vdots \\
    x_n
\end{bmatrix}, \;
\boldsymbol{\mu} = \begin{bmatrix}
    \mu_1 \\
    \mu_2 \\
    \vdots \\
    \mu_n
\end{bmatrix} = \begin{bmatrix}
    E(X_1) \\
    E(X_2) \\
    \vdots \\
    E(X_n)
\end{bmatrix}
$$
如果 $(X_1,X_2,\cdots,X_n)$ 具有概率密度
$$
f(x_1,x_2,\cdots,x_n) = \dfrac{1}{(2\pi)^{\frac{n}{2}} |\boldsymbol{C}|^{\frac{1}{2}}} e^{-\frac{1}{2} (\boldsymbol{X} - \boldsymbol{\mu})^{\text{T}} \boldsymbol{C}^{-1} (\boldsymbol{X} - \boldsymbol{\mu})}
$$
其中 $\boldsymbol{C}$ 为 $(X_1,X_2,\cdots,X_n)$ 的协方差矩阵，则称 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布.

\setcounter{propertyname}{0}

\begin{property}
    $n$ 维随机变量 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布的充分必要条件是：\\
    $X_1,X_2,\cdots,X_n$ 的任意线性组合 $k_1 X_1 + k_2 X_2 + \cdots + k_n X_n$ 都服从一维正态分布，其中 $k_1,k_2,\cdots,k_n$ 是不全为零的常数.
\end{property}

\begin{property}
    设 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布. 如果 $Y_1,Y_2,\cdots,Y_m$ 是 $X_i \, (i=1,2,\cdots,n)$ 的线性函数，则 $(Y_1,Y_2,\cdots,Y_m)$ 也服从多维正态分布.
\end{property}

\begin{property}
    如果 $(X_1,X_2,\cdots,X_n)$ 服从 $n$ 维正态分布，则“随机变量 $X_1,X_2,\cdots,X_n$ 相互独立”与“$X_1,X_2,\cdots,X_n$ 两两不相关”等价.
\end{property}