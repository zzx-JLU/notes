---
title: 机器学习
chrome:
    format: "A4"
    headerTemplate: '<div></div>'
    footerTemplate: '<div style="width:100%; text-align:center; border-top: 1pt solid #eeeeee; margin: 10px 10px 20px; font-size: 8pt;">
    <span class=pageNumber></span> / <span class=totalPages></span></div>'
    displayHeaderFooter: true
    margin:
        top: '40px'
        bottom: '80px'
        left: '60px'
        right: '60px'
---

<h1>机器学习</h1>

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 基础概念](#1-基础概念)
- [2. 线性回归](#2-线性回归)
  - [2.1 单变量线性回归](#21-单变量线性回归)
    - [2.1.1 模型定义](#211-模型定义)
    - [2.1.2 代价函数](#212-代价函数)
    - [2.1.3 梯度下降](#213-梯度下降)

<!-- /code_chunk_output -->

# 1. 基础概念

机器学习（Machine Learning）：不需要明确编程就能让计算机具有学习能力的研究领域。（Arthur Samuel，1959）

机器学习的分类：

- 监督学习（Supervised learning）：学习从输入 $x$ 到输出 $y$ 的映射。
  - 分类（Classification）：标签 $y$ 是离散的、数量有限的。
  - 回归（Regression）：标签 $y$ 是实数，有无限多种可能的取值。
- 无监督学习（Unsupervised learning）：训练数据中只有输入 $x$，没有输出 $y$。
  - 聚类（Clustering）：将相似的数据点聚在一起。
  - 异常检测（Anomaly detection）：检测异常的数据点。
  - 降维（Dimensionality reduction）：压缩数据，降低数据维度。

# 2. 线性回归

## 2.1 单变量线性回归

### 2.1.1 模型定义

基本概念：

- 训练集（Training set）：用于训练模型的数据。
- 输入（input）：用 $x$ 表示，也称为“特征”（feature）。
- 输出（output）：用 $y$ 表示，也称为“目标”（target）。
- 样本（sample）：训练集中的一条数据，用 $(x,y)$ 表示。
- 第 $i$ 个样本：用 $(x^{(i)}, y^{(i)})$ 表示。
- 训练集的大小：训练集中的样本数，用 $m$ 表示。

将训练集输入到学习算法中，学习的结果是一个函数 $f$，称为**模型**（model）。将特征 $x$ 输入到模型中，可以得到目标 $y$ 的估计值（或预测值），记作 $\hat{y}$。

<div align="center">
    <img src="https://github.com/zzx-JLU/images_for_markdown/raw/main/机器学习-吴恩达/图2.1.png" alt="图2.1" width="50%" />
</div>

设计学习算法的关键问题是如何表示函数 $f$，不同的函数 $f$ 有不同的学习算法。如果 $f$ 的数学表达式为

$$
f_{w,b}(x) = wx+b
$$

此时 $f$ 表示一条直线，其中 $x,w,b$ 是实数，$x$ 表示输入特征，$w$ 和 $b$ 称为模型的**参数**（parameter）。给定一组 $w$ 和 $b$ 的取值，就可以唯一确定一个函数 $f$，进而对于任意的输入 $x$ 确定唯一的的预测值 $\hat{y}$。这样的模型称为**单变量线性回归**（Linear regression with one variable，或 Univariate linear regression）。

### 2.1.2 代价函数

对于训练集中的任意一个样本 $(x^{(i)}, y^{(i)})$，将 $x^{(i)}$ 输入模型，可得
$$
\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = w x^{(i)} + b
$$

我们希望预测值 $\hat{y}^{(i)}$ 与标签 $y^{(i)}$ 尽可能地接近，为此需要寻找合适的 $w$ 和 $b$。

为了衡量预测值与标签的吻合程度，定义如下概念：

- **误差**（error）：$\hat{y}^{(i)} - y^{(i)}$
- **均方误差**：$\dfrac{1}{2m} \displaystyle\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2$
- **平方误差代价函数**（Squared error cost function）：$J(w,b) = \dfrac{1}{2m} \displaystyle\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2$

不同的应用场景会使用不同的代价函数，平方误差代价函数在线性回归中最为常用。

代价函数 $J(w,b)$ 是关于模型参数 $w$ 和 $b$ 的函数。对于一个给定的训练集，选择一组 $w$ 和 $b$ 的值就可以确定一个模型，进而确定代价函数的值。学习算法的目的是寻找参数 $w$ 和 $b$，使得代价函数 $J(w,b)$ 最小。

### 2.1.3 梯度下降
