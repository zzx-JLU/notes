---
title: 机器学习
chrome:
    format: "A4"
    headerTemplate: '<div></div>'
    footerTemplate: '<div style="width:100%; text-align:center; border-top: 1pt solid #eeeeee; margin: 10px 10px 20px; font-size: 8pt;">
    <span class=pageNumber></span> / <span class=totalPages></span></div>'
    displayHeaderFooter: true
    margin:
        top: '40px'
        bottom: '80px'
        left: '60px'
        right: '60px'
---

<h1>机器学习</h1>

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 基础概念](#1-基础概念)
- [2. 线性回归模型](#2-线性回归模型)
  - [2.1 单变量线性回归](#21-单变量线性回归)
    - [2.1.1 模型定义](#211-模型定义)
    - [2.1.2 代价函数](#212-代价函数)
    - [2.1.3 梯度下降](#213-梯度下降)
    - [2.1.4 学习算法](#214-学习算法)
  - [2.2 多元线性回归](#22-多元线性回归)
    - [2.2.1 模型定义](#221-模型定义)
    - [2.2.2 特征缩放](#222-特征缩放)
    - [2.2.3 判断梯度下降是否收敛](#223-判断梯度下降是否收敛)
    - [2.2.4 学习率的设置](#224-学习率的设置)
    - [2.2.5 特征工程](#225-特征工程)
- [3 逻辑回归](#3-逻辑回归)
  - [3.1 二元分类](#31-二元分类)
  - [3.2 逻辑回归](#32-逻辑回归)

<!-- /code_chunk_output -->

# 1. 基础概念

机器学习（Machine Learning）：不需要明确编程就能让计算机具有学习能力的研究领域。（Arthur Samuel，1959）

机器学习的分类：

- 监督学习（Supervised learning）：学习从输入 $x$ 到输出 $y$ 的映射。
  - 分类（Classification）：标签 $y$ 是离散的、数量有限的。
  - 回归（Regression）：标签 $y$ 是实数，有无限多种可能的取值。
- 无监督学习（Unsupervised learning）：训练数据中只有输入 $x$，没有输出 $y$。
  - 聚类（Clustering）：将相似的数据点聚在一起。
  - 异常检测（Anomaly detection）：检测异常的数据点。
  - 降维（Dimensionality reduction）：压缩数据，降低数据维度。

# 2. 线性回归模型

## 2.1 单变量线性回归

### 2.1.1 模型定义

基本概念：

- 训练集（Training set）：用于训练模型的数据。
- 输入（input）：用 $x$ 表示，也称为“特征”（feature）。
- 输出（output）：用 $y$ 表示，也称为“目标”（target）。
- 样本（sample）：训练集中的一条数据，用 $(x,y)$ 表示。
- 第 $i$ 个样本：用 $(x^{(i)}, y^{(i)})$ 表示。
- 训练集的大小：训练集中的样本数，用 $m$ 表示。

将训练集输入到学习算法中，学习的结果是一个函数 $f$，称为**模型**（model）。将特征 $x$ 输入到模型中，可以得到目标 $y$ 的估计值（或预测值），记作 $\hat{y}$。

<div align="center">
    <img src="https://github.com/zzx-JLU/images_for_markdown/raw/main/机器学习-吴恩达/图2.1.png" alt="图2.1" width="50%" />
</div>

设计学习算法的关键问题是如何表示函数 $f$，不同的函数 $f$ 有不同的学习算法。如果 $f$ 的数学表达式为

$$
f_{w,b}(x) = wx+b
$$

此时 $f$ 表示一条直线，其中 $x,w,b$ 是实数，$x$ 表示输入特征，$w$ 和 $b$ 称为模型的**参数**（parameter）。给定一组 $w$ 和 $b$ 的取值，就可以唯一确定一个函数 $f$，进而对于任意的输入 $x$ 确定唯一的的预测值 $\hat{y}$。这样的模型称为**单变量线性回归**（Linear regression with one variable，或 Univariate linear regression）。

### 2.1.2 代价函数

对于训练集中的任意一个样本 $(x^{(i)}, y^{(i)})$，将 $x^{(i)}$ 输入模型，可得
$$
\hat{y}^{(i)} = f_{w,b}(x^{(i)}) = w x^{(i)} + b
$$

我们希望预测值 $\hat{y}^{(i)}$ 与标签 $y^{(i)}$ 尽可能地接近，为此需要寻找合适的 $w$ 和 $b$。

为了衡量预测值与标签的吻合程度，定义如下概念：

- **误差**（error）：$\hat{y}^{(i)} - y^{(i)}$
- **均方误差**：$\dfrac{1}{2m} \displaystyle\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2$
- **平方误差代价函数**（Squared error cost function）：$J(w,b) = \dfrac{1}{2m} \displaystyle\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2$

不同的应用场景会使用不同的代价函数，平方误差代价函数在线性回归中最为常用。

代价函数 $J(w,b)$ 是关于模型参数 $w$ 和 $b$ 的函数。对于一个给定的训练集，选择一组 $w$ 和 $b$ 的值就可以确定一个模型，进而确定代价函数的值。学习算法的目的是寻找参数 $w$ 和 $b$，使得代价函数 $J(w,b)$ 最小。

### 2.1.3 梯度下降

**梯度下降**（Gradient Desent）用于调整模型参数，使得代价函数最小。事实证明，梯度下降可用于任意代价函数的最小化。

对于单变量线性回归，其代价函数为 $J(w,b)$，对它进行梯度下降的过程为：

1. 给定参数 $w$ 和 $b$ 的初始值（通常设为 0）。
2. 改变 $w$ 和 $b$ 的值，更新规则为
$$
\begin{aligned}
    & w' = w - \alpha \dfrac{\partial}{\partial w} J(w,b) \\[0.5em]
    & b' = b - \alpha \dfrac{\partial}{\partial b} J(w,b)
\end{aligned}
$$
3. 不断改变 $w$ 和 $b$ 的值，尝试降低代价 $J(w,b)$。每次改变参数称为一次**迭代**（iteration）。
4. 代价函数到达或接近最小值后，参数 $w$ 和 $b$ 的变化幅度会变得非常小，此时模型**收敛**（converge）。

在梯度下降中，导数项决定了下降的方向；$\alpha$ 称为**学习率**（learning rate），通常为 0~1 之间的正数，它决定了每次调整参数的幅度大小。

学习率 $\alpha$ 的选择对梯度下降的效果具有重要影响。如果 $\alpha$ 太小，会使梯度下降的过程变慢，降低收敛速度；如果 $\alpha$ 太大，可能导致模型无法收敛，甚至可能发散。

当接近局部最小值时，导数值会变小，从而使参数的调整幅度变小，最终收敛于局部最小值。因此，在学习率固定的情况下，梯度下降可以到达代价函数的局部最小值。

梯度下降总能找到代价函数的局部最小值点，但未必是全局最小值。当代价函数有多个极小值时，可能会陷入局部最小值，无法到达全局最小值。

如果在梯度下降的每一步迭代中都使用整个训练集，所有样本都参与计算，则将其称为**批量梯度下降**（Batch Gradient Desent，BGD）。线性回归模型中使用批量梯度下降。

### 2.1.4 学习算法

单变量线性回归模型：$f_{w,b}(x) = wx+b$
平方误差代价函数：$J(w,b) = \dfrac{1}{2m} \displaystyle\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2$

利用梯度下降，可以得到单变量线性回归模型的学习算法。

$$
\begin{aligned}
    w' &= w - \alpha \dfrac{\partial}{\partial w} J(w,b) \\
    &= w - \alpha \dfrac{\partial}{\partial w} \dfrac{1}{2m} \displaystyle\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2 \\
    &= w - \alpha \dfrac{\partial}{\partial w} \dfrac{1}{2m} \sum_{i=1}^m (wx^{(i)} + b - y^{(i)})^2 \\
    &= w - \alpha \dfrac{\partial}{\partial w} \dfrac{1}{2m} \sum_{i=1}^m [w^2 (x^{(i)})^2 + 2wx^{(i)}(b - y^{(i)}) + (b - y^{(i)})^2] \\
    &= w - \alpha \dfrac{1}{2m} \sum_{i=1}^m [2w(x^{(i)})^2 + 2x^{(i)}(b - y^{(i)})] \\
    &= w - \alpha \dfrac{1}{2m} \sum_{i=1}^m 2x^{(i)} (wx^{(i)} + b - y^{(i)}) \\
    &= w - \alpha \dfrac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)}) x^{(i)} \\
\end{aligned}
$$

$$
\begin{aligned}
    b' &= b - \alpha \dfrac{\partial}{\partial b} J(w,b) \\
    &= b - \alpha \dfrac{\partial}{\partial b} \dfrac{1}{2m} \displaystyle\sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2 \\
    &= b - \alpha \dfrac{\partial}{\partial b} \dfrac{1}{2m} \sum_{i=1}^m (wx^{(i)} + b - y^{(i)})^2 \\
    &= b - \alpha \dfrac{\partial}{\partial b} \dfrac{1}{2m} \sum_{i=1}^m [b^2 + 2b(wx^{(i)} - y^{(i)}) + (wx^{(i)} - y^{(i)})^2] \\
    &= b - \alpha \dfrac{1}{2m} \sum_{i=1}^m [2b + 2(wx^{(i)} - y^{(i)})] \\
    &= b - \alpha \dfrac{1}{2m} \sum_{i=1}^m 2[b + (wx^{(i)} - y^{(i)})] \\
    &= b - \alpha \dfrac{1}{m} \sum_{i=1}^m (wx^{(i)} + b - y^{(i)}) \\
    &= b - \alpha \dfrac{1}{m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)}) \\
\end{aligned}
$$

平方误差代价函数是一个**凸函数**（convex function），它的图像呈碗形，只有一个极小值点。在凸函数上进行梯度下降时，只要选取合适的学习率，总会收敛到全局最小值。

## 2.2 多元线性回归

### 2.2.1 模型定义

基本概念：

- 特征向量：单个样本中的多个特征组成一个向量，称为特征向量。第 $i$ 个样本的特征向量记作 $\boldsymbol{x}^{(i)}$。
- 特征向量中的特征数量记作 $n$，其中第 $j$ 个特征记作 $x_j$。对于第 $i$ 个样本，其特征向量表示为 $\boldsymbol{x}^{(i)} = (x^{(i)}_1, x^{(i)}_2, \cdots, x^{(i)}_n)$。

如果模型 $f$ 是一个多元线性函数，则称之为**多元线性回归**（Multiple Linear Regression）。

多元线性回归模型定义为：

$$
f_{w_1, w_2, \cdots, w_n, b}(x_1, x_2, \cdots, x_n) = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
$$

记 $\boldsymbol{w} = (w_1, w_2, \cdots, w_n)$，$\boldsymbol{x} = (x_1, x_2, \cdots, x_n)$，则模型可简写为

$$
f_{\boldsymbol{w}, b}(\boldsymbol{x}) = \boldsymbol{w} \cdot \boldsymbol{x} + b
$$

其中 $w_1, w_2, \cdots, w_n, b$ 为模型参数，$\boldsymbol{x}$ 为特征向量。

代价函数为：

$$
J(\boldsymbol{w}, b) = \dfrac{1}{2m} \displaystyle\sum_{i=1}^m (f_{\boldsymbol{w}, b}(\boldsymbol{x}^{(i)}) - y^{(i)})^2
$$

梯度下降过程为：

$$
\begin{aligned}
    & w_1' = w_1 - \alpha \dfrac{1}{m} \sum_{i=1}^m (f_{\boldsymbol{w}, b}(\boldsymbol{x}^{(i)}) - y^{(i)}) x^{(i)}_1 \\
    & w_2' = w_2 - \alpha \dfrac{1}{m} \sum_{i=1}^m (f_{\boldsymbol{w}, b}(\boldsymbol{x}^{(i)}) - y^{(i)}) x^{(i)}_2 \\
    & \cdots \\
    & w_n' = w_n - \alpha \dfrac{1}{m} \sum_{i=1}^m (f_{\boldsymbol{w}, b}(\boldsymbol{x}^{(i)}) - y^{(i)}) x^{(i)}_n \\
    & b' = b - \alpha \dfrac{1}{m} \sum_{i=1}^m (f_{\boldsymbol{w}, b}(\boldsymbol{x}^{(i)}) - y^{(i)})
\end{aligned}
$$

### 2.2.2 特征缩放

不同的特征具有不同的取值范围。对于取值较大的特征，参数的轻微变化就会引起预测值的剧烈波动；而对于取值较小的特征，只有参数变化很大才能造成足够的影响。如果特征的取值差别过大，会使不同特征的更新速度具有较大差异，从而导致梯度下降过程出现剧烈的震荡，降低收敛速度。为了解决这种问题，可以对特征进行缩放，减小特征之间的取值差距。

在给定的样本中，设特征 $x$ 的最大值为 $x_{\text{max}}$，最小值为 $x_{\text{min}}$，均值为 $\mu$，标准差为 $\sigma$，则常见的**特征缩放**（feature scaling）方法有：

- **线性归一化**（min-max normalization）：对原始数据进行线性变换，使结果映射到 $[a, b]$ 的范围，实现对原始数据的等比缩放。
$$
x_{\text{scaled}} = a + \dfrac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}} (b-a)
$$
特别地，映射到 $[0, 1]$ 区间上的方法为
$$
x_{\text{scaled}} = \dfrac{x - x_{\text{min}}}{x_{\text{max}} - x_{\text{min}}}
$$

- **均值归一化**（mean normalization）：使缩放后的数据的均值为 0。
$$
x_{\text{scaled}} = \dfrac{x - \mu}{x_{\text{max}} - x_{\text{min}}}
$$

- **标准归一化**（standard normalization，也称为 Z-score normalization）：将原始数据映射到均值为 0、标准差为 1 的分布上。
$$
x_{\text{scaled}} = \dfrac{x - \mu}{\sigma}
$$

特征缩放的作用：

1. 消除不同特征之间的量纲差异，使得不同特征具有可比性。
2. 使得模型的代价函数更易于优化，从而加快模型的收敛速度。
3. 避免异常值对模型的影响。
4. 提高模型的可解释性。进行缩放之后，不同特征的权重可以直接进行比较，更容易解释模型的预测结果。

### 2.2.3 判断梯度下降是否收敛

以迭代次数为横轴，以代价函数值为纵轴，建立坐标系并绘制曲线，这样的曲线称为**学习曲线**（learning curve）。借助学习曲线可以查看模型迭代过程中代价函数的变化，得到模型是否收敛、何时收敛、收敛速度快慢等信息。

另一种方法是**自动收敛测试**（automatic convergence test）：设 $\varepsilon$ 为一个很小的正数（如 0.001），如果代价函数在一次迭代后的减少量小于 $\varepsilon$，就宣布收敛。

### 2.2.4 学习率的设置

如果代价函数在迭代过程中有时上升、有时下降，就说明梯度下降无法正常工作。这可能是因为代码中有错误，也有可能是因为学习率过大。

如果学习率足够小，代价函数应该在每次迭代后减小。可以设置一个很小的学习率，观察代价函数的变化，如果代价函数有时增加，说明代码中存在错误。

实际使用梯度下降时，可以设置一系列学习率（如 0.001、0.01、0.1、1），对每个学习率执行梯度下降并绘制学习曲线，选择使代价函数快速且持续降低的学习率。

### 2.2.5 特征工程

特征的选择会对学习算法的性能产生巨大影响。对于许多实际应用，选择正确的特征是使算法良好运行的关键步骤。

**特征工程**（Feature Engineering）：利用对问题的知识或直觉来设计新特征，通常是对原始特征进行转换或组合来产生新特征。

利用特征工程可以得到更好的特征，从而使学习算法更容易做出准确的预测。

# 3 逻辑回归

## 3.1 二元分类

只有两种可能输出的分类问题称为**二元分类**（binary classification）。可以用数字 0 或 1 来表示这两种输出，0 表示负类（negative class），1 表示正类（positive class）。输出为 0 的样本称为**反例**（negative example），输出为 1 的样本称为**正例**（positive example）。

线性回归模型不适用于二元分类问题。因为二元分类问题只有 2 种离散的输出，而线性回归模型的输出是连续的，需要设置一个阈值来确定最终的分类结果。对于一个特定的问题，这个阈值应该是不变的。然而，当训练样本不同时，线性回归模型的参数会发生变化，使得相同的阈值可能导致不同的分类结果，这与我们的预期不符。

<div align="center"><img src="https://github.com/zzx-JLU/images_for_markdown/raw/main/机器学习-吴恩达/图3.1.png" alt="图3.1"></div>

## 3.2 逻辑回归

sigmoid 函数：$g(z) = \dfrac{1}{1 + e^{-z}}$

<div align="center"><img src="https://github.com/zzx-JLU/images_for_markdown/raw/main/机器学习-吴恩达/图3.2-sigmoid函数.png" alt="图3.2 sigmoid函数" width=90%></div>

sigmoid 函数的取值范围为 $(0,1)$，且 $g(0) = 0.5$。

逻辑回归模型：$f_{\boldsymbol{w}, b}(\boldsymbol{x}) = g(\boldsymbol{w} \cdot \boldsymbol{x} + b) = \dfrac{1}{1 + e^{-(\boldsymbol{w} \cdot \boldsymbol{x} + b)}}$

逻辑回归模型的输出可以看做在给定输入 $\boldsymbol{x}$ 和参数 $\boldsymbol{w}, b$ 的条件下，标签 $y$ 为 $1$ 的概率，即 $f_{\boldsymbol{w}, b}(\boldsymbol{x}) = P(y=1 \mid \boldsymbol{x}; \boldsymbol{w}, b)$。
