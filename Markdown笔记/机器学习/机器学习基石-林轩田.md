---
title: 机器学习基石
chrome:
    format: "A4"
    headerTemplate: '<div></div>'
    footerTemplate: '<div style="width:100%; text-align:center; border-top: 1pt solid #eeeeee; margin: 10px 10px 20px; font-size: 8pt;">
    <span class=pageNumber></span> / <span class=totalPages></span></div>'
    displayHeaderFooter: true
    margin:
        top: '40px'
        bottom: '80px'
        left: '60px'
        right: '60px'
---

<h1>机器学习基石</h1>

<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1.When Can Machines Learn?](#1when-can-machines-learn)
  - [1.1 The Learning Problem](#11-the-learning-problem)
    - [1.1.1 什么是机器学习](#111-什么是机器学习)
    - [1.1.2 机器学习的组成](#112-机器学习的组成)
  - [1.2 Learning to Answer Yes_No](#12-learning-to-answer-yes_no)
    - [1.2.1 感知机](#121-感知机)
    - [1.2.2 感知机学习算法（PLA）](#122-感知机学习算法pla)
    - [1.2.3 感知机学习算法的收敛性](#123-感知机学习算法的收敛性)
    - [1.2.4 线性不可分数据](#124-线性不可分数据)
  - [1.3 机器学习的分类](#13-机器学习的分类)
    - [1.3.1 按输出空间$\mathcal{Y}$分类](#131-按输出空间mathcaly分类)
    - [1.3.2 按标签$y_i$分类](#132-按标签y_i分类)
    - [1.3.3 按输入数据的方式分类](#133-按输入数据的方式分类)
    - [1.3.4 按输入空间$\mathcal{X}$分类](#134-按输入空间mathcalx分类)
  - [1.4 机器学习的可行性](#14-机器学习的可行性)
    - [1.4.1 Learning is Impossible?](#141-learning-is-impossible)
    - [1.4.2 Probability to the Rescue](#142-probability-to-the-rescue)
    - [1.4.3 Connection to Learning](#143-connection-to-learning)
    - [1.4.4 Connection to Real Learning](#144-connection-to-real-learning)
- [2 Why Can Machines Learn?](#2-why-can-machines-learn)
  - [2.1 训练 vs. 测试](#21-训练-vs-测试)
    - [2.1.1 Recap and Preview](#211-recap-and-preview)
    - [2.1.2 Effective Number of Lines](#212-effective-number-of-lines)
    - [2.1.3 Effective Number of Hypotheses](#213-effective-number-of-hypotheses)
      - [2.1.3.1 正向射线的成长函数](#2131-正向射线的成长函数)
      - [2.1.3.2 正区间的成长函数](#2132-正区间的成长函数)
      - [2.1.3.3 凸集的成长函数](#2133-凸集的成长函数)
    - [2.1.4 断点](#214-断点)
  - [2.2 泛化理论](#22-泛化理论)
    - [2.2.1 Restriction of Break Point](#221-restriction-of-break-point)
    - [2.2.2 Bounding Function: Basic Cases](#222-bounding-function-basic-cases)
    - [2.2.3 Bounding Function: Inductive Cases](#223-bounding-function-inductive-cases)
    - [2.2.4 A Pictorial Proof](#224-a-pictorial-proof)
  - [2.3 VC维](#23-vc维)
    - [2.3.1 VC维的定义](#231-vc维的定义)
    - [2.3.2 感知机的VC维](#232-感知机的vc维)
    - [2.3.3 VC维的物理意义](#233-vc维的物理意义)
    - [2.3.4 VC维的解释](#234-vc维的解释)
  - [2.4 噪声与误差](#24-噪声与误差)
    - [2.4.1 噪声与概率目标](#241-噪声与概率目标)
    - [2.4.2 误差度量](#242-误差度量)
    - [2.4.3 算法误差度量](#243-算法误差度量)
    - [2.4.4 加权分类](#244-加权分类)
- [3 How Can Machines Learning?](#3-how-can-machines-learning)
  - [3.1 线性回归](#31-线性回归)
    - [3.1.1 线性回归问题](#311-线性回归问题)
    - [3.1.2 线性回归算法](#312-线性回归算法)
    - [3.1.3 泛化问题](#313-泛化问题)

<!-- /code_chunk_output -->

# 1.When Can Machines Learn?

## 1.1 The Learning Problem

### 1.1.1 什么是机器学习

<div align="center" style="margin-bottom: 10px">
    <img src="https://raw.githubusercontent.com/zzx-JLU/images_for_markdown/main/机器学习基石-林轩田/图1.1-什么是机器学习.1m6xpjm0md40.png">
    <br>
    图1.1 什么是机器学习
</div>

机器学习：通过对已有的数据进行计算，获得相关经验，从而提高某些方面的性能指标。（授人以渔）

机器学习：构造复杂系统的替代方案。

机器学习三大关键要素：

1. 存在某种可以被学习的 “潜在模式” 或 “潜在规则”，从而性能指标可以得到提高。
2. 这种 “潜在模式” 没有可以用于编程的明确定义，因此需要机器学习来解决。
3. 存在相关的数据，可以供机器去学习。

### 1.1.2 机器学习的组成

1. **输入**：$\boldsymbol{x}\in \mathcal{X}$。
2. **输出**：$y\in \mathcal{Y}$。
3. **目标函数**（target function）：要学习的未知模式，记作 $f:\mathcal{X}\to \mathcal{Y}$。
4. **训练样本**（training examples）：提供给计算机进行学习的数据，记作 $\mathcal{D}=\{(\boldsymbol{x}_1,y_1),(\boldsymbol{x}_2,y_2),\cdots,(\boldsymbol{x}_n,y_n)\}$。
5. **假设函数**（hypothesis）：机器学得的、希望它具有良好性能的技能，记作 $g:\mathcal{X}\to \mathcal{Y}$。
6. **假设集**（hypothesis set）：所有假设函数的集合，记作 $\mathcal{H}$。我们要从中选出最好的一个假设函数 $g$ 并投入应用。
7. **算法**：记为 $\mathcal{A}$。根据训练样本 $D$，从假设集 $\mathcal{H}$ 中选择最接近 $f$ 的一个假设函数 $g$。

$$
机器学习模型=算法\mathcal{A}+假设集\mathcal{H}
$$

<div align="center">
    <img src="https://cdn.jsdelivr.net/gh/zzx-JLU/images_for_markdown@main/机器学习基石-林轩田/图1.2-机器学习的组成.6r80y18w40k0.png">
    <br>
    图1.2 机器学习的组成
</div>

注意：

- 目标函数 $f$ 是未知的。
- 我们希望 $g$ 接近 $f$，但 $g$ 可能不同于 $f$。

## 1.2 Learning to Answer Yes_No

### 1.2.1 感知机

例：给定一个银行用户的相关信息（如年龄、性别、年收入、就业时长、当前负债等），判断是否给这个用户发放信用卡。

设用户特征为 $\boldsymbol{x}=(x_1,x_2,\cdots,x_d)^{\mathrm{T}}$，对每个用户计算一个加权分数，设特征 $x_i$ 的权值为 $w_i$，同时设置一个阈值 $t$，并做出如下规定：
$$
若\sum_{i=1}^d w_ix_i>t,允许发放信用卡;\\
若\sum_{i=1}^d w_ix_i<t,不予发放信用卡.
$$

设输出集合为 $\mathcal{Y}=\{+1,-1\}$，其中 +1 表示允许发放，-1 表示不予发放。则假设函数 $h$ 可定义如下：
$$
\tag{1-1} h(x)=\operatorname{sign}\Bigg(\bigg(\sum_{i=1}^d w_ix_i\bigg)-t\Bigg)
$$

其中
$$
\operatorname{sign}(x)=\begin{cases}
 1 & x>0\\
 -1 & x<0\\
 0 & x=0
\end{cases}
$$

这样的假设函数称为**感知机**（perceptron）。

感知机的向量形式：
$$
\begin{aligned}
 h(x)&=\operatorname{sign}\Bigg(\bigg(\sum_{i=1}^d w_ix_i\bigg)-t\Bigg)\\
 &=\operatorname{sign}\Bigg(\bigg(\sum_{i=1}^d w_ix_i\bigg)+\underbrace{(-t)}_{w_0}\cdot\underbrace{(+1)}_{x_0}\Bigg)\\
 &=\operatorname{sign}\bigg(\sum_{i=0}^d w_ix_i\bigg)\\
 &=\operatorname{sign}(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x})
\end{aligned}
$$

在二维空间中，感知机的形式为 $h(x)=\operatorname{sign}(w_0+w_1x_1+w_2x_2)$，相当于用一条直线 $w_1x_1+w_2x_2+w_0=0$ 进行划分，直线的一侧对应 $y=+1$，另一侧对应 $y=-1$。因此，感知机也称为线性分类器。

### 1.2.2 感知机学习算法（PLA）

假设集 $\mathcal{H}$ 是所有感知机的集合，现在需要从中选出一个最好的作为假设函数 $g$，要求 $g$ 尽可能地接近 $f$。要想让 $g$ 接近 $f$，至少要在训练样本 $\mathcal{D}$ 上尽可能满足 $f(\boldsymbol{x}_i)=g(\boldsymbol{x}_i)=y_i$。

$\mathcal{H}$ 是一个无限集合，不可能遍历其中的所有函数。因此感知机学习算法的基本思想为：从一个给定的假设函数 $g_0$ 出发，不断调整，降低误差，使其接近 $f$。

由于假设函数 $g$ 由参数向量 $\boldsymbol{w}$ 唯一确定，因此可用 $\boldsymbol{w}$ 表示假设函数 $g$，用 $\boldsymbol{w}_0$ 表示初始假设函数 $g_0$。

> 注：$w_0$ 和 $x_0$ 是常数，因此训练过程中只需要对 $w_1,w_2,\cdots,w_n$ 进行调整，此时 $\boldsymbol{w}=(w_1,w_2,\cdots,w_d)^{\mathrm{T}}$。

对假设函数的优化是一轮一轮进行的。假设在第 $t$ 轮优化后得到的参数向量为 $\boldsymbol{w}_t$，如果对于一组输入和输出 $(\boldsymbol{x}_{i(t)},y_{i(t)})$，有 $\operatorname{sign}(\boldsymbol{w}_t^{\mathrm{T}}\boldsymbol{x}_{i(t)})\not=y_{i(t)}$，就说明在这一点处出现了错误，需要修正。

如果 $y_{i(t)}=+1$，而 $\operatorname{sign}(\boldsymbol{w}_t^{\mathrm{T}}\boldsymbol{x}_{i(t)})=-1$，说明 $\boldsymbol{w}_t^{\mathrm{T}}\boldsymbol{x}_{i(t)}<0$，也就是说向量 $\boldsymbol{w}_t$ 与 $\boldsymbol{x}_{i(t)}$ 之间的夹角 $90\degree<\theta<180\degree$，需要使夹角变小，为此可以令 $\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+\boldsymbol{x}_{i(t)}=\boldsymbol{w}_t+y_{i(t)}\boldsymbol{x}_{i(t)}$；反之，如果 $y_{i(t)}=-1$，而 $\operatorname{sign}(\boldsymbol{w}_t^{\mathrm{T}}\boldsymbol{x}_{i(t)})=+1$，说明向量 $\boldsymbol{w}_t$ 与 $\boldsymbol{x}_{i(t)}$ 之间的夹角太小，需要使夹角变大，为此可以令 $\boldsymbol{w}_{t+1}=\boldsymbol{w}_t-\boldsymbol{x}_{i(t)}=\boldsymbol{w}_t+y_{i(t)}\boldsymbol{x}_{i(t)}$。

<div align="center" style="margin-bottom: 10px">
    <img src="https://cdn.jsdelivr.net/gh/zzx-JLU/images_for_markdown@main/机器学习基石-林轩田/图1.3-感知机学习算法.1r0tnoa29ark.png">
    <br>
    图1.3 感知机学习算法
</div>

综合以上两种情况，当 $(\boldsymbol{x}_{i(t)},y_{i(t)})$ 处发现错误时，做如下调整：
$$
\tag{1-2} \boldsymbol{w}_{t+1}=\boldsymbol{w}_t+y_{i(t)}\boldsymbol{x}_{i(t)}
$$

在每轮优化过程中，遍历训练样本中的每个输入 $\boldsymbol{x}_{i(t)}$ 及其对应的输出 $y_{i(t)}$，判断它是否出错，若没有出错则继续，若出错则调整参数。不断重复上述过程，直到不再有错误发生，就将最后一轮产生的参数向量记为 $\boldsymbol{w}_{\text{PLA}}$ 并返回，作为假设函数 $g$ 的参数。这种方式称为 Cyclic PLA。

> 注：平面上一条直线 $Ax+By+C=0$ 的法向量为 $(A,B)$，因此对于二维感知机 $h(x)=\operatorname{sign}(w_0+w_1x_1+w_2x_2)$，其参数向量 $\boldsymbol{w}=(w_1,w_2)$ 恰好为直线 $w_1x_1+w_2x_2+w_0=0$ 的法向量。也就是说，此时的分割线恰好与参数向量 $\boldsymbol{w}$ 垂直。

### 1.2.3 感知机学习算法的收敛性

感知机学习算法收敛的必要条件：对于训练样本 $\mathcal{D}$，存在一个 $\boldsymbol{w}$ 使得没有错误发生。这样的训练样本 $\mathcal{D}$ 称为**线性可分的**（linear separable）。

如果训练样本 $\mathcal{D}$ 是线性可分的，则一定存在 $\boldsymbol{w}_f$，使得对于任意的 $i=1,2,\cdots,n$，都有 $\operatorname{sign}(\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{x}_i)=y_i$。则
$$
\min_{i} y_i\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{x}_i>0
$$

设训练过程中在 $(\boldsymbol{x}_{i(t)},y_{i(t)})$ 处发现错误，在该点处有
$$
y_{i(t)}\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{x}_{i(t)}\geqslant\min_{i} y_i\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{x}_i>0
$$

在 $(\boldsymbol{x}_{i(t)},y_{i(t)})$ 处对参数向量 $\boldsymbol{w}_t$ 进行优化，得到 $\boldsymbol{w}_{t+1}$，有
$$
\begin{aligned}
 \boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{w}_{t+1}&=\boldsymbol{w}_f^{\mathrm{T}}(\boldsymbol{w}_{t}+y_{i(t)}\boldsymbol{x}_{i(t)})\\
 &=\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{w}_{t}+y_{i(t)}\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{x}_{i(t)}\\
 &\geqslant\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{w}_{t}+\min_{i} y_i\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{x}_i\\
 &>\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{w}_{t}
\end{aligned}
$$

在 $(\boldsymbol{x}_{i(t)},y_{i(t)})$ 处发现错误，则  $\operatorname{sign}(\boldsymbol{w}_t^{\mathrm{T}}\boldsymbol{x}_{i(t)})\not=y_{i(t)}$，又有
$$
y_{i(t)}\boldsymbol{w}_t^{\mathrm{T}}\boldsymbol{x}_{i(t)}\leqslant 0
$$

则
$$
\begin{aligned}
 |\boldsymbol{w}_{t+1}|^2&=|\boldsymbol{w}_{t}+y_{i(t)}\boldsymbol{x}_{i(t)}|^2\\
 &=|\boldsymbol{w}_{t}|^2+2y_{i(t)}\boldsymbol{w}_t^{\mathrm{T}}\boldsymbol{x}_{i(t)}+|y_{i(t)}\boldsymbol{x}_{i(t)}|^2\\
 &\leqslant|\boldsymbol{w}_{t}|^2+|y_{i(t)}\boldsymbol{x}_{i(t)}|^2\\
 &\leqslant|\boldsymbol{w}_{t}|^2+\max_{i}|y_i\boldsymbol{x}_{i}|^2
\end{aligned}
$$

设从 $\boldsymbol{w}_0=\boldsymbol{0}$ 开始经过 $T$ 次修正，则有（证明略）
$$
\dfrac{\boldsymbol{w}_f^{\mathrm{T}}}{|\boldsymbol{w}_f|}\dfrac{\boldsymbol{w}_T}{|\boldsymbol{w}_T|}\geqslant\sqrt{T}C,其中C为常数
$$

综上可知，$\boldsymbol{w}_f^{\mathrm{T}}\boldsymbol{w}_t$ 增长很快，而 $|\boldsymbol{w}_t|$ 增长较慢，则 $\boldsymbol{w}_f$ 与 $\boldsymbol{w}_t$ 的夹角在不断变小，二者越来越接近，因此感知机学习算法是收敛的。

由于 $\dfrac{\boldsymbol{w}_f^{\mathrm{T}}}{|\boldsymbol{w}_f|}\dfrac{\boldsymbol{w}_T}{|\boldsymbol{w}_T|} \leqslant 1$，则有 $\sqrt{T}C \leqslant 1$，因此
$$
T \leqslant \dfrac{1}{C^2}
$$

设 $R^2=\max|\boldsymbol{x}_{i}|^2$，$\rho=\min y_i\dfrac{\boldsymbol{w}_f^{\mathrm{T}}}{|\boldsymbol{w}_f|}\boldsymbol{x}_i$，可以证明 $C=\dfrac{\rho}{R}$，则
$$
T \leqslant \dfrac{R^2}{\rho^2}
$$

也就是说，感知机学习算法最多经过 $\dfrac{R^2}{\rho^2}$ 次修正后必定收敛。

### 1.2.4 线性不可分数据

PLA 的优点：便于实现，执行速度快，对训练样本的维度没有限制。

PLA 的缺点：

1. 无法确定训练样本是否线性可分。
2. 无法确定收敛所需的时间。（上面的 $\rho$ 与 $\boldsymbol{w}_f$ 有关，而 $\boldsymbol{w}_f$ 未知）

当训练样本中有噪声时，训练样本不一定线性可分。那么我们就不去找没有错误的分割线，而是寻找错误最小的分割线，如下所示：
$$
\boldsymbol{w}_g=\mathop{\arg\min}\limits_{\boldsymbol{w}}\sum_{i=1}^n⟦y_i\not=\operatorname{sign}(\boldsymbol{w}^{\mathrm{T}}\boldsymbol{x}_i)⟧
$$

求解 $\boldsymbol{w}_g$ 是一个 NP-难度问题。因此我们只能退而求其次，寻找相对较好的分割线，由此提出了**口袋算法**（pocket algorithm）。口袋算法是一种贪心算法，如下所示：

1. 初始化口袋参数 $\hat{\boldsymbol{w}}$。
2. 设轮次为 $t$ 并初始化为 0，设置循环次数的上限，循环执行以下操作：
   （1）对 $\boldsymbol{w}_t$ 随机寻找一个出错点 $(\boldsymbol{x}_{i(t)},y_{i(t)})$。
   （2）进行修正：$\boldsymbol{w}_{t+1}=\boldsymbol{w}_t+y_{i(t)}\boldsymbol{x}_{i(t)}$。
   （3）如果 $\boldsymbol{w}_{t+1}$ 的错误更少，就用 $\boldsymbol{w}_{t+1}$ 替换 $\hat{\boldsymbol{w}}$。
3. 将 $\hat{\boldsymbol{w}}$ 记为 $\boldsymbol{w}_{\text{POCKET}}$ 并返回，作为假设函数 $g$ 的参数。

## 1.3 机器学习的分类

### 1.3.1 按输出空间$\mathcal{Y}$分类

二元分类（binary classification）：输出空间只有 2 个元素。
多元分类（multiclass classification）：输出空间大于 2 个元素。
回归分析（regression）：$\mathcal{Y}=\R\ 或\ \mathcal{Y}=[\text{lower},\text{upper}]\subset\R$。
结构化学习（structured learning）：输入和输出都是具有结构的对象（如数列、列表、树、边界框等）。

其中，二元分类和回归分析是核心工具，可以用来构造更复杂的工具。

### 1.3.2 按标签$y_i$分类

监督学习（supervised learning）：在训练集中，对于每个输入 $\boldsymbol{x}_i$ 都有对应的正确输出 $y_i$。
无监督学习（unsupervised learning）：训练集中没有标签。
聚类（clustering）：无监督多元分类。
半监督学习（semi-supervised learning）：在训练集中，部分数据有标签，而另一部分没有标签。
强化学习（reinforcement learning）：对于期望的输出给予奖励，对不期望的输出实施惩罚，从而使模型表现出所期待的行为。

### 1.3.3 按输入数据的方式分类

批量学习（batch learning）：将训练数据一次性批量输入给学习算法。
在线学习（online learning）：将训练数据逐个输入给学习算法，每输入一个样本就调整一下参数。
主动学习（active learning）：机器主动提问，对于给定的输入 $\boldsymbol{x}_i$，询问其输出 $y_i$ 是什么。

### 1.3.4 按输入空间$\mathcal{X}$分类

具体特征（concrete feature）：$\boldsymbol{x}_i$ 的每个维度都具有复杂的物理意义，带有人类对问题的理解和描述。
原始特征（raw feature）：未加处理的、含义比较简单的特征，如图片、文字等。通常需要转化成具体特征。
抽象特征（abstract feature）：没有物理意义的特征，如用户编号、学号等。

## 1.4 机器学习的可行性

### 1.4.1 Learning is Impossible?

NFL（No Free Lunch，没有免费的午餐）定理：在已知数据 $\mathcal{D}$ 上学习到的模型，用于预测未知数据的结果，如果在未知数据上所有结果的出现概率相同，则预测效果是注定的。

如果我们对所研究的问题一无所知，则对于未知数据而言，所有可能结果的出现概率相同，此时无法判断哪个结果更好，因此机器学习算法的效果不会比随机搜索算法更优。

由此可知，学习算法必须要做出一个与问题领域有关的假设，或者对问题领域有一定的知识。学习算法必须与问题领域相适应，不存在一个与具体应用无关的、普遍适用的最优算法。

### 1.4.2 Probability to the Rescue

霍夫丁不等式（Hoeffding's inequality）：设事件 $A$ 的概率为 $\mu$，样本数量为 $N$，在样本中事件 $A$ 发生的频率为 $\nu$，则对于任意的 $\varepsilon>0$，有
$$
\tag{1-3} P(|\nu-\mu|>\varepsilon)\leqslant 2e^{-2\varepsilon^2 N}
$$

霍夫丁不等式说明，当 $N$ 增大时，$\nu$ 与 $\mu$ 的差值大于 $\varepsilon$ 的概率会减小。那么，当样本数量足够大时，$\nu$ 与 $\mu$ 大致相等，可以用 $\nu$ 代替 $\mu$。

### 1.4.3 Connection to Learning

对于任意的假设函数 $h$，在大小为 $N$ 的数据集上，设样本内错误率（训练误差）为 $E_{\text{in}}(h)$，样本外错误率（泛化误差）为 $E_{\text{out}}(h)$。由霍夫丁不等式可得，对于任意的 $\varepsilon>0$，有
$$
\tag{1-4} P(|E_{\text{in}}(h)-E_{\text{out}}(h)|>\varepsilon)\leqslant 2e^{-2\varepsilon^2 N}
$$

当数据集足够大时，可以认为 $E_{\text{in}}(h) \approx E_{\text{out}}(h)$，则当 $E_{\text{in}}(h)$ 较小时，可以认为 $E_{\text{out}}(h)$ 也较小，因此可以认为在同分布数据上，假设函数 $h$ 接近目标函数 $f$。

以上结论可以用于模型的验证（verification）。当给定一个模型时，如果它在数据集上的准确率较高，就可以认为该模型对于数据集以外的未知数据的准确率也较高。

但是，以上结论并不能用于学习。学习的过程是要在假设集 $\mathcal{H}$ 上选择一个较好的函数 $h$ 作为假设函数 $g$，而以上结论没有选择的过程，只能对于一个给定的函数进行验证。

### 1.4.4 Connection to Real Learning

如果样本选取得不好，会使得 $\nu$ 与 $\mu$ 相差较大。霍夫丁不等式说明，每次取样时取出坏样本的概率不会太大，但是多次取样时出现坏样本的概率会增大。

类似地，不好的数据会使得 $E_{\text{in}}(h)$ 与 $E_{\text{out}}(h)$ 相差较大。由霍夫丁不等式可得，对于任意给定的假设函数 $h$，考虑所有可能的数据集，遇到坏数据的概率不大。设数据集为 $\mathcal{D}$，数据集的大小为 $N$，则对于假设函数 $h_i\in\mathcal{H}$，数据集不好的概率为
$$
P_{\mathcal{D}}(\text{BAD}\ \ \mathcal{D}\ \ \text{for}\ \ h_i)\leqslant 2e^{-2\varepsilon^2 N}
$$

但是，从假设集 $\mathcal{H}$ 中选择假设函数时，只有当某一数据集对所有假设函数而言都是好的，该数据集才是好的，否则该数据集就是不好的。设假设集的大小为 $M$，则数据集 $\mathcal{D}$ 对于假设集 $\mathcal{H}$ 不好的概率为
$$
\begin{aligned}
    & P_{\mathcal{D}}(\text{BAD}\ \ \mathcal{D})\\
    = & P_{\mathcal{D}}(\text{BAD}\ \ \mathcal{D}\ \ \text{for}\ \ h_1\cup\text{BAD}\ \ \mathcal{D}\ \ \text{for}\ \ h_2\cup\cdots\cup\text{BAD}\ \ \mathcal{D}\ \ \text{for}\ \ h_M)\\
    \leqslant & P_{\mathcal{D}}(\text{BAD}\ \ \mathcal{D}\ \ \text{for}\ \ h_1)+P_{\mathcal{D}}(\text{BAD}\ \ \mathcal{D}\ \ \text{for}\ \ h_2)+\cdots+P_{\mathcal{D}}(\text{BAD}\ \ \mathcal{D}\ \ \text{for}\ \ h_M)\\
    \leqslant & 2e^{-2\varepsilon^2 N}+2e^{-2\varepsilon^2 N}+\cdots+2e^{-2\varepsilon^2 N}\\
    = & 2Me^{-2\varepsilon^2 N}
\end{aligned}
$$

上式称为有限霍夫丁不等式（finite-bin Hoeffding's inequality）。由该式可知，当数据集足够大时，数据集不好的概率会足够低，一定能够找到合适的数据集对所有的假设函数都是好的。此时，最好的机器学习算法是选择 $E_{\text{in}}$ 最小的假设函数 $h_i$ 作为 $g$。

综上所述，如果假设集 $\mathcal{H}$ 大小有限，数据集 $\mathcal{D}$ 足够大，则对于机器学习算法所选择的任意假设函数 $g$，都有 $E_{\text{in}}(g)\approx E_{\text{out}}(g)$；如果机器学习算法所选择的假设函数 $g$ 满足 $E_{\text{in}}(g)\approx 0$，则大概率有 $E_{\text{out}}(g)\approx 0$。因此，机器学习是可行的。

# 2 Why Can Machines Learn?

## 2.1 训练 vs. 测试

### 2.1.1 Recap and Preview

学习可以分成 2 个核心问题：

1. 能否保证 $E_{\text{in}}(h)$ 与 $E_{\text{out}}(h)$ 足够接近？
2. 能否使得 $E_{\text{in}}(h)$ 足够小？

设假设集 $\mathcal{H}$ 的大小为 $M$，$M$ 的大小会影响上述 2 个问题。当 $M$ 较小时，根据有限霍夫丁不等式，问题 1 可以得到保证；但是可选的假设函数较少，可能无法找到使得 $E_{\text{in}}(h)$ 足够小的假设函数，问题 2 可能无法满足。相反，当 $M$ 较大时，问题 2 容易满足，而问题 1 难以保证。因此，选择大小合适的假设集就显得非常重要。

然而，对于无限大的假设集（如感知机），根据以上分析，似乎注定是不好的。为此，我们需要证明无限大假设集上的学习也是可行的。

### 2.1.2 Effective Number of Lines

在推导有限霍夫丁不等式的过程中，将若干并事件的概率写成这些事件各自概率之和，那么当假设集无限大时，就会出现无限项的和，其结果可能为无限大。因此，有限霍夫丁不等式只能说明有限假设集的学习是可行的，无限假设集的情况无法确定。

在上述推导过程中，没有考虑并事件之间是否相交，导致相交部分重复计算，因此推导结果要比实际结果更大。

考虑两个相似的假设函数 $h_1$ 和 $h_2$，满足 $E_{\text{out}}(h_1)\approx E_{\text{out}}(h_2)$，并且对大多数数据集有 $E_{\text{in}}(h_1)=E_{\text{in}}(h_2)$，则它们的坏数据集有很大一部分是重合的，可以把这样的假设函数归为一类。对于无限大的假设集，可以考虑将其中的假设函数分成有限类，每一类作为和式中的一项，这样就可以将无限化为有限。

首先考虑一种简单情况：假设集 $\mathcal{H}$ 为平面上的所有直线，每条直线将平面分成两部分，据此将平面上的点分成两类。如果数据集的大小为 $N$，也就是已知平面上的 $N$ 个点，根据这 $N$ 个点的分类结果，最多可以将平面上的所有直线分成 $2^N$ 类。设 $N$ 个点可以将所有直线划分成的最大种类数为 $\operatorname{effective}(N)$，若可以用 $\operatorname{effective}(N)$ 替换有限霍夫丁不等式中的 $M$，且 $\operatorname{effective}(N)\ll 2^N$，则对平面上的所有直线进行学习是可行的。

### 2.1.3 Effective Number of Hypotheses

对于分类问题，其假设函数为 $h:\mathcal{X}\to\mathcal{Y}$。对于一组给定的输入，假设函数对其中的每条数据产生一个输出，将这些输出写成一个向量，形如
$$
h(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_N)=(h(\boldsymbol{x}_1),h(\boldsymbol{x}_2),\cdots,h(\boldsymbol{x}_N))\in\mathcal{Y}^N
$$

这样的向量称为划分。特别地，对于二分类问题，$\mathcal{Y}=\{-1,+1\}$，这样的向量称为二分（dichotomy）。

假设集 $\mathcal{H}$ 对于一组输入可以产生的所有划分的集合记作 $\mathcal{H}(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_N)$。我们希望用 $|\mathcal{H}(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_N)|$ 代替有限霍夫丁不等式中的 $M$。

$|\mathcal{H}(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_N)|$ 与输入 $\{\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_N\}$ 有关，为了去除输入的影响，定义**成长函数**（growth function）如下：
$$
m_{\mathcal{H}}(N)=\max_{\{\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_N\}\subseteq\mathcal{X}}|\mathcal{H}(\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_N)|
$$

对于二分类问题，有 $m_{\mathcal{H}}(N)\leqslant 2^N$，但这只是一个上界，不是上确界。指数级的成长函数不能保证有限霍夫丁不等式的右侧足够小，需要寻找更小的上界。

在计算二分类问题的成长函数之前，先介绍几个计算成长函数的例子。

#### 2.1.3.1 正向射线的成长函数

$$
\mathcal{X}=\R,h(x)=\operatorname{sign}(x-a),a\in\R
$$

<div align="center">
    <img src="https://raw.githubusercontent.com/zzx-JLU/images_for_markdown/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E6%9E%97%E8%BD%A9%E7%94%B0/%E5%9B%BE2.1-%E6%AD%A3%E5%90%91%E5%B0%84%E7%BA%BF.png">
    <br>
    图2.1 正向射线
</div>

$$
m_{\mathcal{H}}(N)=C_{N+1}^1=N+1
$$

#### 2.1.3.2 正区间的成长函数

$$
\mathcal{X}=\R,h(x)=\begin{cases}
    +1 & x\in[l,r)\\
    -1 & 其他
\end{cases}
$$

<div align="center">
    <img src="https://raw.githubusercontent.com/zzx-JLU/images_for_markdown/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E6%9E%97%E8%BD%A9%E7%94%B0/%E5%9B%BE2.2-%E6%AD%A3%E5%8C%BA%E9%97%B4.png">
    <br>
    图2.2 正区间
</div>

$$
\begin{aligned}
    m_{\mathcal{H}}(N)&=C_{N+1}^2+1\\
    &=\dfrac{1}{2}N^2+\dfrac{1}{2}N+1
\end{aligned}
$$

#### 2.1.3.3 凸集的成长函数

$$
\mathcal{X}=\R^2,h(\boldsymbol{x})=\begin{cases}
    +1 & \boldsymbol{x}在凸集中\\
    -1 & \boldsymbol{x}不在凸集中
\end{cases}
$$

实数集合 $\R$ 的向量空间中，如果集合 $S$ 中任意两点的连线上的点都在 $S$ 内，则称集合 $S$ 为凸集。

可以证明，$m_{\mathcal{H}}(N)=2^N$。此时所有分类情况都可以出现，称这 $N$ 个样本可以被假设集 $\mathcal{H}$ 打散（shatter）。

### 2.1.4 断点

如果 $k\in\N$ 个样本不能被假设集 $\mathcal{H}$ 打散，就称 $k$ 为假设集 $\mathcal{H}$ 的一个**断点**（break point）。

如果 $k$ 是断点，则 $k+1,k+2,\cdots,k+n(n\in\N)$ 也是断点。因此，我们更关心最小的断点。

当断点为 $k$ 时，对于 $N$ 维向量（$N>k$），从中任意选取 $k$ 个维度，不能出现所有分类情况的全排列。

## 2.2 泛化理论

### 2.2.1 Restriction of Break Point

对于二分类假设集，设断点为 $k=2$，则

1. 当 $N=1$ 时，$m_{\mathcal{H}}(N)=2$。
2. 当 $N=2$ 时，$m_{\mathcal{H}}(N)<4$。
3. 当 $N=3$ 时，由于断点为 2，任意两个样本点都不能出现全排列，受到这一条件的限制，成长函数可能取到的最大值为 4。

由此可见，当断点为 $k$ 时，$N>k$ 范围内的成长函数可能取到的最大值会受到限制，可能会使得成长函数被限制在多项式级别。

### 2.2.2 Bounding Function: Basic Cases

边界函数（bounding function）：当断点为 $k$ 时，成长函数 $m_{\mathcal{H}}(N)$ 可能取得的最大值，记作 $B(N,k)$。

边界函数的取值与假设集无关。

当 $k=1$ 时，任何一个输入都只能产生一种输出，易得 $B(N,1)=1$。

当 $N<k$ 时，$B(N,k)=2^N$。当 $N=k$ 时，$B(N,k)=2^N-1$。

### 2.2.3 Bounding Function: Inductive Cases

可以证明，$B(N,k)=B(N-1,k)+B(N-1,k-1)$，证明略。利用这一递推关系，使用数学归纳法可得
$$
\tag{2-1} B(N,k)=\sum_{i=0}^{k-1}C_N^i
$$

当 $N\gg k$ 时，有如下近似：
$$
\tag{2-2} B(N,k)=\sum_{i=0}^{k-1}C_N^i\approx C_N^{k-1}\approx \lambda N^{k-1}
$$

上式表示，如果假设集存在断点 $k$，则成长函数的上界为多项式级。

### 2.2.4 A Pictorial Proof

当 $N$ 足够大时，可以用成长函数代替有限霍夫丁不等式中的 $M$，有如下结论：
$$
P(\exist h\in\mathcal{H}，使得|E_{\text{in}}(h)-E_{\text{out}}(h)|>\varepsilon)\leqslant 4m_{\mathcal{H}}(2N)e^{-\frac{1}{8}\varepsilon^2N}
$$

证明略。这一上界称为 Vapnik-Chervonenkis bound（VC 界）。

当 $N$ 和 $k$ 足够大时（$N\geqslant 2,k\geqslant 3$），有
$$
\tag{2-3} m_{\mathcal{H}}(N)\leqslant B(N,k)=\sum_{i=0}^{k-1}C_N^i\leqslant N^{k-1}
$$

对于学习算法选择的任意假设函数 $g\in\mathcal{H}$，当 $N$ 足够大，$k$ 存在且 $k\geqslant 3$ 时，由 VC 界可得
$$
\tag{2-4}
\begin{aligned}
    & P_{\mathcal{D}}(|E_{\text{in}}(g)-E_{\text{out}}(g)|>\varepsilon)\\
    \leqslant & P_{\mathcal{D}}(\exist h\in\mathcal{H}，使得|E_{\text{in}}(h)-E_{\text{out}}(h)|>\varepsilon)\\
    \leqslant & 4m_{\mathcal{H}}(2N)e^{-\frac{1}{8}\varepsilon^2N}\\
    \leqslant & 4(2N)^{k-1}e^{-\frac{1}{8}\varepsilon^2N}
\end{aligned}
$$

由以上推导过程可得，当假设集存在断点（好的假设集）、数据集足够大（好的数据集）时，可以推论 $E_{\text{in}}\approx E_{\text{out}}$。此时，使用学习算法选出 $E_{\text{in}}$ 较小的假设函数 $g$，就可以完成学习。

以上结论是理想状况，实际情况不一定能够达到这样的效果，只是有几率达到。

## 2.3 VC维

### 2.3.1 VC维的定义

假设集 $\mathcal{H}$ 能够打散的最大的样本个数称为 $\mathcal{H}$ 的 VC 维（VC dimension），记作 $d_{\text{VC}}(\mathcal{H})$。

当断点存在时，设最小的断点为 $k_{\text{min}}$，则 $d_{\text{VC}}=k_{\text{min}}-1$。当断点不存在时，$d_{\text{VC}}=\infin$。

当 $N\leqslant d_{\text{VC}}$ 时，$\mathcal{H}$ 可能打散 $N$ 个样本，也就是说，存在特定的 $N$ 个样本能够被打散，但不是所有的输入都能被打散。当 $N>d_{\text{VC}}$ 时，$N$ 是断点，输入的 $N$ 个样本一定不能被打散。

当 $N\geqslant 2,d_{\text{VC}}\geqslant 2$ 时，由式 $(2\text{-}3)$ 可得
$$
\tag{2-5} m_{\mathcal{H}}(N)\leqslant N^{d_{\text{VC}}}
$$

当 $d_{\text{VC}}$ 有限时，有较大概率使得 $E_{\text{in}}\approx E_{\text{out}}$ 成立，并且与学习算法、目标函数以及输入的分布无关。

正向射线：$m_{\mathcal{H}}(N)=N+1$，$d_{\text{VC}}=1$
正区间：$m_{\mathcal{H}}(N)=\dfrac{1}{2}N^2+\dfrac{1}{2}N+1$，$d_{\text{VC}}=2$
凸集：$m_{\mathcal{H}}(N)=2^N$，$d_{\text{VC}}=\infin$
二维感知机：$d_{\text{VC}}=3$

### 2.3.2 感知机的VC维

一维感知机（正区间）：$d_{\text{VC}}=2$
二维感知机：$d_{\text{VC}}=3$

由上述已知结论可以猜想，对于 $d$ 维感知机，$d_{\text{VC}}=d+1$。下面给出证明。

1. 证明 $d_{\text{VC}}\geqslant d+1$

如果存在一个大小为 $d+1$ 并且可以被打散的输入，就说明 $d_{\text{VC}}\geqslant d+1$ 是成立的。构造输入如下：
$$
\boldsymbol{X}=\begin{bmatrix}
    \boldsymbol{x}_1^{\text{T}}\\
    \boldsymbol{x}_2^{\text{T}}\\
    \boldsymbol{x}_3^{\text{T}}\\
    \cdots\\
    \boldsymbol{x}_{d+1}^{\text{T}}\\
\end{bmatrix}=
\begin{bmatrix}
    1 & 0 & 0 & \cdots & 0\\
    1 & 1 & 0 & \cdots & 0\\
    1 & 0 & 1 & \cdots & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    1 & 0 & 0 & \cdots & 1\\
\end{bmatrix}
$$

注意：输入的维度为 $d$，加上阈值的系数 $x_0=+1$，使得 $\boldsymbol{x}_i$ 为 $d+1$ 维向量，而样本个数恰好也为 $d+1$，因此矩阵 $\boldsymbol{X}$ 为 $d+1$ 维方阵，而且是可逆矩阵。

如果对于任意一个输出 $\boldsymbol{y}=[y_1,y_2,\cdots,y_{d+1}]^{\text{T}}$，都存在权值 $\boldsymbol{w}$，使得 $\operatorname{sign}(\boldsymbol{X}\boldsymbol{w})=\boldsymbol{y}$，则这组输入就是可被打散的。

要满足 $\operatorname{sign}(\boldsymbol{X}\boldsymbol{w})=\boldsymbol{y}$，不妨令 $\boldsymbol{X}\boldsymbol{w}=\boldsymbol{y}$，由于 $\boldsymbol{X}$ 是可逆矩阵，可得 $\boldsymbol{w}=\boldsymbol{X}^{-1}\boldsymbol{y}$。因此，任意一种输出都是可以达到的，输入 $\boldsymbol{X}$ 是可以被打散的。

到此为止，我们构造出了一个大小为 $d+1$ 并且可以被打散的输入，因此 $d_{\text{VC}}\geqslant d+1$。

2. 证明 $d_{\text{VC}}\leqslant d+1$

如果任意一个大小为 $d+2$ 的输入都不能被打散，则 $d+2$ 是一个断点，就说明 $d_{\text{VC}}\leqslant d+1$ 是成立的。

不失一般性，设输入为
$$
\boldsymbol{X}=\begin{bmatrix}
    \boldsymbol{x}_1^{\text{T}}\\
    \boldsymbol{x}_2^{\text{T}}\\
    \cdots\\
    \boldsymbol{x}_{d+1}^{\text{T}}\\
    \boldsymbol{x}_{d+2}^{\text{T}}\\
\end{bmatrix}
$$

矩阵 $\boldsymbol{X}$ 的行数为 $d+2$，列数为 $d+1$，也就是有 $d+2$ 个 $d+1$ 维行向量，则这 $d+2$ 个行向量线性相关，任意一个行向量都可以表示为其他行向量的线性组合。因此有
$$
\boldsymbol{x}_{d+2}=a_1\boldsymbol{x}_1+a_2\boldsymbol{x}_2+\cdots+a_{d+1}\boldsymbol{x}_{d+1}
$$

进一步可得
$$
\boldsymbol{w}^{\text{T}}\boldsymbol{x}_{d+2}=a_1\boldsymbol{w}^{\text{T}}\boldsymbol{x}_1+a_2\boldsymbol{w}^{\text{T}}\boldsymbol{x}_2+\cdots+a_{d+1}\boldsymbol{w}^{\text{T}}\boldsymbol{x}_{d+1}
$$

假设输入可以被打散，就一定存在输出满足以下条件：当 $a_i>0$ 时，$\boldsymbol{w}^{\text{T}}\boldsymbol{x}_i>0$，$y_i=1$；当 $a_i<0$ 时，$\boldsymbol{w}^{\text{T}}\boldsymbol{x}_i<0$，$y_i=-1$。此时一定有 $\boldsymbol{w}^{\text{T}}\boldsymbol{x}_{d+2}>0$，$y_{d+2}$ 一定为 $+1$ 而不可能为 $-1$，有一种输出情况无法产生，与假设矛盾。因此，输入 $\boldsymbol{X}$ 不可以被打散。

由 $\boldsymbol{X}$ 的一般性可得，任意一个大小为 $d+2$ 的输入都不能被打散，因此 $d_{\text{VC}}\leqslant d+1$。

综上，对于 $d$ 维感知机，$d_{\text{VC}}\geqslant d+1$ 且 $d_{\text{VC}}\leqslant d+1$，因此 $d_{\text{VC}}=d+1$。

### 2.3.3 VC维的物理意义

VC 维描述了假设集的自由度。VC 维越大，假设集的自由度就越大，模型的描述能力就越强，同时也更复杂。

例如，正向射线有 1 个可变参数，其 VC 维为 1；正区间有 2 个可变参数，其 VC 维为 2。可见，VC 维的大小与模型的可变参数有关。

### 2.3.4 VC维的解释

由式 $(2\text{-}4)$ 和式 $(2\text{-}5)$ 可得
$$
\tag{2-6}
P_{\mathcal{D}}(|E_{\text{in}}(g)-E_{\text{out}}(g)|>\varepsilon)\leqslant 4(2N)^{d_{\text{VC}}}e^{-\frac{1}{8}\varepsilon^2N}
$$

令 $4(2N)^{d_{\text{VC}}}e^{-\frac{1}{8}\varepsilon^2N}=\delta$，整理得
$$
\tag{2-7}
\varepsilon=\sqrt{\dfrac{8}{N}\ln(\dfrac{4(2N)^{d_{\text{VC}}}}{\delta})}
$$

由式 $(2\text{-}6)$ 可得
$$
\tag{2-8}
P_{\mathcal{D}}(|E_{\text{in}}(g)-E_{\text{out}}(g)|\leqslant\varepsilon)\geqslant 1-4(2N)^{d_{\text{VC}}}e^{-\frac{1}{8}\varepsilon^2N}=1-\delta
$$

也就是说，有不小于 $1-\delta$ 的概率使得 $|E_{\text{in}}(g)-E_{\text{out}}(g)|\leqslant\varepsilon$ 成立，将式 $(2\text{-}7)$ 代入并整理得
$$
\tag{2-9}
E_{\text{in}}(g)-\sqrt{\dfrac{8}{N}\ln(\dfrac{4(2N)^{d_{\text{VC}}}}{\delta})}\leqslant E_{\text{out}}(g)\leqslant E_{\text{in}}(g)+\sqrt{\dfrac{8}{N}\ln(\dfrac{4(2N)^{d_{\text{VC}}}}{\delta})}
$$

将 $\sqrt{\dfrac{8}{N}\ln(\dfrac{4(2N)^{d_{\text{VC}}}}{\delta})}$ 记作 $\Omega(N,\mathcal{H},\delta)$，表示模型复杂度所造成的代价。式 $(2\text{-}9)$ 说明，有很大概率使得模型的泛化误差与训练误差满足上述关系。

当 VC 维增大时，模型的复杂度增加，$E_{\text{out}}(g)$ 与 $E_{\text{in}}(g)$ 的差距也越大。而随着 VC 维的增大，模型的表达能力也增强，模型的训练误差减小。VC 维与误差的关系如图 2.3 所示。

<div align="center" style="margin-bottom: 10px">
    <img src="https://raw.githubusercontent.com/zzx-JLU/images_for_markdown/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E6%9E%97%E8%BD%A9%E7%94%B0/%E5%9B%BE2.3-VC%E7%BB%B4%E4%B8%8E%E8%AF%AF%E5%B7%AE%E7%9A%84%E5%85%B3%E7%B3%BB.png">
    <br>
    图2.3 VC维与误差的关系
</div>

从图 2.3 中可以看出，随着 VC 维的增大，$E_{\text{out}}(g)$ 先下降后上升，所以并不是 VC 维越大越好。VC 维越大，模型的描述能力就越强，可以使 $E_{\text{in}}(g)$ 降低，但复杂度也增加，可能使 $E_{\text{out}}(g)$ 增大。因此，最好假设集应使得 VC 维不太大也不太小。

当给定 $\varepsilon$、$\delta$、$d_{\text{VC}}$ 后，可以根据式 $(2\text{-}7)$ 计算 $N$。如果要将 $\delta$ 限定在一定的范围内，可以计算所需 $N$ 的最小值，这个最小值反映了样本的复杂度。理论上所需的 $N$ 的最小值约为 $10000d_{\text{VC}}$，但实际操作中取 $N\approx 10d_{\text{VC}}$ 就够了。由此可见，VC 界还是很宽松的。

## 2.4 噪声与误差

### 2.4.1 噪声与概率目标

没有噪声时，数据集 $\mathcal{D}$ 中的所有数据都满足目标函数 $f$；当数据中有噪声时，存在某些数据不满足目标函数 $f$，我们并不知道哪些数据是噪声，此时不能再将目标函数作为学习目标。

在数据集 $\mathcal{D}$ 中，输入 $\boldsymbol{x}$ 服从一定的概率分布 $P(\boldsymbol{x})$，输出 $y$ 在已知 $\boldsymbol{x}$ 的条件下服从条件分布 $P(y|\boldsymbol{x})$。此时学习目标不再是目标函数，而是在已知 $\boldsymbol{x}$ 的条件下求 $y$ 的分布，因此将 $P(y|\boldsymbol{x})$ 称为目标分布。

目标函数是目标分布的特例。

在有噪声的情况下，VC 界在 $\boldsymbol{x}\overset{\text{i.i.d}}{\sim}P(\boldsymbol{x})$ 并且 $y\overset{\text{i.i.d}}{\sim}P(y|\boldsymbol{x})$ 时仍然成立。

### 2.4.2 误差度量

逐点误差度量：计算每个点的误差，对所有点的误差取平均。用 $\operatorname{err}(g(\boldsymbol{x}),f(\boldsymbol{x}))$ 表示点 $\boldsymbol{x}$ 处的误差，则

$$
E_{\text{in}}(g)=\dfrac{1}{N}\sum_{i=1}^N \operatorname{err}(g(\boldsymbol{x}_i),f(\boldsymbol{x}_i))\\
E_{\text{out}}(g)=\underset{\boldsymbol{x}\sim P}{\mathcal{E}}\operatorname{err}(g(\boldsymbol{x}),f(\boldsymbol{x}))
$$

重要的逐点误差度量方法：

1. 0/1 误差：常用于分类问题

$$
\operatorname{err}(g(\boldsymbol{x}),f(\boldsymbol{x}))=\begin{cases}
    0 & g(\boldsymbol{x})=f(\boldsymbol{x})\\
    1 & g(\boldsymbol{x})\not=f(\boldsymbol{x})
\end{cases}
$$

2. 平方误差：常用于回归问题

$$
\operatorname{err}(g(\boldsymbol{x}),f(\boldsymbol{x}))=(g(\boldsymbol{x})-f(\boldsymbol{x}))^2
$$

当数据有噪声时，目标分布 $P(y|\boldsymbol{x})$ 和误差度量 $\operatorname{err}$ 共同影响目标函数的选取。对于 0/1 误差，$f(\boldsymbol{x})=\underset{y\in\mathcal{Y}}{\argmax}P(y|\boldsymbol{x})$；对于平方误差，$f(\boldsymbol{x})=\underset{y\in\mathcal{Y}}{\sum}yP(y|\boldsymbol{x})$。

VC 维理论对大部分假设集和误差度量方法都是有效的。

### 2.4.3 算法误差度量

2.4.2 节使用的误差度量 $\operatorname{err}$ 是与具体应用无关的。而在实际应用中，同样的错误在不同的应用场景中有着不同的重要程度。以二元分类问题为例，预测结果有 4 种情况，如图 2.4 所示。

<div align="center" style="margin-bottom: 10px">
    <img src="https://raw.githubusercontent.com/zzx-JLU/images_for_markdown/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E6%9E%97%E8%BD%A9%E7%94%B0/%E5%9B%BE2.4-%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E7%9A%84%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C.png">
    <br>
    图2.4 二元分类的预测结果
</div>

对于与机密和隐私相关的问题，如指纹识别，为了保证安全，纳伪错误是不可接受的，而弃真错误尚可接受。但对于其他问题，可能更看重弃真错误，纳伪错误就显得没那么重要。因此，需要对不同的问题使用不同的误差度量。

### 2.4.4 加权分类

在实际问题中，不同的错误具有不同的重要性。为此，在计算误差时，可以为不同的错误赋予不同的权重，用**代价矩阵**表示。

对于加权二元分类问题，可以把带权重的误差度量转化成 0/1 误差度量，因此 VC 界适用于加权二元分类问题。

# 3 How Can Machines Learning?

## 3.1 线性回归

### 3.1.1 线性回归问题

输入：$\boldsymbol{x}=(x_0,x_1,\cdots,x_d)^{\text{T}}$.

线性回归的假设函数：$h(\boldsymbol{x})=\boldsymbol{w}^{\text{T}}\boldsymbol{x}$.

误差度量选择平方误差：$\operatorname{err}(\hat{y},y)=(\hat{y}-y)^2$，则损失函数为
$$
\tag{3-1} E_{\text{in}}(\boldsymbol{w})=\dfrac{1}{N}\sum_{i=1}^N (\boldsymbol{w}^{\text{T}}\boldsymbol{x}_i-y_i)^2
$$

$$
E_{\text{out}}(\boldsymbol{w})=\underset{(\boldsymbol{x},y)\sim P}{\mathcal{E}}(\boldsymbol{w}^{\text{T}}\boldsymbol{x}-y)^2
$$

这样的损失函数称为**均方误差**（mean-square error，MSE）。学习算法的目标是选出使损失函数尽可能小的参数 $\boldsymbol{w}$。

### 3.1.2 线性回归算法

损失函数的矩阵形式：
$$
\tag{3-2}
\begin{aligned}
    E_{\text{in}}(\boldsymbol{w})&=\dfrac{1}{N}\sum_{i=1}^N (\boldsymbol{w}^{\text{T}}\boldsymbol{x}_i-y_i)^2\\
    &=\dfrac{1}{N}\sum_{i=1}^N (\boldsymbol{x}_i^{\text{T}}\boldsymbol{w}-y_i)^2\\
    &=\dfrac{1}{N}\begin{Vmatrix}
        \boldsymbol{x}_1^{\text{T}}\boldsymbol{w}-y_1\\
        \boldsymbol{x}_2^{\text{T}}\boldsymbol{w}-y_2\\
        \cdots\\
        \boldsymbol{x}_N^{\text{T}}\boldsymbol{w}-y_N
    \end{Vmatrix}^2\\
    &=\dfrac{1}{N}\begin{Vmatrix}
        \begin{bmatrix}
            \boldsymbol{x}_1^{\text{T}}\\
            \boldsymbol{x}_2^{\text{T}}\\
            \cdots\\
            \boldsymbol{x}_N^{\text{T}}
        \end{bmatrix}
        \boldsymbol{w}-\begin{bmatrix}
            y_1\\
            y_2\\
            \cdots\\
            y_N
        \end{bmatrix}
    \end{Vmatrix}^2\\
    &=\dfrac{1}{N}\begin{Vmatrix}
        \boldsymbol{X}\boldsymbol{w}-\boldsymbol{y}
    \end{Vmatrix}^2
\end{aligned}
$$

其中，$\boldsymbol{X}=\begin{bmatrix}
            \boldsymbol{x}_1^{\text{T}}\\
            \boldsymbol{x}_2^{\text{T}}\\
            \cdots\\
            \boldsymbol{x}_N^{\text{T}}
        \end{bmatrix}$，$\boldsymbol{y}=(y_1,y_2,\cdots,y_N)^{\text{T}}$.

可以证明，$E_{\text{in}}(\boldsymbol{w})$ 是连续、可导的凸函数。

> 注：此处凸函数的定义为，对于一个定义在某个向量空间的凸子集 $C$ 上的实值函数 $f$，如果对于凸子集 $C$ 中任意两个向量 $\boldsymbol{x}_1,\boldsymbol{x}_2$ 有 $f(\dfrac{\boldsymbol{x}_1+\boldsymbol{x}_2}{2})\leqslant\dfrac{f(\boldsymbol{x}_1)+f(\boldsymbol{x}_2)}{2}$，则称 $f$ 为 $C$ 上的凸函数。

$E_{\text{in}}(\boldsymbol{w})$ 取得最小值的必要条件为 $\nabla E_{\text{in}}(\boldsymbol{w})=\boldsymbol{0}$。因此，要寻找最优参数 $\boldsymbol{w}_{\text{LIN}}$ 使得 $\nabla E_{\text{in}}(\boldsymbol{w}_{\text{LIN}})=\boldsymbol{0}$。

将损失函数的矩阵形式展开，得
$$
E_{\text{in}}(\boldsymbol{w})=
\dfrac{1}{N}
\begin{Vmatrix}
    \boldsymbol{X}\boldsymbol{w}-\boldsymbol{y}
\end{Vmatrix}^2
=\dfrac{1}{N}(\boldsymbol{w}^{\text{T}}\boldsymbol{X}^{\text{T}}\boldsymbol{X}\boldsymbol{w}-2\boldsymbol{w}^{\text{T}}\boldsymbol{X}^{\text{T}}\boldsymbol{y}+\boldsymbol{y}^{\text{T}}\boldsymbol{y})
$$

可以证明，损失函数的梯度为
$$
\tag{3-3}
\nabla E_{\text{in}}(\boldsymbol{w})=\dfrac{2}{N}(\boldsymbol{X}^{\text{T}}\boldsymbol{X}\boldsymbol{w}-\boldsymbol{X}^{\text{T}}\boldsymbol{y})
$$

令 $\nabla E_{\text{in}}(\boldsymbol{w})=\boldsymbol{0}$，当 $\boldsymbol{X}^{\text{T}}\boldsymbol{X}$ 可逆时，由式 $(3\text{-}3)$ 易得
$$
\tag{3-4}
\boldsymbol{w}_{\text{LIN}}=(\boldsymbol{X}^{\text{T}}\boldsymbol{X})^{-1}\boldsymbol{X}^{\text{T}}\boldsymbol{y}
$$

令 $(\boldsymbol{X}^{\text{T}}\boldsymbol{X})^{-1}\boldsymbol{X}^{\text{T}}=X^{\dag}$，称为 $\boldsymbol{X}$ 的伪逆矩阵（pseudo-inverse），则
$$
\tag{3-5}
\boldsymbol{w}_{\text{LIN}}=\boldsymbol{X}^{\dag}\boldsymbol{y}
$$

当 $\boldsymbol{X}^{\text{T}}\boldsymbol{X}$ 不可逆时，极小值点不唯一。通过改变 $\boldsymbol{X}^{\dag}$ 的定义，可以使得式 $(3\text{-}5)$ 仍然成立，从而找到最小值点。

线性回归算法：

1. 构造输入矩阵 $\boldsymbol{X}$ 和输出向量 $\boldsymbol{y}$。
2. 计算伪逆矩阵 $\boldsymbol{X}^{\dag}$。
3. 输出结果 $\boldsymbol{w}_{\text{LIN}}=\boldsymbol{X}^{\dag}\boldsymbol{y}$。

得到最优参数 $\boldsymbol{w}_{\text{LIN}}$ 后，就可以计算预测值 $\hat{y}=\boldsymbol{w}_{\text{LIN}}^{\text{T}}\boldsymbol{x}$。如果将所有预测值组织成向量，可得 $\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{X}^{\dag}\boldsymbol{y}$。

### 3.1.3 泛化问题

$$
\begin{aligned}
    E_{\text{in}}(\boldsymbol{w}_{\text{LIN}})&=\dfrac{1}{N}\begin{Vmatrix}
        \boldsymbol{y}-\hat{\boldsymbol{y}}
    \end{Vmatrix}^2\\
    &=\dfrac{1}{N}\begin{Vmatrix}
        \boldsymbol{y}-\boldsymbol{X}\boldsymbol{X}^{\dag}\boldsymbol{y}
    \end{Vmatrix}^2\\
    &=\dfrac{1}{N}\begin{Vmatrix}
        (\boldsymbol{I}-\boldsymbol{X}\boldsymbol{X}^{\dag})\boldsymbol{y}
    \end{Vmatrix}^2
\end{aligned}
$$

将 $\boldsymbol{X}\boldsymbol{X}^{\dag}$ 称为帽子矩阵（hat matrix），记作 $\boldsymbol{H}$。因为 $\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{X}^{\dag}\boldsymbol{y}$，把 $\boldsymbol{X}\boldsymbol{X}^{\dag}$ 乘在 $\boldsymbol{y}$ 前面就会给 $\boldsymbol{y}$ 戴上帽子。

帽子矩阵的几何解释如图 3.1 所示。

<div align="center" style="margin-bottom: 10px">
    <img src="https://raw.githubusercontent.com/zzx-JLU/images_for_markdown/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E6%9E%97%E8%BD%A9%E7%94%B0/%E5%9B%BE3.1-%E5%B8%BD%E5%AD%90%E7%9F%A9%E9%98%B5%E7%9A%84%E5%87%A0%E4%BD%95%E8%A7%A3%E9%87%8A.png">
    <br>
    图3.1 帽子矩阵的几何解释
</div>

由于 $\hat{\boldsymbol{y}}=\boldsymbol{X}\boldsymbol{w}_{\text{LIN}}$，输出向量的每个分量满足 $\hat{y_i}=\boldsymbol{x}_i^{\text{T}}\boldsymbol{w}_{\text{LIN}}$，是 $\boldsymbol{x}_i$ 的各分量的线性组合，因此 $\hat{\boldsymbol{y}}$ 位于 $\boldsymbol{X}$ 张成的线性空间中。而 $\boldsymbol{y}$ 不一定位于 $\boldsymbol{X}$ 张成的线性空间中，为了使 $\boldsymbol{y}-\hat{\boldsymbol{y}}$ 最小，应使得 $\boldsymbol{y}-\hat{\boldsymbol{y}}$ 与 $\boldsymbol{X}$ 张成的线性空间垂直。

帽子矩阵的作用就是将 $\boldsymbol{y}$ 投影到 $\boldsymbol{X}$ 张成的线性空间中，得到 $\hat{\boldsymbol{y}}$，此时 $\boldsymbol{y}-\hat{\boldsymbol{y}}$ 与 $\boldsymbol{X}$ 张成的线性空间垂直，可以保证 $\boldsymbol{y}-\hat{\boldsymbol{y}}$ 最小。$\boldsymbol{I}-\boldsymbol{H}$ 是一个线性变换，作用在 $\boldsymbol{y}$ 上，将 $\boldsymbol{y}$ 转换成 $\boldsymbol{y}-\hat{\boldsymbol{y}}$。

由 $\boldsymbol{H}$ 和 $\boldsymbol{I}-\boldsymbol{H}$ 的几何意义可以直观地得到下列结论：$\boldsymbol{H}^2=\boldsymbol{H}$，$(\boldsymbol{I}-\boldsymbol{H})^2=\boldsymbol{I}-\boldsymbol{H}$。

矩阵 $\boldsymbol{I}-\boldsymbol{H}$ 的迹满足如下关系：
$$
\tag{3-6} tr(\boldsymbol{I}-\boldsymbol{H})=N-(d+1)
$$

证明：
$$
\begin{aligned}
    tr(\boldsymbol{I}-\boldsymbol{H})&=tr(\boldsymbol{I}_{N\times N})-tr(\boldsymbol{H})\\
    &=N-tr(\boldsymbol{X}\boldsymbol{X}^{\dag})\\
    &=N-tr(\boldsymbol{X}(\boldsymbol{X}^{\text{T}}\boldsymbol{X})^{-1}\boldsymbol{X}^{\text{T}})\\
    &=N-tr(\boldsymbol{X}^{\text{T}}\boldsymbol{X}(\boldsymbol{X}^{\text{T}}\boldsymbol{X})^{-1})\\
    &=N-tr(\boldsymbol{I}_{(d+1)\times(d+1)})\\
    &=N-(d+1)
\end{aligned}
$$

实际训练时，数据集中存在噪声。在输出向量 $\boldsymbol{y}$ 中，有些分量从目标函数 $f$ 中产生，有些是噪声，如果用向量 $\boldsymbol{n}$ 表示噪声，则 $\boldsymbol{y}=f(\boldsymbol{X})+\boldsymbol{n}$，如图 3.2 所示。

<div align="center" style="margin-bottom: 10px">
    <img src="https://raw.githubusercontent.com/zzx-JLU/images_for_markdown/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E6%9E%97%E8%BD%A9%E7%94%B0/%E5%9B%BE3.2-%E5%B8%A6%E5%99%AA%E5%A3%B0%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92.png">
    <br>
    图3.2 带噪声的线性回归
</div>

从图中可知，对噪声使用 $\boldsymbol{I}-\boldsymbol{H}$ 进行变换也可以得到 $\boldsymbol{y}-\hat{\boldsymbol{y}}$，因此有
$$
\tag{3-7}
\begin{aligned}
    E_{\text{in}}(\boldsymbol{w}_{\text{LIN}})&=\dfrac{1}{N}\begin{Vmatrix}
        \boldsymbol{y}-\hat{\boldsymbol{y}}
    \end{Vmatrix}^2\\
    &=\dfrac{1}{N}\begin{Vmatrix}
        (\boldsymbol{I}-\boldsymbol{H})\boldsymbol{n}
    \end{Vmatrix}^2\\
    &=\dfrac{1}{N}[N-(d+1)]\begin{Vmatrix}
        \boldsymbol{n}
    \end{Vmatrix}^2
\end{aligned}
$$

对于任意一个数据集，都可以训练得到一个最优参数 $\boldsymbol{w}_{\text{LIN}}$，每个最优参数都满足式 $(3\text{-}7)$。对所有数据集的损失函数值取平均，得
$$
\tag{3-8}
\overline{E_{\text{in}}}=(1-\dfrac{d+1}{N})\cdot(\text{noise level})
$$

类似地，可以证明
$$
\tag{3-9}
\overline{E_{\text{out}}}=(1+\dfrac{d+1}{N})\cdot(\text{noise level})
$$

由式 $(3\text{-}8)$ 和 $(3\text{-}9)$，可以画出学习曲线，如图 3.3 所示。

<div align="center" style="margin-bottom: 10px">
    <img src="https://raw.githubusercontent.com/zzx-JLU/images_for_markdown/main/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-%E6%9E%97%E8%BD%A9%E7%94%B0/%E5%9B%BE3.3-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF.png">
    <br>
    图3.3 线性回归的学习曲线
</div>

从图 3.3 可以看出，当 $N\to\infin$ 时，$\overline{E_{\text{in}}}$ 和 $\overline{E_{\text{out}}}$ 都趋向于 $\sigma^2$（即 noise level） 。

可以证明，泛化误差的期望值为 $\dfrac{2(d+1)}{N}$。这一结论与 VC 界的推导结果相似。
